__copyright__ = """\
(c). Copyright 2008-2013, Vyper Logix Corp., 

                   All Rights Reserved.

Published under Creative Commons License 
(http://creativecommons.org/licenses/by-nc/3.0/) 
restricted to non-commercial educational use only., 

http://www.VyperLogix.com for details

THE AUTHOR VYPER LOGIX CORP DISCLAIMS ALL WARRANTIES WITH REGARD TO
THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS, IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL,
INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING
FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,
NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION
WITH THE USE OR PERFORMANCE OF THIS SOFTWARE !

USE AT YOUR OWN RISK.
"""

import re

from vyperlogix import misc
from vyperlogix.misc import _utils
from vyperlogix.hash import lists

try:
    from django.utils import simplejson as jsonSerializer
except ImportError:
    import simplejson as jsonSerializer

__Version = 'Version/'

__browsers = {}
__browsers['Firefox'] = 'Firefox/'              #Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.9) Gecko/20100315 Firefox/3.5.9 FBSMTWB
__browsers['Safari'] = ['Safari/',__Version]    #Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/533.16 (KHTML, like Gecko) Version/5.0 Safari/533.16
__browsers['Chrome'] = 'Chrome/'                #Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.1 (KHTML, like Gecko) Chrome/6.0.422.0 Safari/534.1
__browsers['Opera'] = ['Opera/',__Version]      #Opera/9.80 (Windows NT 6.1; U; en) Presto/2.6.30 Version/10.60
__browsers['WebKit'] = 'WebKit/'                #Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.1 (KHTML, like Gecko) Chrome/6.0.422.0 Safari/534.1

__msie = {}
__msie['MSIE'] = 'MSIE '                        #Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; GTB6.4; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET CLR 3.5.21022; Media Center PC 5.0; MS-RTC LM 8; OfficeLiveConnector.1.3; OfficeLivePatch.0.0; SLCC1; InfoPath.3; .NET4.0C; .NET4.0E; AskTB5.6) 

__android = {}
__android['Android'] = 'Android '               #Mozilla/5.0 (Linux; U; Android 2.1-update1; en-us; ADR6300 Build/ERE27) AppleWebKit/530.17 (KHTML, like Gecko) Version/4.0 Mobile Safari/530.17

__bots_json = '''
{"ObjectsSearch/": {"robot-history": " Developed by Software Objects Inc.", "robot-cover-url": " http://www.ObjectsSearch.com/", "robot-description": " Objects Search Spider", "robot-environment": " commercial", "modified-by": " support@thesoftwareobjects.com", "robot-status": " active", "robot-owner-name": " Software Objects, Inc", "robot-useragent": "ObjectsSearch/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " ObjectsSearch", "robot-id": " objectssearch", "robot-exclusion": " yes", "robot-name": " ObjectsSearch", "robot-availability": " none", "robot-details-url": " ", "robot-type": " standalone", "modified-date": " Friday March 05, 2004", "robot-noindex": " yes", "robot-owner-url": " http://www.thesoftwareobjects.com/", "robot-host": "", "robot-language": " java", "robot-platform": " unix, windows", "robot-owner-email": " support@thesoftwareobjects.com"}, "Peregrinator-Mathematics/": {"robot-history": "      commenced operation in August 1994", "robot-cover-url": "    http://www.maths.usyd.edu.au:8000/jimr/pe/Peregrinator.html", "robot-description": "  This robot is being used to generate an index of documentson Web sites connected with mathematics and statistics. Itignores off-site links, so does not stray from a list ofservers specified initially.", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Jim Richardson", "robot-useragent": "Peregrinator-Mathematics/", "robot-from": "         yes", "robot-purpose": "      ", "robot-exclusion-useragent": "", "robot-id": "           perignator", "robot-exclusion": "    yes", "robot-name": "         The Peregrinator", "robot-availability": " ", "robot-details-url": "", "robot-type": "         ", "modified-date": "      ", "robot-noindex": "      no", "robot-owner-url": "    http://www.maths.usyd.edu.au:8000/jimr.html", "robot-host": "         ", "robot-language": "     perl 4", "robot-platform": "     ", "robot-owner-email": "  jimr@maths.su.oz.au"}, "BSpider/": {"robot-history": " Starts Apr 1997 in a research project at Fuji Xerox Corp. Research Lab.", "robot-cover-url": "", "robot-description": " BSpider is crawling inside of Japanese domain for indexing.", "robot-environment": " research", "modified-by": " Yo Okumura", "robot-status": " active ", "robot-owner-name": "", "robot-useragent": "BSpider/", "robot-from": " yes", "robot-purpose": " caching ", "robot-exclusion-useragent": " bspider", "robot-id": " brightnet", "robot-exclusion": " norobot-noindex:robot-host: 209.143.1.46robot-from: norobot-useragent: Mozilla/3.01 (compatible;)robot-language:robot-description:robot-history:robot-environment:modified-date: Fri Nov 13 14:08:01 EST 1998modified-by: brian d foy <comdog@computerdog.com>robot-id: bspiderrobot-name: BSpiderrobot-cover-url: not yetrobot-details-url: not yetrobot-owner-name: Yo Okumurarobot-owner-url: not yetrobot-owner-email: okumura@rsl.crl.fujixerox.co.jprobot-status: activerobot-purpose: indexingrobot-type: standalonerobot-platform: Unixrobot-availability: nonerobot-exclusion: yes", "robot-name": " bright.net caching robot", "robot-availability": " none", "robot-details-url": "", "robot-type": "", "modified-date": " Mon, 21 Apr 1997 18:00:00 JST", "robot-noindex": " yes", "robot-owner-url": "", "robot-host": " 210.159.73.34, 210.159.73.35", "robot-language": " perl", "robot-platform": " ", "robot-owner-email": ""}, "WebWatch": {"robot-history": "      ", "robot-cover-url": "    http://www.specter.com/users/janos/specter", "robot-description": "  Its purpose is to validate HTML, and generate statistics.Check URLs modified since a given date.", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Joseph Janos", "robot-useragent": "WebWatch", "robot-from": "         no", "robot-purpose": "      maintainance, statistics", "robot-exclusion-useragent": "", "robot-id": "           webwatch", "robot-exclusion": "    no", "robot-name": "         WebWatch", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Wed Jul 26 13:36:32 1995", "robot-noindex": "      no", "robot-owner-url": "    http://www.specter.com/users/janos/specter", "robot-host": "         ", "robot-language": "     c++", "robot-platform": "     ", "robot-owner-email": "  janos@specter.com"}, "void-bot/": {"robot-history": " Development was started in october 2003, spidering began in january 2004.", "robot-cover-url": " http://www.void.be/", "robot-description": " The void-bot is used to build a database for the void search service, as well as for link validation.", "robot-environment": " research", "modified-by": " bot@void.be", "robot-status": " development", "robot-owner-name": " Tristan Crombez", "robot-useragent": "void-bot/", "robot-from": " no", "robot-purpose": " indexing,maintenance", "robot-exclusion-useragent": " void-bot", "robot-id": " voidbot", "robot-exclusion": " no", "robot-name": " void-bot", "robot-availability": " none", "robot-details-url": " http://www.void.be/void-bot.html", "robot-type": " standalone", "modified-date": " Mon, 9 Feb 2004 11:51:10 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.void.be/tristan/", "robot-host": " void.be", "robot-language": " perl5", "robot-platform": " FreeBSD,Linux", "robot-owner-email": " bot@void.be"}, "Victoria/": {"robot-history": "", "robot-cover-url": "", "robot-description": " Victoria is part of a groupware produced by Victoria Real Ltd. (voice: +44 [0]1273 774469, fax: +44 [0]1273 779960 email: victoria@pavilion.co.uk). Victoria is used to monitor changes in W3 documents, both intranet and internet based. Contact Victoria Real for more information.", "robot-environment": " commercial", "modified-by": " victoria@pavilion.co.uk", "robot-status": " development", "robot-owner-name": " Adrian Howard", "robot-useragent": "Victoria/", "robot-from": "", "robot-purpose": " maintenance", "robot-exclusion-useragent": " Victoria", "robot-id": " victoria", "robot-exclusion": " yes", "robot-name": " Victoria", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Fri, 22 Nov 1996 16:45 GMT", "robot-noindex": " yes", "robot-owner-url": "", "robot-host": "", "robot-language": " perl,c", "robot-platform": " unix", "robot-owner-email": " adrianh@oneworld.co.uk"}, "WebWalker/": {"robot-history": "A Web maintenance robot for expository purposes,               first published in the book Internet Agents: Spiders,               Wanderers, Brokers, and Bots by the robot's author.", "robot-cover-url": "", "robot-description": " WebWalker performs WWW traversal for individual                   sites and tests for the integrity of all hyperlinks                   to external sites. ", "robot-environment": " hobby", "modified-by": " Fah-Chun Cheong", "robot-status": " active", "robot-owner-name": " Fah-Chun Cheong", "robot-useragent": "WebWalker/", "robot-from": " yes", "robot-purpose": " maintenance", "robot-exclusion-useragent": " WebWalker", "robot-id": " webwalker", "robot-exclusion": " yes", "robot-name": " WebWalker", "robot-availability": " source", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Thu, 25 Jul 1996 16:00:52 PDT", "robot-noindex": " no", "robot-owner-url": " http://www.cs.berkeley.edu/~fccheong/", "robot-host": " *", "robot-language": " perl4", "robot-platform": " unix", "robot-owner-email": " fccheong@cs.berkeley.edu"}, "w3mir": {"robot-history": "", "robot-cover-url": " http://www.metastatic.org/wlm/robot-owner-name: Casey Marshallrobot-owner-url: http://www.metastatic.org/robot-owner-email: rsdio@metastatic.orgrobot-status: activerobot-purpose: statisticsrobot-type: standalonerobot-platform: unix, windows,robot-availability: source, datarobot-exclusion: norobot-exclusion-useragent: wlmrobot-noindex: norobot-nofollow: norobot-host: blossom.metastatic.orgrobot-from: norobot-useragent: wlm-1.1robot-language: javarobot-description1: Builds the 'Picture of Weblogs' applet.robot-description2: See http://www.metastatic.org/wlm/.robot-environment: hobbymodified-date: Fri, 2 Nov 2001 04:55:00 PSTrobot-id:           wmirrobot-name:         w3mirrobot-cover-url:    http://www.ifi.uio.no/~janl/w3mir.html", "robot-description": "  W3mir uses the If-Modified-Since HTTP header and recursesonly the directory and subdirectories of it's startdocument.  Known to work on U*ixes and WindowsNT.", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Nicolai Langfeldt", "robot-useragent": "w3mir", "robot-from": "         yes", "robot-purpose": "      mirroring.", "robot-exclusion-useragent": "", "robot-id": " wlm", "robot-exclusion": "    no.", "robot-name": " Weblog Monitorrobot-details-url: http://www.metastatic.org/wlm/", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Wed Apr 24 13:23:42 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.ifi.uio.no/~janl/w3mir.html", "robot-host": "", "robot-language": "     Perl", "robot-platform": "     UNIX, WindowsNT", "robot-owner-email": "  w3mir-core@usit.uio.no"}, "Robofox": {"robot-history": "", "robot-cover-url": "", "robot-description": " scheduled utility to download and database a domain", "robot-environment": " service", "modified-by": " Ian Hicks", "robot-status": " development", "robot-owner-name": " Ian Hicks", "robot-useragent": "Robofox", "robot-from": " no", "robot-purpose": " site download", "robot-exclusion-useragent": " robofox", "robot-id": " robofox", "robot-exclusion": " no", "robot-name": " RoboFox", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Tue, 6 Mar 2001 02:15:00 GMT", "robot-noindex": " no", "robot-owner-url": "", "robot-host": " *", "robot-language": " Visual FoxPro", "robot-platform": " windows9x, windowsme, windowsNT4, windows2000", "robot-owner-email": " robo_fox@hotmail.com"}, "Linkidator/": {"robot-history": " Built using WWW-Robot-0.022 perl module.  Currently in beta test.  Seeking approval for public release.", "robot-cover-url": "", "robot-description": " Recursively checks all links on a site, looking for broken or redirected links.  Checks all off-site links using HEAD requests and does not progress further.  Designed to behave well and to be very configurable.", "robot-environment": " internal", "modified-by": " Thomas Gimon", "robot-status": " development", "robot-owner-name": " Thomas Gimon", "robot-useragent": "Linkidator/", "robot-from": " yes", "robot-purpose": " maintenance", "robot-exclusion-useragent": " Linkidator", "robot-id": " linkidator", "robot-exclusion": " yes", "robot-name": " Link Validator", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Fri, 20 Jan 2001 02:22:00 EST", "robot-noindex": " yesrobot-nofollow: yes", "robot-owner-url": "", "robot-host": " *.mitre.org", "robot-language": " perl5", "robot-platform": " unix, windows", "robot-owner-email": " tgimon@mitre.org"}, "GetURL": {"robot-history": "      ", "robot-cover-url": "    http://Snark.apana.org.au/James/GetURL/", "robot-description": "  Its purpose is to validate links, perform mirroring, andcopy document trees. Designed as a tool for retrieving webpages in batch mode without the encumbrance of a browser.Can be used to describe a set of pages to fetch, and tomaintain an archive or mirror. Is not run by a central siteand accessed by clients - is run by the end user or archivemaintainer", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   James Burton", "robot-useragent": "GetURL", "robot-from": "         no", "robot-purpose": "      maintenance, mirroring", "robot-exclusion-useragent": "", "robot-id": "           geturl", "robot-exclusion": "    no", "robot-name": "         GetURL", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Tue May 9 15:13:12 1995", "robot-noindex": "      no", "robot-owner-url": "    http://Snark.apana.org.au/James/", "robot-host": "         *", "robot-language": "     ARexx (Amiga REXX)", "robot-platform": "     ", "robot-owner-email": "  James@Snark.apana.org.au"}, "urlck/": {"robot-history": "              Originally designed to validate URLs.", "robot-cover-url": "            http://www.cutternet.com/products/webcheck.html", "robot-description": "          The robot is used to manage, maintain, and modify                            web sites.  It builds a database detailing the                            site, builds HTML reports describing the site, and                            can be used to up-load pages to the site or to                            modify existing pages and URLs within the site.  It                            can also be used to mirror whole or partial sites.                            It supports HTTP, File, FTP, and Mailto schemes.", "robot-environment": "          commercial", "modified-by": "                Dave Finnegan", "robot-status": "               active", "robot-owner-name": "           Dave Finnegan", "robot-useragent": "urlck/", "robot-from": "                 yes", "robot-purpose": "              maintenance", "robot-exclusion-useragent": "  urlck", "robot-id": "                   urlck", "robot-exclusion": "            yes", "robot-name": "                 URL Check", "robot-availability": "         binary", "robot-details-url": "          http://www.cutternet.com/products/urlck.html", "robot-type": "                 standalone", "modified-date": "              July 9, 1997", "robot-noindex": "              no", "robot-owner-url": "            http://www.cutternet.com", "robot-host": "                 *", "robot-language": "             c", "robot-platform": "             unix", "robot-owner-email": "          dave@cutternet.com"}, "MerzScope": {"robot-history": " ", "robot-cover-url": "http://www.merzcom.com", "robot-description": " Robot is part of a Web-Mapping package called MerzScope, to be used mainly by consultants, and web masters to create and publish maps, on and of the World wide web.", "robot-environment": "", "modified-by": " Philip Lenir, MerzScope lead developper", "robot-status": "actively in use", "robot-owner-name": "(Client based robot)", "robot-useragent": "MerzScope", "robot-from": "", "robot-purpose": "WebMapping", "robot-exclusion-useragent": " MerzScope", "robot-id": "merzscope", "robot-exclusion": " yes", "robot-name": "MerzScope", "robot-availability": "binary", "robot-details-url": "http://www.merzcom.com", "robot-type": "standalone", "modified-date": " Fri, 13 March 1997 16:31:00", "robot-noindex": " no", "robot-owner-url": "(Client based robot)", "robot-host": "(Client Based)", "robot-language": "java", "robot-platform": "(Java Based) unix,windows95,windowsNT,os2,mac etc ..", "robot-owner-email": ""}, "JubiiRobot/": {"robot-history": "      Will be in constant operation from Spring1996", "robot-cover-url": "    http://www.jubii.dk/robot/default.htm", "robot-description": "  Its purpose is to generate a Resource Discovery database,and validate links. Used for indexing the .dk top-leveldomain as well as other Danish sites for aDanish webdatabase, as well as link validation.", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Jakob Faarvang", "robot-useragent": "JubiiRobot/", "robot-from": "         yes", "robot-purpose": "      indexing, maintainance", "robot-exclusion-useragent": "", "robot-id": "           jubii", "robot-exclusion": "    yes", "robot-name": "         The Jubii Indexing Robot", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sat Jan  6 20:58:44 1996", "robot-noindex": "      no", "robot-owner-url": "    http://www.cybernet.dk/staff/jakob/", "robot-host": "         any host in the cybernet.dk domain", "robot-language": "     visual basic 4.0", "robot-platform": "     ", "robot-owner-email": "  jakob@jubii.dk"}, "BoxSeaBot/": {"robot-history": " The robot code uses Nutch.  Earlier experimental crawls were done under various user agent names such as NutchCVS(boxsea)", "robot-cover-url": " http://www.boxsea.com/crawler", "robot-description": " This robot is used to find pages for building the BoxSea search engine indices.", "robot-environment": "", "modified-by": " BoxSeaBot", "robot-status": " active", "robot-owner-name": " BoxSea Search Engine", "robot-useragent": "BoxSeaBot/", "robot-from": "", "robot-purpose": " indexing", "robot-exclusion-useragent": " boxseabot", "robot-id": " boxseabot", "robot-exclusion": " yes", "robot-name": " BoxSeaBot", "robot-availability": "", "robot-details-url": " http://www.boxsea.com/crawler", "robot-type": " standalone", "modified-date": " Fri, 23 Jul 2004 11:58:00 PST", "robot-noindex": " ", "robot-owner-url": " http://www.boxsea.com", "robot-host": " ", "robot-language": " java", "robot-platform": " linux", "robot-owner-email": " boxseasearch@yahoo.com"}, "Die Blinde Kuh": {"robot-history": " The robot was developed by Stefan R. Mueller to help by the manual proof of registered Links.", "robot-cover-url": " http://www.blinde-kuh.de/", "robot-description": " The robot is use for indixing and proofing the registered urls in the german language search-engine for kids. Its a none-comercial one-woman-project of Birgit Bachmann living in Hamburg, Germany.", "robot-environment": " hobby", "modified-by": " Stefan R. Mueller", "robot-status": " development", "robot-owner-name": " Stefan R. Mueller", "robot-useragent": "Die Blinde Kuh", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": "", "robot-id": " blindekuh", "robot-exclusion": " no", "robot-name": " Die Blinde Kuh", "robot-availability": " none", "robot-details-url": " http://www.blinde-kuh.de/robot.html (german language)", "robot-type": " browser", "modified-date": " Mon Jul 22 1998", "robot-noindex": " no", "robot-owner-url": " http://www.rrz.uni-hamburg.de/philsem/stefan_mueller/", "robot-host": " minerva.sozialwiss.uni-hamburg.de", "robot-language": " perl5", "robot-platform": " unix", "robot-owner-email": "maschinist@blinde-kuh.de"}, "weblayers/": {"robot-history": "      ", "robot-cover-url": " http://www.thunderstone.com/texis/site/pages/webinator.htmlrobot-owner-name: robot-owner-email: robot-status: active, under further enhancement.robot-purpose: information retrievalrobot-type: standalonerobot-exclusion: yesrobot-noindex: yesrobot-exclusion-useragent: T-H-U-N-D-E-R-S-T-O-N-Erobot-host: severalrobot-from: Norobot-language: Texis Vortexrobot-history: robot-environment: Commercialrobot-id:           weblayersrobot-name:         weblayersrobot-cover-url:    http://www.univ-paris8.fr/~loic/weblayers/", "robot-description": "  Its purpose is to validate, cache and maintain links. It isdesigned to maintain the cache generated by the emacs emacsw3 mode (N*tscape replacement) and to support annotateddocuments (keep them in sync with the original document viadiff/patch).", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Loic Dachary", "robot-useragent": "weblayers/", "robot-from": "         ", "robot-purpose": "      maintainance", "robot-exclusion-useragent": "", "robot-id": " webinator", "robot-exclusion": "    yes", "robot-name": " Webinatorrobot-details-url: http://www.thunderstone.com/texis/site/pages/webinator4_admin.html", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Fri Jun 23 16:30:42 FRE 1995", "robot-noindex": "      no", "robot-owner-url": "    http://www.univ-paris8.fr/~loic/", "robot-host": "         ", "robot-language": "     perl 5", "robot-platform": "     ", "robot-owner-email": "  loic@afp.com"}, "appie/": {"robot-history": " The spider was built in march/april 2000", "robot-cover-url": " www.walhello.com", "robot-description": " The appie-spider is used to collect and index web pages for the Walhello search engine", "robot-environment": " commercial", "modified-by": " Aimo Pieterse", "robot-status": " active", "robot-owner-name": " Aimo Pieterse", "robot-useragent": "appie/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " appie", "robot-id": " appie", "robot-exclusion": " yes", "robot-name": " Walhello appie", "robot-availability": " none", "robot-details-url": " www.walhello.com/aboutgl.html", "robot-type": " standalone", "modified-date": " Thu, 20 Jul 2000 22:38:00 GMT", "robot-noindex": " yes", "robot-owner-url": " www.walhello.com", "robot-host": " 213.10.10.116, 213.10.10.117, 213.10.10.118", "robot-language": " Visual C++", "robot-platform": " windows98", "robot-owner-email": " aimo@walhello.com"}, "Muninn/": {"robot-history": " It's hard to keep track of things. Automation helps.", "robot-cover-url": " http://www.goodlookingcooking.co.ukrobot-owner-name: Chris Ridingsrobot-owner-url: http://www.goodlookingcooking.co.ukrobot-owner-email: muncher@ridings.org.ukrobot-status: developmentrobot-purpose: indexingrobot-type: standalonerobot-platform: unixrobot-availability: nonerobot-exclusion: yesrobot-exclusion-useragent: muncherrobot-noindex: yesrobot-nofollow: yesrobot-host: www.goodlookingcooking.co.ukrobot-from: norobot-useragent: yesrobot-language: perlrobot-description: Used to build the index for www.goodlookingcooking.co.uk. Seeks out cooking and recipe pages.robot-history: Private project september 2001robot-environment: hobbymodified-date: Wed, 5 Sep 2001 19:21:00 GMTrobot-id: muninnrobot-name: Muninnrobot-cover-url: http://people.freenet.de/Muninn/eyrie.html", "robot-description": " Muninn looks at museums within my reach and tells me about current exhibitions.", "robot-environment": " hobby", "modified-by": " Sandra Groth", "robot-status": " development", "robot-owner-name": " Sandra Groth", "robot-useragent": "Muninn/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " muninn", "robot-id": " muncher", "robot-exclusion": " yes", "robot-name": " Muncherrobot-details-url: http://www.goodlookingcooking.co.uk/info.htm", "robot-availability": " source, data", "robot-details-url": " http://people.freenet.de/Muninn/", "robot-type": " standalone", "modified-date": " Thu Jun  3 16:36:47 CEST 2004", "robot-noindex": " yesrobot-nofollow: yes", "robot-owner-url": " http://santana.dynalias.net/", "robot-host": " santana.dynalias.net, 80.185.*, *", "robot-language": " Perl5", "robot-platform": " unix", "robot-owner-email": " muninn_bot@gmx.net"}, "Gromit/": {"robot-history": " This robot is based on the Perl5 LWP::RobotUA module.", "robot-cover-url": " http://www.austlii.edu.au/", "robot-description": " Gromit is a Targetted Web Spider that indexes legal sites contained in the AustLII legal links database.", "robot-environment": " research", "modified-by": " Daniel Austin", "robot-status": " development", "robot-owner-name": " Daniel Austin", "robot-useragent": "Gromit/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " Gromit", "robot-id": " gromit", "robot-exclusion": " yes", "robot-name": " Gromit", "robot-availability": " none", "robot-details-url": " http://www2.austlii.edu.au/~dan/gromit/", "robot-type": " standalone", "modified-date": " Wed, 11 Jun 1997 03:58:40 GMT", "robot-noindex": " no", "robot-owner-url": " http://www2.austlii.edu.au/~dan/", "robot-host": " *.austlii.edu.au", "robot-language": " perl5", "robot-platform": " unix", "robot-owner-email": " dan@austlii.edu.au"}, "SLCrawler": {"robot-history": "It is SLCrawler to crawl html page on Internet.", "robot-cover-url": "", "robot-description": "To build the site map.", "robot-environment": " commercial: is a commercial product", "modified-by": "Karen Ng", "robot-status": "active", "robot-owner-name": "Inxight Software", "robot-useragent": "SLCrawler", "robot-from": "", "robot-purpose": "To build the site map.", "robot-exclusion-useragent": "SLCrawler/2.0", "robot-id": "slcrawler", "robot-exclusion": "yes", "robot-name": "SLCrawler", "robot-availability": "none", "robot-details-url": "", "robot-type": "standalone", "modified-date": "Nov. 15, 2000", "robot-noindex": "no", "robot-owner-url": "http://www.inxight.com", "robot-host": "n/a", "robot-language": "Java", "robot-platform": "windows, windows95, windowsNT", "robot-owner-email": "kng@inxight.com"}, "CACTVS Chemistry Spider": {"robot-history": "", "robot-cover-url": "    http://schiele.organik.uni-erlangen.de/cactvs/spider.html", "robot-description": "  Locates chemical structures in Chemical MIME formats on WWWand FTP servers and downloads them into database searchablewith structure queries (substructure, fullstructure,formula, properties etc.)", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   W. D. Ihlenfeldt", "robot-useragent": "CACTVS Chemistry Spider", "robot-from": "         no", "robot-purpose": "      indexing.", "robot-exclusion-useragent": "", "robot-id": "           cactvschemistryspider", "robot-exclusion": "    yes", "robot-name": "         CACTVS Chemistry Spider", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sat Mar 30 00:55:40 1996.", "robot-noindex": "", "robot-owner-url": "    http://schiele.organik.uni-erlangen.de/cactvs/", "robot-host": "         utamaro.organik.uni-erlangen.de", "robot-language": "     TCL, C", "robot-platform": "", "robot-owner-email": "  wdi@eros.ccc.uni-erlangen.de"}, "moget/": {"robot-history": "", "robot-cover-url": "", "robot-description": " This robot is used to build the database for the search service operated by goo", "robot-environment": "service", "modified-by": "moget@goo.ne.jp", "robot-status": "active", "robot-owner-name": "NTT-ME Infomation Xing,Inc", "robot-useragent": "moget/", "robot-from": "yes", "robot-purpose": "indexing,statistics", "robot-exclusion-useragent": "moget", "robot-id": "moget", "robot-exclusion": "yes", "robot-name": "moget", "robot-availability": "none", "robot-details-url": "", "robot-type": "standalone", "modified-date": "Thu, 30 Mar 2000 18:40:37 GMT", "robot-noindex": "yes", "robot-owner-url": "http://www.nttx.co.jp", "robot-host": "*.goo.ne.jp", "robot-language": "c", "robot-platform": "unix", "robot-owner-email": "moget@goo.ne.jp"}, "BackRub/": {"robot-history": "", "robot-cover-url": "", "robot-description": "", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Larry Page", "robot-useragent": "BackRub/", "robot-from": "         yes", "robot-purpose": "      indexing, statistics", "robot-exclusion-useragent": "", "robot-id": "           backrub", "robot-exclusion": "    yes", "robot-name": "         BackRub", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Wed Feb 21 02:57:42 1996.", "robot-noindex": "", "robot-owner-url": "    http://backrub.stanford.edu/", "robot-host": "         *.stanford.edu", "robot-language": "     Java.", "robot-platform": "", "robot-owner-email": "  page@leland.stanford.edu"}, "Scooter/": {"robot-history": " Version 2 of Scooter/1.0 developed by Louis Monier of WRL.", "robot-cover-url": " http://www.altavista.com/", "robot-description": " Scooter is AltaVista's prime index agent.", "robot-environment": " service", "modified-by": " steves@avs.dec.com", "robot-status": " active", "robot-owner-name": " AltaVista", "robot-useragent": "Scooter/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " Scooter", "robot-id": " scooter", "robot-exclusion": " yes", "robot-name": " Scooter", "robot-availability": " none", "robot-details-url": " http://www.altavista.com/av/content/addurl.htm", "robot-type": " standalone", "modified-date": " Wed, 13 Jan 1999 17:18:59 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.altavista.com/", "robot-host": " *.av.pa-x.dec.com", "robot-language": " c", "robot-platform": " unix", "robot-owner-email": " scooter@pa.dec.com"}, "aWapClient": {"robot-history": " new", "robot-cover-url": " http://www.skymob.com/", "robot-description": " WAP content Crawler.", "robot-environment": " service", "modified-by": " Owen Lydiard", "robot-status": " active", "robot-owner-name": " Have IT Now Limited.", "robot-useragent": "aWapClient", "robot-from": " searchmaster@skymob.com", "robot-purpose": " indexing", "robot-exclusion-useragent": " skymob", "robot-id": " skymob", "robot-exclusion": " yes", "robot-name": " Skymob.com", "robot-availability": " none", "robot-details-url": " http://www.skymob.com/about.html", "robot-type": " standalone", "modified-date": " Thu Sep  6 17:50:32 BST 2001", "robot-noindex": " no", "robot-owner-url": " http://www.skymob.com/", "robot-host": " www.skymob.com", "robot-language": " c++", "robot-platform": " unix", "robot-owner-email": " searchmaster@skymob.com"}, "root/": {"robot-history": "      First versions since October 1995.", "robot-cover-url": "    http://www.di.uminho.pt/wc", "robot-description": "  Parallel robot developed in Minho Univeristy in Portugal tocatalog relations among URLs and to support a specialnavigation aid.", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Jorge Portugal Andrade", "robot-useragent": "root/", "robot-from": "         no", "robot-purpose": "      indexing, maintenance", "robot-exclusion-useragent": "", "robot-id": "           core", "robot-exclusion": "    yes", "robot-name": "         Web Core / Roots", "robot-availability": "", "robot-details-url": "", "robot-type": "", "modified-date": "      Wed Jan 10 23:19:08 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.di.uminho.pt/~cbm", "robot-host": "         shiva.di.uminho.pt, from www.di.uminho.pt", "robot-language": "     perl", "robot-platform": "", "robot-owner-email": "  wc@di.uminho.pt"}, "ssearcher100": {"robot-history": "  Released 4/4/1999", "robot-cover-url": " www.satacoy.com", "robot-description": " Site Searcher scans web sites for specific file types. (JPG, MP3, MPG, etc)", "robot-environment": " hobby", "modified-by": "", "robot-status": " active", "robot-owner-name": " Zackware", "robot-useragent": "ssearcher100", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": "", "robot-id": " ssearcher", "robot-exclusion": " no", "robot-name": " Site Searcher", "robot-availability": " binary", "robot-details-url": " www.satacoy.com", "robot-type": " standalone", "modified-date": " 04/26/1999robot-id: sukerobot-name: Sukerobot-cover-url: http://www.kensaku.org/robot-details-url: http://www.kensaku.org/robot-owner-name: Yosuke Kurodarobot-owner-url: http://www.kensaku.org/yk/robot-owner-email: robot@kensaku.orgrobot-status: developmentrobot-purpose: indexingrobot-type: standalonerobot-platform: FreeBSD3.*robot-availability: sourcerobot-exclusion: yesrobot-exclusion-useragent: sukerobot-noindex: norobot-host: *robot-from: yesrobot-useragent: suke/*.*robot-language: crobot-description: This robot visits mainly sites in japan.robot-history: since 1999robot-environment: servicerobot-id: suntekrobot-name: suntek search enginerobot-cover-url: http://www.portal.com.hk/robot-details-url: http://www.suntek.com.hk/robot-owner-name: Suntek Computer Systemsrobot-owner-url: http://www.suntek.com.hk/robot-owner-email: karen@suntek.com.hkrobot-status: operationalrobot-purpose: to create a search portal on Asian web sitesrobot-type:robot-platform: NT, Linux, UNIXrobot-availability: available nowrobot-exclusion:robot-exclusion-useragent:robot-noindex: yesrobot-host: search.suntek.com.hkrobot-from: yesrobot-useragent: suntek/1.0robot-language: Javarobot-description: A multilingual search engine with emphasis on Asia contentsrobot-history:robot-environment:modified-date:", "robot-noindex": " no", "robot-owner-url": " www.satacoy.com", "robot-host": " *", "robot-language": " C++", "robot-platform": " winows95, windows98, windowsNT", "robot-owner-email": " zackware@hotmail.com"}, "gestaltIconoclast/": {"robot-history": " This robot has a history in mathematics and english", "robot-cover-url": " http://gestalt.sewanee.edu/ic/", "robot-description": " This guy likes statistics", "robot-environment": " research", "modified-by": " chris@gestalt.sewanee.edu", "robot-status": " development", "robot-owner-name": " Chris Cappuccio", "robot-useragent": "gestaltIconoclast/", "robot-from": " yes ", "robot-purpose": " statistics ", "robot-exclusion-useragent": "", "robot-id": " iconoclast", "robot-exclusion": " no ", "robot-name": " Popular Iconoclast", "robot-availability": " source", "robot-details-url": " http://gestalt.sewanee.edu/ic/info.html", "robot-type": " standalone", "modified-date": " Wed, 5 Mar 1997 17:35:16 CST", "robot-noindex": " no", "robot-owner-url": " http://sefl.satelnet.org/~ccappuc/", "robot-host": " gestalt.sewanee.edu", "robot-language": " c,perl5", "robot-platform": " unix (OpenBSD)", "robot-owner-email": " chris@gestalt.sewanee.edu"}, "MindCrawler": {"robot-history": "", "robot-cover-url": " http://www.mindpass.com/_technology_faq.htm", "robot-description": " ", "robot-environment": "", "modified-by": "", "robot-status": " active", "robot-owner-name": " Mindpass", "robot-useragent": "MindCrawler", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " MindCrawler", "robot-id": " MindCrawler", "robot-exclusion": " yes", "robot-name": " MindCrawler", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Tue Mar 28 11:30:09 CEST 2000", "robot-noindex": " no", "robot-owner-url": " http://www.mindpass.com/", "robot-host": " *", "robot-language": " c++", "robot-platform": " linux", "robot-owner-email": " support@mindpass.com"}, "Fish-Search-Robot": {"robot-history": "      Originated as an addition to Mosaic for X", "robot-cover-url": "    http://www.win.tue.nl/bin/fish-search", "robot-description": "  Its purpose is to discover resources on the fly a versionexists that is integrated into the T&uuml;bingen Mosaic2.4.2 browser (also written in C)", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Paul De Bra", "robot-useragent": "Fish-Search-Robot", "robot-from": "         no", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           fish", "robot-exclusion": "    no", "robot-name": "         Fish search", "robot-availability": " binary", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Mon May 8 09:31:19 1995", "robot-noindex": "      no", "robot-owner-url": "    http://www.win.tue.nl/win/cs/is/debra/", "robot-host": "         www.win.tue.nl", "robot-language": "     c", "robot-platform": "     ", "robot-owner-email": "  debra@win.tue.nl"}, "wired-digital-newsbot/": {"robot-history": "", "robot-cover-url": "", "robot-description": " this is a test", "robot-environment": " research", "modified-by": " bowen@hotwired.com", "robot-status": " development", "robot-owner-name": " Bowen Dwelle", "robot-useragent": "wired-digital-newsbot/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " hotwired", "robot-id": " wired-digital", "robot-exclusion": " yes", "robot-name": " Wired Digital", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Thu, 30 Oct 1997", "robot-noindex": " no", "robot-owner-url": "", "robot-host": " gossip.hotwired.com", "robot-language": " perl-5.004", "robot-platform": " unix", "robot-owner-email": " bowen@hotwired.com"}, "ESIRover": {"robot-history": "  Used as the front-end to SmartSpider (another Spider    product sold by Engineeering Software, Inc.)", "robot-cover-url": " http://www.engsoftware.com/fetch.htm", "robot-description": " FetchRover fetches Web Pages.     It is an automated page-fetching engine. FetchRover can be   used stand-alone or as the front-end to a full-featured Spider.   Its database can use any ODBC compliant database server, including   Microsoft Access, Oracle, Sybase SQL Server, FoxPro, etc.", "robot-environment": " commercial, service", "modified-by": " Ken Wadland", "robot-status": " active", "robot-owner-name": " Dr. Kenneth R. Wadland", "robot-useragent": "ESIRover", "robot-from": " yes", "robot-purpose": " maintenance, statistics", "robot-exclusion-useragent": " ESI", "robot-id": " fetchrover", "robot-exclusion": " yes", "robot-name": " FetchRover", "robot-availability": " binary, source", "robot-details-url": " http://www.engsoftware.com/spiders/", "robot-type": " standalone", "modified-date": " Thu, 03 Apr 1997 21:49:50 EST", "robot-noindex": " N/A", "robot-owner-url": " http://www.engsoftware.com/", "robot-host": " *", "robot-language": " C++", "robot-platform": " Windows/NT, Windows/95, Solaris SPARC", "robot-owner-email": " ken@engsoftware.com"}, "Senrigan": {"robot-history": "      It has been running since Dec 1994", "robot-cover-url": "    http://www.info.waseda.ac.jp/search-e.html", "robot-description": "  This robot now gets HTMLs from only jp domain.", "robot-environment": "  research", "modified-by": "        TAMURA Kent", "robot-status": "       active", "robot-owner-name": "   TAMURA Kent", "robot-useragent": "Senrigan", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "Senrigan", "robot-id": "           senrigan", "robot-exclusion": "    yes", "robot-name": "         Senrigan", "robot-availability": " none", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Mon Jul  1 07:30:00 GMT 1996", "robot-noindex": "      yes", "robot-owner-url": "    http://www.info.waseda.ac.jp/muraoka/members/kent/", "robot-host": "         aniki.olu.info.waseda.ac.jp", "robot-language": "     Java", "robot-platform": "     Java", "robot-owner-email": "  kent@muraoka.info.waseda.ac.jp"}, "DWCP/": {"robot-history": " Developed from scratch by Dridus Norwind.", "robot-cover-url": " http://www.dridus.com/~rmm/dwcp.php3", "robot-description": " The DWCP robot is used to gather information for Dridus' Web Cataloging Project, which is intended to catalog domains and urls (no content).", "robot-environment": " hobby", "modified-by": " Ross Mellgren", "robot-status": " development", "robot-owner-name": " Ross Mellgren (Dridus Norwind)", "robot-useragent": "DWCP/", "robot-from": " dridus@dridus.com", "robot-purpose": " indexing, statistics", "robot-exclusion-useragent": " dwcp", "robot-id": " dwcp", "robot-exclusion": " yes", "robot-name": " DWCP (Dridus' Web Cataloging Project)", "robot-availability": " source, binary, data", "robot-details-url": " http://www.dridus.com/~rmm/dwcp.php3", "robot-type": " standalone", "modified-date": " Sat, 10 Jul 1999 00:05:40 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.dridus.com/~rmm", "robot-host": " *.dridus.com", "robot-language": " java", "robot-platform": " java", "robot-owner-email": " rmm@dridus.com"}, "AlkalineBOT": {"robot-history": " Vestris Inc. search engine designed at the University of Geneva ", "robot-cover-url": " http://www.vestris.com/alkaline", "robot-description": " Unix/NT internet/intranet search engine", "robot-environment": " commercial research ", "modified-by": " Daniel Doubrovkine <dblock@vestris.com>", "robot-status": " development active", "robot-owner-name": " Daniel Doubrovkine", "robot-useragent": "AlkalineBOT", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " AlkalineBOT ", "robot-id": " Alkaline", "robot-exclusion": " yes", "robot-name": " Alkaline", "robot-availability": " binary      ", "robot-details-url": " http://www.vestris.com/alkaline", "robot-type": " standalone     ", "modified-date": " Thu Dec 10 14:01:13 MET 1998", "robot-noindex": " yes", "robot-owner-url": " http://cuiwww.unige.ch/~doubrov5 ", "robot-host": " *", "robot-language": " c++", "robot-platform": " unix windows95 windowsNT", "robot-owner-email": " dblock@vestris.com"}, "borg-bot/": {"robot-history": "  ", "robot-cover-url": " ", "robot-description": " Developmental crawler to feed a search engine", "robot-environment": " research service ", "modified-by": " Sat, 20 Oct 2001 04:00:00 GMT", "robot-status": " development", "robot-owner-name": " James Bragg", "robot-useragent": "borg-bot/", "robot-from": " yes", "robot-purpose": " indexing statistics", "robot-exclusion-useragent": " borg-bot/0.9", "robot-id": " borg-bot", "robot-exclusion": " yes", "robot-name": " Borg-Bot", "robot-availability": " none", "robot-details-url": " http://www.skunkfarm.com/borgbot.htm", "robot-type": " standalone", "modified-date": " Sat, 20 Oct 2001 04:00:00 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.skunkfarm.com", "robot-host": " 24.11.13.173", "robot-language": " python", "robot-platform": " Linux Windows2000", "robot-owner-email": " botdev@skunkfarm.com"}, "AnthillV": {"robot-history": "This is a reasearch project at the University of Mannheim in Germany, professorship Prof. Martin Schader, assistant Dr. Stefan Kuhlins", "robot-cover-url": "http://www.anthill.org/index.html", "robot-description": "Anthill is used to gather priceinformation automatically from online stores.support for international versions.", "robot-environment": "research", "modified-by": "Torsten Kaubisch", "robot-status": "development", "robot-owner-name": "Torsten Kaubisch", "robot-useragent": "AnthillV", "robot-from": "no", "robot-purpose": "indexing", "robot-exclusion-useragent": "anthill", "robot-id": "anthill", "robot-exclusion": "no (soon in V1.2)", "robot-name": "Anthill", "robot-availability": "not yet", "robot-details-url": "http://www.anthill.org/index.html", "robot-type": "standalone", "modified-date": "Thu, 6 Dec 2001 01:55:00 GMT", "robot-noindex": "no", "robot-owner-url": "http://www.anthill.org/index.html", "robot-host": "anywhere", "robot-language": "java", "robot-platform": "independent", "robot-owner-email": "info@anthill.org"}, "JoeBot/": {"robot-history": "", "robot-cover-url": "", "robot-description": "", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Ray Waldin", "robot-useragent": "JoeBot/", "robot-from": "         yes", "robot-purpose": "", "robot-exclusion-useragent": "", "robot-id": "           joebot", "robot-exclusion": "    yes", "robot-name": "         JoeBot", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sun May 19 08:13:06 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.primenet.com/~rwaldin", "robot-host": "", "robot-language": "     java JoeBot is a generic web crawler implemented as acollection of Java classes which can be used in a variety ofapplications, including resource discovery, link validation,mirroring, etc.  It currently limits itself to one visit perhost per minute.", "robot-platform": "", "robot-owner-email": "  rwaldin@primenet.com"}, "Bjaaland/": {"robot-history": " None, yet", "robot-cover-url": " http://www.textuality.com", "robot-description": " Crawls sites listed in the ODP (see http://dmoz.org)", "robot-environment": " service", "modified-by": " tbray@textuality.com", "robot-status": " development", "robot-owner-name": " Tim Bray", "robot-useragent": "Bjaaland/", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " Bjaaland", "robot-id": " bjaaland", "robot-exclusion": " yes", "robot-name": " Bjaaland", "robot-availability": " none", "robot-details-url": " http://www.textuality.com", "robot-type": " standalone", "modified-date": " Monday, 19 July 1999, 13:46:00 PDT", "robot-noindex": " no", "robot-owner-url": " http://www.textuality.com", "robot-host": " barry.bitmovers.net", "robot-language": " perl5", "robot-platform": " unix", "robot-owner-email": " tbray@textuality.com"}, "DragonBot/": {"robot-history": "", "robot-cover-url": " http://www.paczone.com/", "robot-description": " Collects web pages related to East Asia", "robot-environment": " service", "modified-by": "", "robot-status": " active", "robot-owner-name": " Paul Law", "robot-useragent": "DragonBot/", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " DragonBot", "robot-id": " dragonbot", "robot-exclusion": " yes", "robot-name": " DragonBot", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Mon, 11 Aug 1997 00:00:00 GMT", "robot-noindex": " no", "robot-owner-url": "", "robot-host": " *.paczone.com", "robot-language": " C++", "robot-platform": " windowsNT", "robot-owner-email": " admin@paczone.com"}, "MSNBOT/": {"robot-history": " Developed by Microsoft Corp.", "robot-cover-url": " http://search.msn.com", "robot-description": " MSN Search Crawler", "robot-environment": " commercial", "modified-by": " msnbot@microsoft.com", "robot-status": " active", "robot-owner-name": " Microsoft Corp.", "robot-useragent": "MSNBOT/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " msnbot", "robot-id": " msnbot", "robot-exclusion": " yes", "robot-name": " MSNBot", "robot-availability": " none", "robot-details-url": " http://search.msn.com/msnbot.htm", "robot-type": " standalone", "modified-date": " June 23, 2003", "robot-noindex": " yes", "robot-owner-url": " http://www.microsoft.com", "robot-host": " <TBD>", "robot-language": " C++", "robot-platform": " Windows Server 2000, Windows Server 2003", "robot-owner-email": " msnbot@microsoft.com"}, "uptimebot": {"robot-history": " This robot is a local research product of the UtimeBot team.", "robot-cover-url": " http://mysearch.udm.net/robot-owner-name: Alexander Barkovrobot-owner-url: http://mysearch.udm.net/robot-owner-email: bar@izhcom.rurobot-status: activerobot-purpose: indexing, validationrobot-type: standalonerobot-platform: unixrobot-availability: source, binaryrobot-exclusion: yesrobot-exclusion-useragent: UdmSearchrobot-noindex: yesrobot-host: *robot-from: norobot-useragent: UdmSearch/2.1.1robot-language: crobot-description: UdmSearch is a free web search engine software for intranet/small domain internet serversrobot-history: Developed since 1998, origin purpose is a search engine over republic of Udmurtia http://search.udm.netrobot-environment: hobbymodified-date: Mon, 6 Sep 1999 10:28:52 GMTrobot-id: uptimebotrobot-name: UptimeBotrobot-cover-url: http://www.uptimebot.com", "robot-description": " UptimeBot is a web crawler that checks return codes of web servers and calculates average number of current servers status. The robot runs daily, and visits sites in a random order.", "robot-environment": " research", "modified-by": " UptimeBot team", "robot-status": " active", "robot-owner-name": " UCO team", "robot-useragent": "uptimebot", "robot-from": " no", "robot-purpose": " indexing, statistics", "robot-exclusion-useragent": " no", "robot-id": " udmsearch", "robot-exclusion": " uptimebot", "robot-name": " UdmSearchrobot-details-url: http://mysearch.udm.net/", "robot-availability": " none", "robot-details-url": " http://www.uptimebot.com", "robot-type": " standalone", "modified-date": " Sat, 19 March 2004 21:19:03 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.uptimebot.com", "robot-host": " uptimebot.com", "robot-language": " c++", "robot-platform": " unix", "robot-owner-email": " luft_master@ukr.net"}, "ComputingSite Robi/": {"robot-history": " It was born on August 1997.", "robot-cover-url": " http://www.computingsite.com/robi/", "robot-description": " Intelligent agent used to build the ComputingSite Search Directory.", "robot-environment": " service", "modified-by": " Jorge Alegre", "robot-status": " Active", "robot-owner-name": " Tecor Communications S.L.", "robot-useragent": "ComputingSite Robi/", "robot-from": "", "robot-purpose": " indexing,maintenance", "robot-exclusion-useragent": " robi", "robot-id": " robi", "robot-exclusion": " yes", "robot-name": " ComputingSite Robi/1.0", "robot-availability": "", "robot-details-url": " http://www.computingsite.com/robi/", "robot-type": " standalone", "modified-date": " Wed, 13 May 1998 17:28:52 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.tecor.com/", "robot-host": " robi.computingsite.com", "robot-language": " python", "robot-platform": " UNIX", "robot-owner-email": " robi@computingsite.com"}, "BlackWidow": {"robot-history": "", "robot-cover-url": "    http://140.190.65.12/~khooghee/index.html", "robot-description": "  Started as a research project and now is used to find linksfor a random link generator.  Also is used to research thegrowth of specific sites.", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Kevin Hoogheem", "robot-useragent": "BlackWidow", "robot-from": "         yes", "robot-purpose": "      indexing, statistics", "robot-exclusion-useragent": "", "robot-id": "           blackwidow", "robot-exclusion": "    no", "robot-name": "         BlackWidow", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Fri Feb  9 00:11:22 1996.", "robot-noindex": "", "robot-owner-url": "", "robot-host": "         140.190.65.*", "robot-language": "     C, C++.", "robot-platform": "", "robot-owner-email": "  khooghee@marys.smumn.edu"}, "bbot/": {"robot-history": "                  Started project in 11/2000. Called BBot since 24/04/2003.", "robot-cover-url": "                http://www.otthon.net/search", "robot-description": "              Mainly intended for site level search, sometimes set loose.", "robot-environment": "              hobby", "modified-by": "                    Istvan Fulop", "robot-status": "                   development", "robot-owner-name": "               Istvan Fulop", "robot-useragent": "bbot/", "robot-from": "                     yes", "robot-purpose": "                  indexing, maintenance", "robot-exclusion-useragent": "      bbot", "robot-id": " robot-name: bayspider", "robot-exclusion": "                yes", "robot-name": "                     BBot", "robot-availability": "             none", "robot-details-url": "              http://www.otthon.net/search/bbot", "robot-type": "                     standalone", "modified-date": "                  Sun, 04 May 2003 10:15:00 GMT", "robot-noindex": "                  yesrobot-nofollow:yes", "robot-owner-url": "                http://www.otthon.net", "robot-host": "                     *.netcologne.de", "robot-language": "                 perl", "robot-platform": "                 windows", "robot-owner-email": "              poluf1 at yahoo dot co dot uk"}, "UCSD-Crawler": {"robot-history": "", "robot-cover-url": "    http://www.mib.org/~ucsdcrawl", "robot-description": "  Should hit ONLY within UC San Diego - trying to countservers here.", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Adam Tilghman", "robot-useragent": "UCSD-Crawler", "robot-from": "         yes", "robot-purpose": "      indexing, statistics", "robot-exclusion-useragent": "", "robot-id": "           ucsd", "robot-exclusion": "    yes", "robot-name": "         UCSD Crawl", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sat Jan 27 09:21:40 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.mib.org/~atilghma", "robot-host": "         nuthaus.mib.org scilib.ucsd.edu", "robot-language": "     Perl 4", "robot-platform": "", "robot-owner-email": "  atilghma@mib.org"}, "Big Brother": {"robot-history": "", "robot-cover-url": " http://pauillac.inria.fr/~fpottier/mac-soft.html.en", "robot-description": " Macintosh-hosted link validation tool.", "robot-environment": " shareware", "modified-by": " Francois Pottier", "robot-status": " active", "robot-owner-name": " Francois Pottier", "robot-useragent": "Big Brother", "robot-from": " not as of 1.0", "robot-purpose": " maintenance", "robot-exclusion-useragent": "", "robot-id": " bigbrother", "robot-exclusion": " no", "robot-name": " Big Brother", "robot-availability": " binary", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Thu Sep 19 18:01:46 MET DST 1996", "robot-noindex": " no", "robot-owner-url": " http://pauillac.inria.fr/~fpottier/", "robot-host": " *", "robot-language": " c++", "robot-platform": " mac", "robot-owner-email": " Francois.Pottier@inria.fr"}, "PackRat/": {"robot-history": " In the making...", "robot-cover-url": " http://web.cps.msu.edu/~dexterte/isl/packrat.html", "robot-description": " Used for local maintenance and for gathering web pages sothat local statisistical info can be used in artificial intelligence programs.         Funded by NEMOnline.", "robot-environment": " research", "modified-by": " Terry Dexter", "robot-status": " development", "robot-owner-name": " Terry Dexter", "robot-useragent": "PackRat/", "robot-from": " ", "robot-purpose": " both maintenance and mirroring", "robot-exclusion-useragent": " packrat or *", "robot-id": " packrat", "robot-exclusion": " yes ", "robot-name": " Pack Rat", "robot-availability": "  at the moment, none...source when developed.", "robot-details-url": " ", "robot-type": " standalone", "modified-date": " Tue, 20 Aug 1996 15:45:11", "robot-noindex": " no, not yet", "robot-owner-url": " http://web.cps.msu.edu/~dexterte", "robot-host": " cps.msu.edu", "robot-language": " perl with libwww-5.0", "robot-platform": " unix", "robot-owner-email": " dexterte@cps.msu.edu"}, "LinkWalker": {"robot-history": " Constructed late 1997 through April 1998. In full service April 1998.", "robot-cover-url": " http://www.seventwentyfour.com", "robot-description": " LinkWalker generates a database of links. We send reports of bad ones to webmasters.", "robot-environment": " service", "modified-by": " Roy Bryant", "robot-status": " active", "robot-owner-name": " Roy Bryant", "robot-useragent": "LinkWalker", "robot-from": " yes", "robot-purpose": " maintenance, statistics", "robot-exclusion-useragent": " linkwalker", "robot-id": " linkwalker", "robot-exclusion": " yes", "robot-name": " LinkWalker", "robot-availability": " none", "robot-details-url": " http://www.seventwentyfour.com/tech.html", "robot-type": " standalone", "modified-date": " Wed, 22 Apr 1998", "robot-noindex": " yes", "robot-owner-url": " ", "robot-host": " *.seventwentyfour.com", "robot-language": " c++", "robot-platform": " windowsNT", "robot-owner-email": " rbryant@seventwentyfour.com"}, "FastCrawler": {"robot-history": " Robot started in April 1999", "robot-cover-url": " http://www.1klik.dk/omos/", "robot-description": " FastCrawler is used to build the databases for search engines used by 1klik.dk and it's partners", "robot-environment": " commercial", "modified-by": " Kim Gam-Jensen", "robot-status": " active", "robot-owner-name": " 1klik.dk A/S", "robot-useragent": "FastCrawler", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " fastcrawler", "robot-id": " fastcrawler", "robot-exclusion": " yes", "robot-name": " FastCrawler", "robot-availability": " none", "robot-details-url": " http://www.1klik.dk/omos/", "robot-type": " standalone", "modified-date": " 05-08-2001", "robot-noindex": " yes", "robot-owner-url": " http://www.1klik.dk", "robot-host": " 1klik.dk", "robot-language": " C++", "robot-platform": " Windows 2000 Adv. Server", "robot-owner-email": " crawler@1klik.dk"}, "Digger/": {"robot-history": "", "robot-cover-url": " http://www.diggit.com/", "robot-description": " indexing web sites for the Diggit! search engine", "robot-environment": " service", "modified-by": "", "robot-status": " active", "robot-owner-name": " Benjamin Lipchak", "robot-useragent": "Digger/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " digger", "robot-id": " digger", "robot-exclusion": " yes", "robot-name": " Digger", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": "", "robot-noindex": " yes", "robot-owner-url": "", "robot-host": "", "robot-language": " java", "robot-platform": " unix, windows", "robot-owner-email": " admin@bulldozersoftware.com"}, "PortalJuice.com/": {"robot-history": " Indexing the web since 1998 for the purposes of offering our commerical Portal Juice search engine services.", "robot-cover-url": " http://www.portaljuice.com", "robot-description": " Indexing web documents for Portal Juice vertical portal search engine", "robot-environment": " service", "modified-by": " pjspider@portaljuice.com", "robot-status": " active", "robot-owner-name": " Nextopia Software Corporation", "robot-useragent": "PortalJuice.com/", "robot-from": " yes", "robot-purpose": " indexing, statistics", "robot-exclusion-useragent": " pjspider", "robot-id": " pjspider", "robot-exclusion": " yes", "robot-name": " Portal Juice Spider", "robot-availability": " none", "robot-details-url": " http://www.portaljuice.com/pjspider.html", "robot-type": " standalone", "modified-date": " Wed Jun 23 17:00:00 EST 1999", "robot-noindex": " yes", "robot-owner-url": " http://www.portaljuice.com", "robot-host": " *.portaljuice.com, *.nextopia.com", "robot-language": " C/C++", "robot-platform": " unix", "robot-owner-email": " pjspider@portaljuice.com"}, "AURESYS/": {"robot-history": " This robot finds its roots in a research project at the  University of Marseille in 1995-1996", "robot-cover-url": " http://crrm.univ-mrs.fr", "robot-description": " The AURESYS is used to build a personnal database for  somebody who search information. The database is structured to be  analysed. AURESYS can found new server by IP incremental. It generate  statistics... ", "robot-environment": " used for Research", "modified-by": " Mannina Bruno", "robot-status": " robot actively in use", "robot-owner-name": " Mannina Bruno ", "robot-useragent": "AURESYS/", "robot-from": " Yes", "robot-purpose": " indexing,statistics", "robot-exclusion-useragent": "  ", "robot-id": " auresys", "robot-exclusion": " Yes", "robot-name": " AURESYS", "robot-availability": " Protected by Password", "robot-details-url": " http://crrm.univ-mrs.fr      ", "robot-type": " Standalone", "modified-date": " Mon, 1 Jul 1996 14:30:00 GMT ", "robot-noindex": " no", "robot-owner-url": " ftp://crrm.univ-mrs.fr/pub/CVetud/Etudiants/Mannina/CVbruno.htm        ", "robot-host": " crrm.univ-mrs.fr, 192.134.99.192", "robot-language": " Perl 5.001m", "robot-platform": " Aix, Unix", "robot-owner-email": " mannina@crrm.univ-mrs.fr     "}, "whatUseek_winona/": {"robot-history": " Winona was developed in November of 1996.", "robot-cover-url": " http://www.whatUseek.com/", "robot-description": " The whatUseek robot, Winona, is used for site-level search engines.  It is also implemented in several meta-search engines.", "robot-environment": " service", "modified-by": " Neil Mansilla", "robot-status": " active", "robot-owner-name": " Neil Mansilla", "robot-useragent": "whatUseek_winona/", "robot-from": " no", "robot-purpose": " Robot used for site-level search and meta-search engines.", "robot-exclusion-useragent": " winona", "robot-id": " whatuseek", "robot-exclusion": " yes", "robot-name": " whatUseek Winona", "robot-availability": " none", "robot-details-url": " http://www.whatUseek.com/", "robot-type": " standalone", "modified-date": " Wed, 17 Jan 2001 11:52:00 EST", "robot-noindex": " yes", "robot-owner-url": " http://www.whatUseek.com/", "robot-host": " *.whatuseek.com, *.aol2.com", "robot-language": " c++", "robot-platform": " unix", "robot-owner-email": " neil@whatUseek.com"}, "Cusco/": {"robot-history": " The Cusco search engine started in the company ViaTecla as a project to demonstrate our development capabilities and to fill the need of a portuguese-specific search engine. Now, we are developping new functionalities that cannot be found in any other on-line search engines.", "robot-cover-url": " http://www.cusco.pt/", "robot-description": " The Cusco robot is part of the CUCE indexing sistem. It gathers information from several sources: HTTP, Databases or filesystem. At this moment, it's universe is the .pt domain and the information it gathers is available at the Portuguese search engine Cusco http://www.cusco.pt/.", "robot-environment": "service, research", "modified-by": " Filipe Costa Clerigo", "robot-status": " active", "robot-owner-name": " Filipe Costa Clerigo", "robot-useragent": "Cusco/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " cusco", "robot-id": " cusco", "robot-exclusion": " yes", "robot-name": " Cusco", "robot-availability": " none", "robot-details-url": " http://www.cusco.pt/", "robot-type": " standlone", "modified-date": " Mon, 21 Jun 1999 14:00:00 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.viatecla.pt/", "robot-host": " *.cusco.pt, *.viatecla.pt", "robot-language": " Java", "robot-platform": " any", "robot-owner-email": " clerigo@viatecla.pt"}, "MuscatFerret/": {"robot-history": "", "robot-cover-url": " http://www.muscat.co.uk/euroferret/", "robot-description": " Used to build the database for the EuroFerret <URL:http://www.muscat.co.uk/euroferret/>", "robot-environment": " service", "modified-by": " olly@muscat.co.uk", "robot-status": " active", "robot-owner-name": " Olly Betts", "robot-useragent": "MuscatFerret/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " MuscatFerret", "robot-id": " muscatferret", "robot-exclusion": " yes", "robot-name": " Muscat Ferret", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Tue, 21 May 1997 17:11:00 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.muscat.co.uk/~olly/", "robot-host": " 193.114.89.*, 194.168.54.11", "robot-language": " c, perl5", "robot-platform": " unix", "robot-owner-email": " olly@muscat.co.uk"}, "Openfind data gatherer": {"robot-history": "", "robot-cover-url": "http://www.openfind.com.tw/", "robot-description": "", "robot-environment": "", "modified-by": "stanislav shalunov <shalunov@internet2.edu>", "robot-status": "active", "robot-owner-name": "", "robot-useragent": "Openfind data gatherer", "robot-from": "", "robot-purpose": "indexing", "robot-exclusion-useragent": "", "robot-id": "openfind", "robot-exclusion": "yes", "robot-name": "Openfind data gatherer", "robot-availability": "", "robot-details-url": "http://www.openfind.com.tw/robot.html", "robot-type": "standalone", "modified-date": "Thu, 26 Apr 2001 02:55:21 GMT", "robot-noindex": "", "robot-owner-url": "", "robot-host": "66.7.131.132", "robot-language": "", "robot-platform": "", "robot-owner-email": "robot-response@openfind.com.tw"}, "Tarantula/": {"robot-owner-name": " Markus Hoevener", "robot-details-url": " http://www.nathan.de/", "robot-language": " C", "robot-type": " standalone", "robot-useragent": "Tarantula/", "robot-cover-url": " http://www.nathan.de/nathan/software.html#TARANTULA", "robot-noindex": " yes", "robot-from": " no", "robot-purpose": " indexing", "robot-availability": " none", "robot-id": "tarantula", "robot-exclusion-useragent": " yes", "robot-description": " Tarantual gathers information for german search engine Nathanrobot-history: Started February 1997", "robot-platform": " unix", "robot-exclusion": " yes", "robot-owner-url": "", "robot-host": " yes", "robot-owner-email": " Markus.Hoevener@evision.de", "robot-name": " Tarantula", "robot-status": " development"}, "Shagseeker": {"robot-history": "none yet", "robot-cover-url": "http://www.shagseek.com", "robot-description": "Shagseeker is the gatherer for the Shagseek.com search  engine and goes out weekly.", "robot-environment": "service", "modified-by": "Joseph Reynolds", "robot-status": "active", "robot-owner-name": "Joseph Reynolds", "robot-useragent": "Shagseeker", "robot-from": "", "robot-purpose": "indexing", "robot-exclusion-useragent": "Shagseeker", "robot-id": "shaggy", "robot-exclusion": "yes", "robot-name": "ShagSeeker", "robot-availability": "data", "robot-details-url": "", "robot-type": "standalone", "modified-date": "Mon 17 Jan 2000 10:00:00 EST", "robot-noindex": "yes", "robot-owner-url": "http://www.shagseek.com", "robot-host": "shagseek.com", "robot-language": "perl5", "robot-platform": "unix", "robot-owner-email": "joe.reynolds@shagseek.com"}, "TechBOT": {"robot-history": " TechBOT started his life as a Page Change Detection robot, but has taken on many new and exciting roles.", "robot-cover-url": " http://www.techaid.net/", "robot-description": " TechBOT is constantly upgraded. Currently he is used for Link Validation, Load Time, HTML Validation and much much more.", "robot-environment": " service", "modified-by": " techbot@techaid.net", "robot-status": " active", "robot-owner-name": " TechAID Internet Services", "robot-useragent": "TechBOT", "robot-from": " yes", "robot-purpose": "statistics, maintenance", "robot-exclusion-useragent": " TechBOT", "robot-id": " techbot", "robot-exclusion": " yes", "robot-name": " TechBOT", "robot-availability": " none", "robot-details-url": " http://www.echaid.net/TechBOT/", "robot-type": " standalone", "modified-date": " Sat, 18 Dec 1998 14:26:00 EST", "robot-noindex": " yes", "robot-owner-url": " http://www.techaid.net/", "robot-host": " techaid.net", "robot-language": " perl5", "robot-platform": " Unix", "robot-owner-email": " techbot@techaid.net"}, "web robot PEGASUS": {"robot-history": " This robot was created as an implementation of a final project on Informatics Engineering Department, Institute of Technology Bandung, Indonesia.", "robot-cover-url": " http://opensource.or.id/projects.html", "robot-description": " pegasus gathers information from HTML pages (7 important tags). The indexing process can be started based on starting URL(s) or a range of IP address.", "robot-environment": " research", "modified-by": " A.Y.Kiky Shannon", "robot-status": " inactive - open source", "robot-owner-name": " A.Y.Kiky Shannon", "robot-useragent": "web robot PEGASUS", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " pegasus", "robot-id": " pegasus", "robot-exclusion": " yes", "robot-name": " pegasus", "robot-availability": " source, binary", "robot-details-url": " http://pegasus.opensource.or.id", "robot-type": " standalone", "modified-date": " Fri, 20 Oct 2000 14:58:40 GMT", "robot-noindex": " yes", "robot-owner-url": " http://go.to/ayks", "robot-host": " *", "robot-language": " perl5", "robot-platform": " unix", "robot-owner-email": " shannon@opensource.or.id"}, "Confuzzledbot/": {"robot-history": " Developed 2000-2002. Only minor changes recently ", "robot-cover-url": " http://www.blue.lu/", "robot-description": " The robot is used to build a searchable database for luxembourgish sites. It only indexes .lu domains and luxembourgish sites added to the directory.", "robot-environment": " hobby", "modified-by": " Britz Thibaut", "robot-status": " development", "robot-owner-name": " Britz Thibaut", "robot-useragent": "Confuzzledbot/", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " confuzzledbot", "robot-id": " confuzzledbot", "robot-exclusion": " yes", "robot-name": " ConfuzzledBot", "robot-availability": " none", "robot-details-url": " http://bot.confuzzled.lu/", "robot-type": " standalone", "modified-date": " Tue, 11 May 2004 17:45:00 CET", "robot-noindex": " yesrobot-nofollow: yes", "robot-owner-url": " http://www.confuzzled.lu/", "robot-host": " *.ion.lu", "robot-language": " perl5", "robot-platform": " Linux,Freebsd", "robot-owner-email": " bot@confuzzled.lu"}, "Snooper/": {"robot-history": "", "robot-cover-url": " http://darsun.sit.qc.ca", "robot-description": "", "robot-environment": "", "modified-by": "", "robot-status": " part under development and part active", "robot-owner-name": " Isabelle A. Melnick", "robot-useragent": "Snooper/", "robot-from": "", "robot-purpose": "", "robot-exclusion-useragent": " snooper", "robot-id": " snooper", "robot-exclusion": " yes", "robot-name": " Snooper", "robot-availability": " none", "robot-details-url": "", "robot-type": "", "modified-date": "", "robot-noindex": "", "robot-owner-url": "", "robot-host": "", "robot-language": "", "robot-platform": "", "robot-owner-email": " melnicki@sit.ca"}, "Occam/": {"robot-history": " The robot is a descendant of Rodney,               an earlier project at the University of Washington.", "robot-cover-url": " http://www.cs.washington.edu/research/projects/ai/www/occam/", "robot-description": " The robot takes high-level queries, breaks them down into                multiple web requests, and answers them by combining disparate                data gathered in one minute from numerous web sites, or from                the robots cache.  Currently the only user is me.", "robot-environment": " research", "modified-by": " friedman@cs.washington.edu (Marc Friedman)", "robot-status": " development", "robot-owner-name": " Marc Friedman", "robot-useragent": "Occam/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " Occam", "robot-id": " occam", "robot-exclusion": " yes", "robot-name": " Occam", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Thu, 21 Nov 1996 20:30 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.cs.washington.edu/homes/friedman/", "robot-host": " gentian.cs.washington.edu, sekiu.cs.washington.edu, saxifrage.cs.washington.edu", "robot-language": " CommonLisp, perl4", "robot-platform": " unix", "robot-owner-email": " friedman@cs.washington.edu"}, "NEC-MeshExplorer": {"robot-history": " Prototype version of this robot was developed in C&C Research Laboratories, NEC Corporation. Current robot (Version 1.0) is based on the prototype and has more functions.", "robot-cover-url": "http://netplaza.biglobe.or.jp/", "robot-description": "The NEC-MeshExplorer robot is used to build database for the NETPLAZA search service operated by NEC Corporation. The robot searches URLs around sites in japan(JP domain). The robot runs every day, and visits sites in a random order.", "robot-environment": "research", "modified-by": "Nobuya Kubo, Hajime Takano", "robot-status": "active", "robot-owner-name": "web search service maintenance group", "robot-useragent": "NEC-MeshExplorer", "robot-from": "yes", "robot-purpose": "indexing", "robot-exclusion-useragent": "NEC-MeshExplorer", "robot-id": "meshexplorer", "robot-exclusion": "yes", "robot-name": "NEC-MeshExplorer", "robot-availability": "none", "robot-details-url": "http://netplaza.biglobe.or.jp/keyword.html", "robot-type": "standalone", "modified-date": "Jan 1, 1997", "robot-noindex": "no", "robot-owner-url": "http://netplaza.biglobe.or.jp/keyword.html", "robot-host": "meshsv300.tk.mesh.ad.jp", "robot-language": "c", "robot-platform": "unix", "robot-owner-email": "web-dir@mxa.meshnet.or.jp"}, "TITAN/": {"robot-history": "      ", "robot-cover-url": "    http://isserv.tas.ntt.jp/chisho/titan-e.html", "robot-description": "  Its purpose is to generate a Resource Discovery    database, and copy document trees. Our primary goal is to develop    an advanced method for indexing the WWW documents. Uses libwww-perl", "robot-environment": "", "modified-by": "        Yoshihiko HAYASHI", "robot-status": "       active", "robot-owner-name": "   Yoshihiko HAYASHI", "robot-useragent": "TITAN/", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           titan", "robot-exclusion": "    yes", "robot-name": "         TITAN", "robot-availability": " no", "robot-details-url": "  http://isserv.tas.ntt.jp/chisho/titan-help/eng/titan-help-e.html", "robot-type": "         standalone", "modified-date": "      Mon Jun 24 17:20:44 PDT 1996", "robot-noindex": "      no", "robot-owner-url": "    ", "robot-host": "         nlptitan.isl.ntt.jp", "robot-language": "     perl 4", "robot-platform": "     SunOS 4.1.4", "robot-owner-email": "  hayashi@nttnly.isl.ntt.jp"}, "SimBot/": {"robot-history": " This robot is a part of simmany service and simmini products. The simmini is the Web products that make use of the indexing and retrieving modules of simmany.", "robot-cover-url": " http://simmany.hnc.net/", "robot-description": " The Simmany Robot is used to build the Map(DB) for the simmany service operated by HNC(Hangul & Computer Co., Ltd.). The robot runs weekly, and visits sites that have a useful korean information in a defined order.", "robot-environment": " service, commercial", "modified-by": " Youngsik, Lee", "robot-status": " development & active", "robot-owner-name": " Youngsik, Lee(@L?5=D)", "robot-useragent": "SimBot/", "robot-from": " no", "robot-purpose": " indexing, maintenance, statistics", "robot-exclusion-useragent": " SimBot", "robot-id": " simbot", "robot-exclusion": " yes", "robot-name": " Simmany Robot Ver1.0", "robot-availability": " none", "robot-details-url": " http://simmany.hnc.net/irman1.html", "robot-type": " standalone", "modified-date": " Thu, 19 Sep 1996 07:02:26 GMT", "robot-noindex": " no ", "robot-owner-url": "", "robot-host": " sansam.hnc.net", "robot-language": " C", "robot-platform": " unix", "robot-owner-email": " ailove@hnc.co.kr"}, "Golem/": {"robot-history": " Personal project turned into a contract service for private  clients.", "robot-cover-url": " http://www.quibble.com/golem/", "robot-description": " Golem generates status reports on collections of URLs  supplied by clients. Designed to assist with editorial updates of  Web-related sites or products.", "robot-environment": " service,research", "modified-by": " Geoff Duncan", "robot-status": " active", "robot-owner-name": " Geoff Duncan", "robot-useragent": "Golem/", "robot-from": " yes", "robot-purpose": " maintenance", "robot-exclusion-useragent": " golem", "robot-id": " golem", "robot-exclusion": " yes", "robot-name": " Golem", "robot-availability": " none", "robot-details-url": " http://www.quibble.com/golem/", "robot-type": " standalone", "modified-date": " Wed, 16 Apr 1997 20:50:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.quibble.com/geoff/", "robot-host": " *.quibble.com", "robot-language": " HyperTalk/AppleScript/C++", "robot-platform": " mac", "robot-owner-email": " geoff@quibble.com"}, "EbiNess/": {"robot-history": "Dreamed it up over some beers", "robot-cover-url": "http://sourceforge.net/projects/ebiness", "robot-description": "Used to build a url relationship database, to be viewed in 3D", "robot-environment": "hobby", "modified-by": "Mike Davis", "robot-status": "Pre-Alpha", "robot-owner-name": "Mike Davis", "robot-useragent": "EbiNess/", "robot-from": "no", "robot-purpose": "statistics", "robot-exclusion-useragent": "ebiness", "robot-id": "ebiness", "robot-exclusion": "yes", "robot-name": "EbiNess", "robot-availability": "Open Source", "robot-details-url": "http://ebiness.sourceforge.net/", "robot-type": "standalone", "modified-date": "Mon, 27 Nov 2000 12:26:00 GMT", "robot-noindex": "no", "robot-owner-url": "http://www.carisbrook.co.uk/mike", "robot-host": "", "robot-language": "c++", "robot-platform": "unix(Linux)", "robot-owner-email": "mdavis@kieser.net"}, "ArchitextSpider": {"robot-history": "      ", "robot-cover-url": "    http://www.excite.com/", "robot-description": "  Its purpose is to generate a Resource Discovery database,and to generate statistics. The ArchitextSpider collectsinformation for the Excite and WebCrawler search engines.", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Architext Software", "robot-useragent": "ArchitextSpider", "robot-from": "         yes", "robot-purpose": "      indexing, statistics", "robot-exclusion-useragent": "", "robot-id": "           architext", "robot-exclusion": "    yes", "robot-name": "         ArchitextSpider", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Tue Oct  3 01:10:26 1995", "robot-noindex": "      no", "robot-owner-url": "    http://www.atext.com/spider.html", "robot-host": "         *.atext.com", "robot-language": "     perl 5 and c", "robot-platform": "     ", "robot-owner-email": "  spider@atext.com"}, "WebFetcher/": {"robot-history": "", "robot-cover-url": "    http://www.ontv.com/", "robot-description": "  don't wait! OnTV's WebFetcher mirrors whole sites down toyour hard disk on a TV-like schedule. Catch w3documentation. Catch discovery.com without waiting! A fullyoperational web robot for NT/95 today, most UNIX soon, MACtomorrow.", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "", "robot-useragent": "WebFetcher/", "robot-from": "         yes", "robot-purpose": "      mirroring", "robot-exclusion-useragent": "", "robot-id": "           webfetcher", "robot-exclusion": "    no", "robot-name": "         webfetcher", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sat Jan 27 10:31:43 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.ontv.com/", "robot-host": "         *", "robot-language": "     C++", "robot-platform": "", "robot-owner-email": "  webfetch@ontv.com"}, "Lockon/": {"robot-history": "This robot was developed in the Tokyo university of information sciences in 1998.", "robot-cover-url": "", "robot-description": "This robot gathers only HTML document.", "robot-environment": "research", "modified-by": "Seiji Sasazuka & Takahiro Ohmori", "robot-status": "active", "robot-owner-name": "Seiji Sasazuka & Takahiro Ohmori", "robot-useragent": "Lockon/", "robot-from": "yes", "robot-purpose": "indexing", "robot-exclusion-useragent": "Lockon", "robot-id": "lockon", "robot-exclusion": "yes", "robot-name": "Lockon", "robot-availability": "none", "robot-details-url": "", "robot-type": "standalone", "modified-date": "Tue. 10 Nov 1998 20:00:00 GMT", "robot-noindex": "yes", "robot-owner-url": "", "robot-host": "*.hitech.tuis.ac.jp", "robot-language": "perl5 ", "robot-platform": "UNIX", "robot-owner-email": "search@rsch.tuis.ac.jp"}, "WebBandit/": {"robot-history": "Inspired by reading of Internet Programming book by Jamsa/Cope ", "robot-cover-url": "http://pw2.netcom.com/~wooger/", "robot-description": "multithreaded, hyperlink-following, resource finding webspider ", "robot-environment": "commercial ", "modified-by": "Jerry Walsh", "robot-status": "active", "robot-owner-name": "Jerry Walsh", "robot-useragent": "WebBandit/", "robot-from": "no", "robot-purpose": "Resource Gathering / Server Benchmarking", "robot-exclusion-useragent": "WebBandit/1.0", "robot-id": "webbandit", "robot-exclusion": "no", "robot-name": "WebBandit Web Spider", "robot-availability": "source, binary", "robot-details-url": "http://pw2.netcom.com/~wooger/", "robot-type": "standalone application", "modified-date": "11/21/96", "robot-noindex": "no", "robot-owner-url": "http://pw2.netcom.com/~wooger/", "robot-host": "ix.netcom.com", "robot-language": "C++", "robot-platform": "Intel - windows95", "robot-owner-email": "wooger@ix.netcom.com"}, "SafetyNet Robot": {"robot-history": "", "robot-cover-url": "    http://www.urlabs.com/", "robot-description": "  Finds URLs for K-12 content management.", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Michael L. Nelson", "robot-useragent": "SafetyNet Robot", "robot-from": "         yes", "robot-purpose": "      indexing.", "robot-exclusion-useragent": "", "robot-id": "           safetynetrobot", "robot-exclusion": "    no.", "robot-name": "         SafetyNet Robot", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sat Mar 23 20:12:39 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.urlabs.com/", "robot-host": "         *.urlabs.com", "robot-language": "     Perl 5", "robot-platform": "", "robot-owner-email": "  m.l.nelson@urlabs.com"}, "WebQuest/": {"robot-history": "The developent of WebQuest was motivated by the need for a customized robot in various projects of COSMO Information & Communication Co., Ltd. in Korea.", "robot-cover-url": "", "robot-description": "WebQuest will be used to build the databases for various web search service sites which will be in service by early 1998. Until the end of Jan. 1998, WebQuest will run from time to time. Since then, it will run daily(for few hours and very slowly).", "robot-environment": "service  ", "modified-by": "TaeYoung Choi", "robot-status": "development", "robot-owner-name": "TaeYoung Choi", "robot-useragent": "WebQuest/", "robot-from": "yes", "robot-purpose": "indexing", "robot-exclusion-useragent": "webquest", "robot-id": "webquest", "robot-exclusion": "yes", "robot-name": "WebQuest", "robot-availability": "none", "robot-details-url": "", "robot-type": "standalone", "modified-date": "Tue, 30 Dec 1997 09:27:20 GMT", "robot-noindex": "no", "robot-owner-url": "http://www.cosmocyber.co.kr:8080/~cty/index.html", "robot-host": "210.121.146.2, 210.113.104.1, 210.113.104.2", "robot-language": "perl5", "robot-platform": "unix", "robot-owner-email": "cty@cosmonet.co.kr"}, "PGP-KA/": {"robot-history": "      Originated as a research project at Salerno                     University in 1995.", "robot-cover-url": "    http://www.starnet.it/pgp", "robot-description": "  This program search the pgp public key for the                     specified user.", "robot-environment": "  Research", "modified-by": "        Massimiliano Pucciarelli", "robot-status": "       Active", "robot-owner-name": "   Massimiliano Pucciarelli", "robot-useragent": "PGP-KA/", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           pka", "robot-exclusion": "    no", "robot-name": "         PGP Key Agent", "robot-availability": " none", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      June 27 1996.", "robot-noindex": "      no", "robot-owner-url": "    http://www.starnet.it/puma", "robot-host": "         salerno.starnet.it", "robot-language": "     Perl 5", "robot-platform": "     UNIX, Windows NT", "robot-owner-email": "  puma@comm2000.it"}, "Emacs-w3/": {"robot-history": "      ", "robot-cover-url": "    http://www.cs.indiana.edu/elisp/w3/docs.html", "robot-description": "  Its purpose is to generate a Resource Discovery databaseThis code has not been looked at in a while, but will bespruced up for the Emacs-w3 2.2.0 release sometime thismonth. It will honor the /robots.txt file at thattime.", "robot-environment": "", "modified-by": "", "robot-status": "       retired", "robot-owner-name": "   William M. Perry", "robot-useragent": "Emacs-w3/", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           emacs", "robot-exclusion": "    no", "robot-name": "         Emacs-w3 Search Engine", "robot-availability": " ", "robot-details-url": "", "robot-type": "         browser", "modified-date": "      Fri May 5 16:09:18 1995", "robot-noindex": "      no", "robot-owner-url": "    http://www.cs.indiana.edu/hyplan/wmperry.html", "robot-host": "         *", "robot-language": "     lisp", "robot-platform": "     ", "robot-owner-email": "  wmperry@spry.com"}, "Solbot/": {"robot-history": " This robot is the result of a 3 years old late night hack when the Verity robot (of that time) was unable to index sites with iso8859 characters (in URL and other places), and we just _had_ to have something up and going the next day...", "robot-cover-url": " http://kvasir.sol.no/", "robot-description": "Builds data for the Kvasir search service.  Only searches sites which ends with one of the following domains: no, se, dk, is, fi", "robot-environment": " service", "modified-by": " Frank Tore Johansen <ftj@sys.sol.no>", "robot-status": " active", "robot-owner-name": " Frank Tore Johansen", "robot-useragent": "Solbot/", "robot-from": "", "robot-purpose": " indexing", "robot-exclusion-useragent": " solbot", "robot-id": " solbot", "robot-exclusion": " yes", "robot-name": " Solbot", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Tue Apr  7 16:25:05 MET DST 1998", "robot-noindex": " yes", "robot-owner-url": "", "robot-host": " robot*.sol.no", "robot-language": " perl, c", "robot-platform": " unix", "robot-owner-email": " ftj@sys.sol.no"}, "Nomad-V2": {"robot-history": "      Developed in 1995 at Colorado State University.", "robot-cover-url": "    http://www.cs.colostate.edu/~sonnen/projects/nomad.html", "robot-description": "", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Richard Sonnen", "robot-useragent": "Nomad-V2", "robot-from": "         no", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           nomad", "robot-exclusion": "    no", "robot-name": "         Nomad", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sat Jan 27 21:02:20 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.cs.colostate.edu/~sonnen/", "robot-host": "         *.cs.colostate.edu", "robot-language": "     Perl 4", "robot-platform": "", "robot-owner-email": "  sonnen@cs.colostat.edu"}, "EIT-Link-Verifier-Robot/": {"robot-history": "      Announced on 12 July 1994", "robot-cover-url": "    http://wsk.eit.com/wsk/dist/doc/admin/webtest/verify_links.html", "robot-description": "  Combination of an HTML form and a CGI script that verifieslinks from a given starting point (with some controls toprevent it going off-site or limitless)", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Jim McGuire", "robot-useragent": "EIT-Link-Verifier-Robot/", "robot-from": "         ", "robot-purpose": "      maintenance", "robot-exclusion-useragent": "", "robot-id": "           eit", "robot-exclusion": "    ", "robot-name": "         EIT Link Verifier Robot", "robot-availability": " ", "robot-details-url": "", "robot-type": "         ", "modified-date": "      ", "robot-noindex": "      no", "robot-owner-url": "    http://www.eit.com/people/mcguire.html", "robot-host": "         *", "robot-language": "     ", "robot-platform": "     ", "robot-owner-email": "  mcguire@eit.COM"}, "image.kapsi.net/": {"robot-history": " The Robot was build for image.kapsi.net's database in year 2001.", "robot-cover-url": " http://image.kapsi.net/", "robot-description": " The image.kapsi.net robot is used to build the database for the image.kapsi.net search service. The robot runs currently in a random times.", "robot-environment": " hobby, research", "modified-by": "", "robot-status": " development", "robot-owner-name": " Jaakko Heusala", "robot-useragent": "image.kapsi.net/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " image.kapsi.net", "robot-id": " kapsi", "robot-exclusion": " yes", "robot-name": " image.kapsi.net", "robot-availability": " data", "robot-details-url": " http://image.kapsi.net/index.php?page=robot", "robot-type": " standalone", "modified-date": " Thu, 13 Dec 2001 23:28:23 EET", "robot-noindex": " no", "robot-owner-url": " http://huoh.kapsi.net/", "robot-host": " addr-212-50-142-138.suomi.net", "robot-language": " perl", "robot-platform": " unix", "robot-owner-email": " Jaakko.Heusala@kapsi.net"}, "inspectorwww/": {"robot-history": "  development started in Mar 1997", "robot-cover-url": " http://www-cse.ucsd.edu/users/fil/agents/agents.htmlrobot-owner-name: Filippo Menczerrobot-owner-url: http://www-cse.ucsd.edu/users/fil/robot-owner-email: fil@cs.ucsd.edurobot-status: developmentrobot-purpose: searchrobot-type: standalonerobot-platform: unix, macrobot-availability: nonerobot-exclusion: yesrobot-exclusion-useragent: InfoSpidersrobot-noindex: norobot-host: *.ucsd.edurobot-from: yesrobot-useragent: InfoSpiders/0.1robot-language: c, perl5robot-description: application of artificial life algorithm to adaptive distributed information retrievalrobot-history: UC San Diego, Computer Science Dept. PhD research project (1995-97) under supervision of Prof. Rik Belewrobot-environment: researchmodified-date: Mon, 16 Sep 1996 14:08:00 PDTrobot-id:  inspectorwwwrobot-name:  Inspector Webrobot-cover-url:  http://www.greenpac.com/inspector/", "robot-description": "  Provide inspection reports which give advise to WWW site owners on missing links, images resize problems, syntax errors, etc.", "robot-environment": "  commercial", "modified-by": "  Doug Green", "robot-status": "  active:  robot significantly developed, but still undergoing fixes", "robot-owner-name": "  Doug Green", "robot-useragent": "inspectorwww/", "robot-from": "  yes", "robot-purpose": "  maintentance:  link validation, html validation, image size validation, etc", "robot-exclusion-useragent": "  inspectorwww", "robot-id": " infospider", "robot-exclusion": "  yes", "robot-name": " InfoSpiders", "robot-availability": "  free service and more extensive commercial service", "robot-details-url": "  http://www.greenpac.com/inspector/ourrobot.html", "robot-type": "  standalone", "modified-date": "  Tue Jun 17 09:24:58 EST 1997", "robot-noindex": "  no", "robot-owner-url": "  http://www.greenpac.com", "robot-host": "  www.corpsite.com, www.greenpac.com, 38.234.171.*", "robot-language": "  c", "robot-platform": " unix", "robot-owner-email": "  doug@greenpac.com"}, "Duppies": {"robot-history": "", "robot-cover-url": "    http://www.maxum.com/phantom/", "robot-description": "  Designed to allow webmasters to provide a searchable indexof their own site as well as to other sites, perhaps withsimilar content.", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Larry Burke", "robot-useragent": "Duppies", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           phantom", "robot-exclusion": "    yes", "robot-name": "         Phantom", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Fri Jan 19 05:08:15 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.aktiv.com/", "robot-host": "", "robot-language": "", "robot-platform": "     Macintosh", "robot-owner-email": "  lburke@aktiv.com"}, "Robbie/": {"robot-history": " The DISCO system is a resource-discovery component in               the OLLA system, which is a prototype system, developed               under DARPA funding, to support computer-based education               and training.", "robot-cover-url": " http://stage.perceval.be (under developpement)", "robot-description": " Used to define document collections for the DISCO system.                   Robbie is still under development and runs several                   times a day, but usually only for ten minutes or so.                   Sites are visited in the order in which references                   are found, but no host is visited more than once in                   any two-minute period.", "robot-environment": " research", "modified-by": "", "robot-status": " developmentrobot-purpose1: indexingrobot-purpose2: maintenancerobot-purpose3: statisticsrobot-type: standalonerobot-platform1: unix (FreeBSD & Linux)robot-availability: nonerobot-exclusion: no (under development)robot-exclusion-useragent: RHCSrobot-noindex: no (under development)robot-host: stage.perceval.berobot-from: norobot-useragent: RHCS/1.0arobot-language: crobot-description: robot used tp build the database for the RoadHouse search service project operated by Perceval  robot-history: The need of this robot find its roots in the actual RoadHouse directory not maintenained since 1997robot-environment: servicemodified-date: Fri, 26 Feb 1999 12:00:00 GMTmodified-by: Gregoire Welraedsrobot-id: rixbotrobot-name: RixBotrobot-cover-url: http://www.oops-as.no/rixrobot-details-url: http://www.oops-as.no/roy/rixrobot-owner-name: HYrobot-owner-url: http://www.oops-as.no/royrobot-status: active", "robot-owner-name": " Gregoire Welraeds, Emmanuel Bergmans", "robot-useragent": "Robbie/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " Robbie", "robot-id": " rhcs", "robot-exclusion": " yes", "robot-name": " RoadHouse Crawling System", "robot-availability": " none", "robot-details-url": "", "robot-type": "standalone", "modified-date": " Wed,  5 Feb 1997 19:00:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.perceval.be", "robot-host": " *.lmco.com", "robot-language": " java", "robot-platform": "macrobot-exclusion: yesrobot-exclusion-useragent: RixBotrobot-noindex: yesrobot-nofollow: yesrobot-host: www.oops-as.norobot-from: norobot-useragent: RixBot (http://www.oops-as.no/rix/)robot-language: REBOLrobot-description: The RixBot indexes any page containing the word rebol.robot-history: Hobby projectrobot-environment: Hobbymodified-date: Fri, 14 May 2004 19:58:52 GMTrobot-id: roadrunnerrobot-name: Road Runner: The ImageScape Robotrobot-owner-name: LIM Grouprobot-owner-email: lim@cs.leidenuniv.nlrobot-status: development/activerobot-purpose: indexingrobot-type: standalonerobot-platform: UNIXrobot-exclusion: yesrobot-exclusion-useragent: roadrunnerrobot-useragent: Road Runner: ImageScape Robot (lim@cs.leidenuniv.nl)robot-language: C, perl5robot-description: Create Image/Text index for WWWrobot-history: ImageScape Projectrobot-environment: commercial servicemodified-date: Dec. 1st, 1996robot-id: robbierobot-name: Robbie the Robotrobot-cover-url:robot-details-url:robot-owner-name: Robert H. Pollackrobot-owner-url:robot-owner-email: robert.h.pollack@lmco.comrobot-status: developmentrobot-purpose: indexingrobot-type: standalonerobot-platform: unix, windows95, windowsNT", "robot-owner-email": " helpdesk@perceval.be"}, "MediaFox/": {"robot-history": " Project at the University of Siegen", "robot-cover-url": " none", "robot-description": " The robot is used to index meta information of a                   specified set of documents and update a database                   accordingly.", "robot-environment": " research", "modified-by": " Lars Eilebrecht", "robot-status": " development", "robot-owner-name": " Lars Eilebrecht   ", "robot-useragent": "MediaFox/", "robot-from": " yes", "robot-purpose": " indexing and maintenance", "robot-exclusion-useragent": " mediafox", "robot-id": " mediafox", "robot-exclusion": " yes", "robot-name": " MediaFox", "robot-availability": " none", "robot-details-url": " none", "robot-type": " standalone", "modified-date": " Fri Aug 14 03:37:56 CEST 1998", "robot-noindex": " yes", "robot-owner-url": " http://www.home.unix-ag.org/sfx/", "robot-host": " 141.99.*.*", "robot-language": " Java", "robot-platform": " (Java)", "robot-owner-email": " sfx@uni-media.de"}, "WOLP/": {"robot-history": "", "robot-cover-url": " http://www.suchfibel.de/maschinisten", "robot-description": " The robot gathers information about specified web-projects and generates knowledge bases in Javascript or an own formatrobot-environment: hobbymodified-date: 22 Jul 1998modified-by: Marius Dahlerrobot-id:           wombatrobot-name:         The Web Wombat robot-cover-url:    http://www.intercom.com.au/wombat/robot-details-url:robot-owner-name:   Internet Communicationsrobot-owner-url:    http://www.intercom.com.au/robot-owner-email:  phill@intercom.com.aurobot-status:robot-purpose:      indexing, statistics.robot-type:robot-platform:robot-availability:robot-exclusion:    no.robot-exclusion-useragent:robot-noindex:robot-host:         qwerty.intercom.com.aurobot-from:         norobot-useragent:    norobot-language:     IBM Rexx/VisualAge C++ under OS/2.robot-description:  The robot is the basis of the Web Wombat search engine(Australian/New Zealand content ONLY).", "robot-environment": "", "modified-by": "", "robot-status": " active", "robot-owner-name": " Marius Dahler", "robot-useragent": "WOLP/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " WOLP", "robot-id": " wolp", "robot-exclusion": " yes", "robot-name": " WebStolperer", "robot-availability": " none", "robot-details-url": " http://www.suchfibel.de/maschinisten/text/werkzeuge.htm (in German)", "robot-type": " standalone", "modified-date": "      Thu Feb 29 00:39:49 1996.", "robot-noindex": " yes", "robot-owner-url": " http://www.suchfibel.de/maschinisten", "robot-host": " www.suchfibel.de", "robot-language": " perl5", "robot-platform": " unix, NT", "robot-owner-email": " mda@suchfibel.de"}, "IncyWincy/": {"robot-history": "", "robot-cover-url": "    http://osiris.sunderland.ac.uk/sst-scripts/simon.html", "robot-description": "  Various Research projects at the University ofSunderland", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Simon Stobart", "robot-useragent": "IncyWincy/", "robot-from": "         yes", "robot-purpose": "", "robot-exclusion-useragent": "", "robot-id": "           incywincy", "robot-exclusion": "    yes", "robot-name": "         IncyWincy", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Fri Jan 19 21:50:32 1996.", "robot-noindex": "", "robot-owner-url": "    http://osiris.sunderland.ac.uk/sst-scripts/simon.html", "robot-host": "         osiris.sunderland.ac.uk", "robot-language": "     C++", "robot-platform": "", "robot-owner-email": "  simon.stobart@sunderland.ac.uk"}, "Checkbot/": {"robot-history": "      ", "robot-cover-url": "    http://www.xs4all.nl/~graaff/checkbot/", "robot-description": "  Checkbot checks links in agiven set of pages on one or more servers. It reports linkswhich returned an error code", "robot-environment": "  hobby", "modified-by": "        Hans de Graaff", "robot-status": "       active", "robot-owner-name": "   Hans de Graaff", "robot-useragent": "Checkbot/", "robot-from": "         no", "robot-purpose": "      maintenance", "robot-exclusion-useragent": "", "robot-id": "           checkbot", "robot-exclusion": "    no", "robot-name": "         Checkbot", "robot-availability": " source", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Tue Jun 25 07:44:00 1996", "robot-noindex": "      no", "robot-owner-url": "    http://www.xs4all.nl/~graaff/checkbot/", "robot-host": "         *", "robot-language": "     perl 5", "robot-platform": "     unix,WindowsNT", "robot-owner-email": "  graaff@xs4all.nl"}, "FunnelWeb": {"robot-history": "      ", "robot-cover-url": " http://euroseek.net/robot-owner-name: Jesper Ekhallrobot-owner-email: ekhall@freeside.netrobot-status: activerobot-purpose: indexingrobot-type: standalonerobot-platform: unixrobot-availability: nonerobot-exclusion: yesrobot-exclusion-useragent: Freecrawlrobot-noindex: norobot-host: *.freeside.netrobot-from: yesrobot-useragent: Freecrawlrobot-language: crobot-description: The Freecrawl robot is used to build a database for the  EuroSeek service.robot-environment: servicerobot-id:           funnelwebrobot-name:         FunnelWebrobot-cover-url:    http://funnelweb.net.au", "robot-description": "  Its purpose is to generate a Resource Discovery database,and generate statistics. Localised South Pacific Discoveryand Search Engine, plus distributed operation underdevelopment.", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   David Eagles", "robot-useragent": "FunnelWeb", "robot-from": "         yes", "robot-purpose": "      indexing, statisitics", "robot-exclusion-useragent": "", "robot-id": " freecrawl", "robot-exclusion": "    yes", "robot-name": " Freecrawl", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Mon Nov 27 21:30:11 1995", "robot-noindex": "      no", "robot-owner-url": "    http://www.pc.com.au", "robot-host": "         earth.planets.com.au", "robot-language": "     c and c++", "robot-platform": "     ", "robot-owner-email": "  eaglesd@pc.com.au"}, "LWP::": {"robot-history": " -", "robot-cover-url": " http://www.thatrobotsite.com/agents/ecollector.htm", "robot-description": "e-collector in the simplist terms is a e-mail address collector, thus the name e-collector. So what? Have you ever wanted to have the email addresses of as many companys that sell or supply for example dried fruit, i personnaly don't but this is just an example. Those of you who may use this type of robot will know exactly what you can do with information, first don't spam with it,  for those still not sure what this type of robot will do for you then take this for example: Your a international distributer of dried fruit and you boss has told you if you rise sales by 10% then he will bye you a new car (Wish i had a boss like that), well anyway there are thousands of shops distributers ect, that you could be doing business with but you don't know who they are?, because there in other countries or the nearest town but have never heard about them before.  Has the penny droped yet, no well now you have the opertunity to find out who they are with an internet address and a person to contact in that company just by downloading and running e-collector. Plus it's free,  you don't have to do any leg work just run the program and sit back and watch your potential customers arriving.", "robot-environment": " Service", "modified-by": " Dean Smart", "robot-status": " Active", "robot-owner-name": " Dean Smart", "robot-useragent": "LWP::", "robot-from": " No", "robot-purpose": " email collector", "robot-exclusion-useragent": " ecollector", "robot-id": " e-collector", "robot-exclusion": " No", "robot-name": " e-collector", "robot-availability": " Binary", "robot-details-url": " http://www.thatrobotsite.com/agents/ecollector.htm", "robot-type": " Collector of email addresses", "modified-date": " Weekly", "robot-noindex": " No", "robot-owner-url": " http://www.thatrobotsite.com", "robot-host": " *", "robot-language": " Perl5", "robot-platform": " Windows 9*/NT/2000", "robot-owner-email": " smarty@thatrobotsite.com"}, "Googlebot/": {"robot-history": " Developed by Google Inc", "robot-cover-url": " http://www.googlebot.com/ ", "robot-description": " Google's crawler", "robot-environment": " commercial", "modified-by": " googlebot@google.com", "robot-status": " active", "robot-owner-name": " Google Inc.", "robot-useragent": "Googlebot/", "robot-from": " yes ", "robot-purpose": " indexing", "robot-exclusion-useragent": " googlebot", "robot-id": " googlebot", "robot-exclusion": " yes", "robot-name": " Googlebot", "robot-availability": " none", "robot-details-url": " http://www.googlebot.com/bot.html", "robot-type": " standalone ", "modified-date": " Thu Mar 29 21:00:07 PST 2001", "robot-noindex": " yes", "robot-owner-url": " http://www.google.com/", "robot-host": " googlebot.com", "robot-language": " c++", "robot-platform": " Linux", "robot-owner-email": " googlebot@google.com "}, "webwalk": {"robot-history": "      ", "robot-cover-url": "    ", "robot-description": "  Its purpose is to generate a Resource Discovery database,validate links, validate HTML, perform mirroring, copydocument trees, and generate statistics. Webwalk is easilyextensible to perform virtually any maintenance functionwhich involves web traversal, in a way much like the '-exec'option of the find(1) command. Webwalk is usually usedbehind the HP firewall", "robot-environment": "", "modified-by": "", "robot-status": "       retired", "robot-owner-name": "   Rich Testardi", "robot-useragent": "webwalk", "robot-from": "         yes", "robot-purpose": "      indexing, maintentance, mirroring, statistics", "robot-exclusion-useragent": "", "robot-id": "           webwalk", "robot-exclusion": "    yes", "robot-name": "         webwalk", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Wed Nov 15 09:51:59 PST 1995", "robot-noindex": "      no", "robot-owner-url": "    ", "robot-host": "         ", "robot-language": "     c", "robot-platform": "     ", "robot-owner-email": "  "}, "Evliya Celebi": {"robot-history": "      Started in 1995 to provide a comprehensive index                    to WWW pages within New Zealand. Now also used in                    Malaysia and other countries.", "robot-cover-url": " http://search.falconsoft.com/robot-owner-name: Tim Gustafsonrobot-owner-url: http://www.falconsoft.com/robot-owner-email:      tim@falconsoft.comrobot-status: activerobot-purpose: indexingrobot-type: standalonerobot-platform: unix (FreeBSD 2.2.8)robot-availability: datarobot-exclusion: yesrobot-exclusion-useragent: estherrobot-noindex: norobot-host: *.falconsoft.comrobot-from: yesrobot-useragent: estherrobot-language: perl5robot-description: This crawler is used to build the search database at http://search.falconsoft.com/robot-history: Developed by FalconSoft.robot-environment: servicemodified-date: Tue, 22 Dec 1998 00:22:00 PSTrobot-id: evliyacelebirobot-name: Evliya Celebirobot-cover-url: http://ilker.ulak.net.tr/EvliyaCelebi", "robot-description": "crawles pages under .tr domain or having turkish character encoding (iso-8859-9 or windows-1254)robot-environment: hobbymodified-date: Fri Mar 31 15:03:12 GMT 2000robot-id:           nzexplorerrobot-name:         nzexplorerrobot-cover-url:    http://nzexplorer.co.nz/robot-details-url:robot-owner-name:   Paul Bourkerobot-owner-url:    http://bourke.gen.nz/paul.htmlrobot-owner-email:  paul@bourke.gen.nzrobot-status:       activerobot-purpose:      indexing, statisticsrobot-type:         standalonerobot-platform:     UNIXrobot-availability: source (commercial)robot-exclusion:    norobot-exclusion-useragent:robot-noindex:      norobot-host:         bitz.co.nzrobot-from:         norobot-useragent:    explorersearchrobot-language:     c++", "robot-environment": "  service", "modified-by": "        Paul Bourke", "robot-status": " development", "robot-owner-name": " Ilker TEMIR", "robot-useragent": "Evliya Celebi", "robot-from": " ilker@ulak.net.tr", "robot-purpose": " indexing turkish content", "robot-exclusion-useragent": " N/A", "robot-id": " esther", "robot-exclusion": " yes", "robot-name": " Estherrobot-details-url: http://search.falconsoft.com/", "robot-availability": " source", "robot-details-url": " http://ilker.ulak.net.tr/EvliyaCelebi", "robot-type": " standalone", "modified-date": "      Tues, 25 Jun 1996", "robot-noindex": " norobot-nofollow: no", "robot-owner-url": " http://ilker.ulak.net.tr", "robot-host": " 193.140.83.*", "robot-language": " perl5robot-history:", "robot-platform": " unix", "robot-owner-email": " ilker@ulak.net.tr"}, "LinkScan Server/": {"robot-history": " First developed by Elsop in January,1997", "robot-cover-url": "http://www.elsop.com/", "robot-description": "LinkScan checks links, validates HTML and creates site maps", "robot-environment": "Commercial", "modified-by": " Kenneth R. Churilla", "robot-status": "Robot actively in use", "robot-owner-name": "Electronic Software Publishing Corp. (Elsop)", "robot-useragent": "LinkScan Server/", "robot-from": "", "robot-purpose": "Link checker, SiteMapper, and HTML Validator", "robot-exclusion-useragent": "", "robot-id": "linkscan", "robot-exclusion": "No", "robot-name": "LinkScan", "robot-availability": "Program is shareware", "robot-details-url": "http://www.elsop.com/linkscan/overview.html", "robot-type": "Standalone", "modified-date": "Fri, 3 September 1999 17:00:00 PDT", "robot-noindex": "Yes", "robot-owner-url": "http://www.elsop.com/", "robot-host": "*", "robot-language": "perl5", "robot-platform": "Unix, Linux, Windows 98/NT", "robot-owner-email": "sales@elsop.com"}, "TitIn/": {"robot-history": "        It was done as result of desperate need for central index of        Croatian web servers in December 1996.", "robot-cover-url": " http://www.foi.hr/~dpavlin/titin/", "robot-description": "        The TitIn is used to index all titles of Web server in        .hr domain.", "robot-environment": " research", "modified-by": " Dobrica Pavlinusic", "robot-status": " development", "robot-owner-name": " Dobrica Pavlinusic", "robot-useragent": "TitIn/", "robot-from": " no", "robot-purpose": " indexing, statistics", "robot-exclusion-useragent": " titin", "robot-id": " titin", "robot-exclusion": " yes", "robot-name": " TitIn", "robot-availability": " data, source on request", "robot-details-url": " http://www.foi.hr/~dpavlin/titin/tehnical.htm", "robot-type": " standalone", "modified-date": " Thu, 12 Dec 1996 16:06:42 MET", "robot-noindex": " no", "robot-owner-url": " http://www.foi.hr/~dpavlin/", "robot-host": " barok.foi.hr", "robot-language": " perl5, c", "robot-platform": " unix", "robot-owner-email": " dpavlin@foi.hr"}, "Spider": {"robot-history": " Alcorkn (Madrid) - Europa 2000/2001", "robot-cover-url": " http://www.cienciaficcion.net/", "robot-description": " Robot encargado de la indexacin de las pginas para www.cienciaficcion.net", "robot-environment": " hobby", "modified-by": " David Fernndez", "robot-status": " active", "robot-owner-name": " David Fernndez", "robot-useragent": "Spider", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": "", "robot-id": " cienciaficcion", "robot-exclusion": " no", "robot-name": " cIeNcIaFiCcIoN.nEt", "robot-availability": " none", "robot-details-url": " http://www.cienciaficcion.net/", "robot-type": " standalone", "modified-date": " Sat, 18 Aug 2001 00:38:52 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.cyberdark.net/", "robot-host": " epervier.cqhost.net", "robot-language": " php,perl", "robot-platform": " linux", "robot-owner-email": " root@cyberdark.net"}, "XGET/": {"robot-owner-name": " Hiroyuki Shigenaga", "robot-details-url": " http://www2.117.ne.jp/~moremore/x68000/soft/soft.html", "robot-language": " c", "robot-type": " standalone", "robot-useragent": "XGET/", "robot-cover-url": " http://www.imaginon.comrobot-owner-name: ImaginOn, Incrobot-owner-url: http://www.imaginon.comrobot-owner-email: info@imaginon.comrobot-status: activerobot-purpose: indexingrobot-type: standalonerobot-platform: windows95, windowsNT 4, mac, solaris, unixrobot-availability: binaryrobot-exclusion: norobot-exclusion-useragent: nonerobot-noindex: norobot-host: http://www.imaginon.com/wzindex.html *robot-from: norobot-useragent: nonerobot-language: javarobot-description: commercial Web Bot that accepts plain text queries, uses webcrawler, lycos or excite to get URLs, then visits sites.  If the user's filter parameters are met, downloads one picture and a paragraph of test. Playsback slide show format of one text paragraph plus image from each site.robot-history: developed by ImaginOn in 1996 and 1997robot-environment: commercialmodified-date: Wed, 11 Sep 1997 02:00:00 GMTmodified-by: schwartz@imaginon.comrobot-id: xgetrobot-name: XGETrobot-cover-url: http://www2.117.ne.jp/~moremore/x68000/soft/soft.html", "robot-noindex": " no", "robot-from": " yes", "robot-purpose": " mirroring", "robot-availability": " binary", "robot-id": " wz101", "robot-exclusion-useragent": " XGET", "robot-description": " Its purpose is to retrieve updated files.It is run by the end userrobot-history: 1997", "robot-platform": " X68000, X68030", "robot-exclusion": " yes", "robot-owner-url": " http://www2.117.ne.jp/~moremore/", "robot-host": " *", "robot-owner-email": " shige@mh1.117.ne.jp", "robot-name": " WebZingerrobot-details-url: http://www.imaginon.com/wzindex.html", "robot-status": " active"}, "Templeton/": {"robot-history": " This robot was originally created as a test-of-concept.", "robot-cover-url": " http://www.bmtmicro.com/catalog/tton/", "robot-description": " Templeton is a very configurable robots for mirroring, mapping, and automating applications on retrieved documents.", "robot-environment": " service, commercial, research, hobby", "modified-by": " Neal Krawetz", "robot-status": " active", "robot-owner-name": " Neal Krawetz", "robot-useragent": "Templeton/", "robot-from": " yes", "robot-purpose": " mirroring, mapping, automating web applications", "robot-exclusion-useragent": " templeton", "robot-id": " templeton", "robot-exclusion": " yes", "robot-name": " Templeton", "robot-availability": " binary", "robot-details-url": " http://www.bmtmicro.com/catalog/tton/", "robot-type": " standalone", "modified-date": " Sun, 6 Apr 1997 10:00:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.cs.tamu.edu/people/nealk/", "robot-host": " *", "robot-language": " C", "robot-platform": " OS/2, Linux, SunOS, Solaris", "robot-owner-email": " nealk@net66.com"}, "AraybOt/": {"robot-history": " ", "robot-cover-url": " http://www.araykoo.com/", "robot-description": " AraybOt is the agent software of AraykOO! which crawls web sites listed in http://dmoz.org/Adult/, in order to build a adult search engine.", "robot-environment": " service", "modified-by": " Guti", "robot-status": " active", "robot-owner-name": " Guti", "robot-useragent": "AraybOt/", "robot-from": " no", "robot-purpose": " indexing maintenance", "robot-exclusion-useragent": " AraybOt", "robot-id": " araybot", "robot-exclusion": " yes", "robot-name": " AraybOt", "robot-availability": " none", "robot-details-url": " http://www.araykoo.com/araybot.html", "robot-type": " standalone", "modified-date": " Sat, 19 Jun 2004 20:25:00 GMT+1", "robot-noindex": " yes", "robot-owner-url": " http://www.araykoo.com/", "robot-host": " *", "robot-language": " perl5", "robot-platform": " Linux", "robot-owner-email": " robot@araykoo.com"}, "ParaSite/": {"robot-history": " Second generation of ianett.com spidering technology, originally called Sven.", "robot-cover-url": " http://www.ianett.com/parasite/", "robot-description": " Builds index for ianett.com search database. Runs continiously.", "robot-environment": " service", "modified-by": " Marty Anstey", "robot-status": " active", "robot-owner-name": " iaNett.com", "robot-useragent": "ParaSite/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " ParaSite", "robot-id": " parasite", "robot-exclusion": " yes", "robot-name": " ParaSite", "robot-availability": " none", "robot-details-url": " http://www.ianett.com/parasite/", "robot-type": " standalone", "modified-date": " July 28, 2000", "robot-noindex": " yesrobot-nofollow: yes", "robot-owner-url": " http://www.ianett.com/", "robot-host": " *.ianett.com", "robot-language": " c++", "robot-platform": " windowsNT", "robot-owner-email": " parasite@ianett.com"}, "Slurp/": {"robot-history": " Switch from Slurp/1.0 to Slurp/2.0 November 1996", "robot-cover-url": " http://www.inktomi.com/", "robot-description": " Indexing documents for the HotBot search engine(www.hotbot.com), collecting Web statistics", "robot-environment": " service", "modified-by": " slurp@inktomi.com", "robot-status": " active", "robot-owner-name": " Inktomi Corporation", "robot-useragent": "Slurp/", "robot-from": " yes", "robot-purpose": " indexing, statistics", "robot-exclusion-useragent": " slurp", "robot-id": " slurp", "robot-exclusion": " yes", "robot-name": " Inktomi Slurp", "robot-availability": " none", "robot-details-url": " http://www.inktomi.com/slurp.html", "robot-type": " standalone", "modified-date": " Fri Feb 28 13:57:43 PST 1997", "robot-noindex": " yes", "robot-owner-url": " http://www.inktomi.com/", "robot-host": " *.inktomi.com", "robot-language": " C/C++", "robot-platform": " unix", "robot-owner-email": " slurp@inktomi.com"}, "INGRID/": {"robot-history": "", "robot-cover-url": "", "robot-description": "  ", "robot-environment": "", "modified-by": " Ilse", "robot-status": " Running", "robot-owner-name": " Ilse c.v.", "robot-useragent": "INGRID/", "robot-from": " Yes", "robot-purpose": " Indexing", "robot-exclusion-useragent": " INGRID/0.1", "robot-id": " Ilse", "robot-exclusion": " Yes", "robot-name": " Ingrid", "robot-availability": " Commercial as part of search engine package", "robot-details-url": "", "robot-type": " Web Indexer", "modified-date": " 06/13/1997", "robot-noindex": " Yes", "robot-owner-url": " http://www.ilse.nl/", "robot-host": " bart.ilse.nl", "robot-language": " C", "robot-platform": " UNIX", "robot-owner-email": " ilse@ilse.nl"}, "WWWWanderer": {"robot-history": "      ", "robot-cover-url": "    http://www.mit.edu/people/mkgray/net/", "robot-description": "  Run initially in June 1993, its aim is to measure                    the growth in the web.", "robot-environment": "  research", "modified-by": "", "robot-status": "       active", "robot-owner-name": "   Matthew Gray", "robot-useragent": "WWWWanderer", "robot-from": "         ", "robot-purpose": "      statistics", "robot-exclusion-useragent": "", "robot-id": "           wanderer", "robot-exclusion": "    no", "robot-name": "         the World Wide Web Wanderer", "robot-availability": " data", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      ", "robot-noindex": "      no", "robot-owner-url": "    http://www.mit.edu:8001/people/mkgray/mkgray.html", "robot-host": "         *.mit.edu", "robot-language": "     perl4", "robot-platform": "     unix", "robot-owner-email": "  mkgray@mit.edu"}, "HKU WWW Robot": {"robot-history": "", "robot-cover-url": "    http://phoenix.cs.hku.hk:1234/~jax/w3rui.shtml", "robot-description": "  HKU Octopus is an ongoing project for resource discovery inthe Hong Kong and China WWW domain . It is a researchproject conducted by three undergraduate at the Universityof Hong Kong", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Law Kwok Tung , Lee Tak Yeung , Lo Chun Wing", "robot-useragent": "HKU WWW Robot", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           octopus", "robot-exclusion": "    no.", "robot-name": "         HKU WWW Octopus", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Thu Mar  7 14:21:55 1996.", "robot-noindex": "", "robot-owner-url": "    http://phoenix.cs.hku.hk:1234/~jax", "robot-host": "         phoenix.cs.hku.hk", "robot-language": "     Perl 5, C, Java.", "robot-platform": "", "robot-owner-email": "  jax@cs.hku.hk"}, "NetMechanic": {"robot-history": "", "robot-cover-url": " http://www.netmechanic.com", "robot-description": "  NetMechanic is a link validation and HTML validation robot run using a web page interface.", "robot-environment": "", "modified-by": "", "robot-status": " development", "robot-owner-name": " Tom Dahm", "robot-useragent": "NetMechanic", "robot-from": " no", "robot-purpose": " Link and HTML validation", "robot-exclusion-useragent": " WebMechanic", "robot-id": "  netmechanic", "robot-exclusion": " Yes", "robot-name": "  NetMechanic", "robot-availability": " via web page", "robot-details-url": " http://www.netmechanic.com/faq.html", "robot-type": " standalone with web gateway", "modified-date": " Sat, 17 Aug 1996 12:00:00 GMT", "robot-noindex": " no", "robot-owner-url": "  http://iquest.com/~tdahm", "robot-host": " 206.26.168.18", "robot-language": " C", "robot-platform": " UNIX", "robot-owner-email": " tdahm@iquest.com"}, "Deweb/": {"robot-history": "      ", "robot-cover-url": "    http://deweb.orbit.de/", "robot-description": "  Its purpose is to generate a Resource Discovery database,perform mirroring, and generate statistics. Uses combinationof Informix(tm) Database and WN 1.11 serversoftware forindexing/ressource discovery, fulltext search, textexcerpts.", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Marc Mielke", "robot-useragent": "Deweb/", "robot-from": "         yes", "robot-purpose": "      indexing, mirroring, statistics", "robot-exclusion-useragent": "", "robot-id": "           deweb", "robot-exclusion": "    yes", "robot-name": "         DeWeb(c) Katalog/Index", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Wed Jan 10 08:23:00 1996", "robot-noindex": "      no", "robot-owner-url": "    http://www.orbit.de/", "robot-host": "         deweb.orbit.de", "robot-language": "     perl 4", "robot-platform": "     ", "robot-owner-email": "  dewebmaster@orbit.de"}, "Valkyrie/": {"robot-history": " This robot has been used since Oct. 1995 for author's research.", "robot-cover-url": " http://kichijiro.c.u-tokyo.ac.jp/odin/", "robot-description": " used to collect resources from Japanese Web sites for ODIN search engine.", "robot-environment": " service research", "modified-by": " harada@graco.c.u-tokyo.ac.jp", "robot-status": " active", "robot-owner-name": " Masanori Harada", "robot-useragent": "Valkyrie/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " Valkyrie libwww-perl", "robot-id": " valkyrie", "robot-exclusion": " yes", "robot-name": " Valkyrie", "robot-availability": " none", "robot-details-url": " http://kichijiro.c.u-tokyo.ac.jp/odin/robot.html", "robot-type": " standalone", "modified-date": " Thu Mar 20 19:09:56 JST 1997", "robot-noindex": " no", "robot-owner-url": " http://www.graco.c.u-tokyo.ac.jp/~harada/", "robot-host": " *.c.u-tokyo.ac.jp", "robot-language": " perl4", "robot-platform": " unix", "robot-owner-email": " harada@graco.c.u-tokyo.ac.jp"}, "Infoseek Sidewinder": {"robot-history": "", "robot-cover-url": "    http://www.infoseek.com/", "robot-description": "", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Mike Agostino", "robot-useragent": "Infoseek Sidewinder", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           infoseeksidewinder", "robot-exclusion": "    yes", "robot-name": "         Infoseek Sidewinder", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sat Apr 27 01:20:15 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.infoseek.com/", "robot-host": "", "robot-language": "     C Collects WWW pages for both InfoSeek's free WWW searchservices. Uses a unique, incremental, very fast proprietaryalgorithm to find WWW pages. ", "robot-platform": "", "robot-owner-email": "  mna@infoseek.com"}, "Katipo/": {"robot-history": "      ", "robot-cover-url": "    http://www.vuw.ac.nz/~newbery/Katipo.html", "robot-description": "  Watches all the pages you have previously visitedand tells you when they have changed.", "robot-environment": "  commercial (free)", "modified-by": "        Michael Newbery", "robot-status": "       active", "robot-owner-name": "   Michael Newbery", "robot-useragent": "Katipo/", "robot-from": "         yes", "robot-purpose": "      maintenance", "robot-exclusion-useragent": "", "robot-id": "           katipo", "robot-exclusion": "    no", "robot-name": "         Katipo", "robot-availability": " binary", "robot-details-url": "  http://www.vuw.ac.nz/~newbery/Katipo/Katipo-doc.html", "robot-type": "         standalone", "modified-date": "      Tue, 25 Jun 96 11:40:07 +1200", "robot-noindex": "      no", "robot-owner-url": "    http://www.vuw.ac.nz/~newbery", "robot-host": "         *", "robot-language": "     c", "robot-platform": "     Macintosh", "robot-owner-email": "  Michael.Newbery@vuw.ac.nz"}, "DIIbot": {"robot-history": " ", "robot-cover-url": " http://www.digital-integrity.com/robotinfo.html", "robot-description": "", "robot-environment": "", "modified-by": "", "robot-status": " Production", "robot-owner-name": " Digital Integrity, Inc.", "robot-useragent": "DIIbot", "robot-from": "", "robot-purpose": " WWW Indexing", "robot-exclusion-useragent": " DIIbot", "robot-id": " diibot", "robot-exclusion": " Conforms to robots.txt convention", "robot-name": " Digital Integrity Robot", "robot-availability": " none", "robot-details-url": " http://www.digital-integrity.com/robotinfo.html", "robot-type": "", "modified-date": "", "robot-noindex": " Yes", "robot-owner-url": " ", "robot-host": " digital-integrity.com", "robot-language": " Java/C", "robot-platform": " unix", "robot-owner-email": " robot@digital-integrity.com"}, "searchprocess/": {"robot-history": " This is the son of Auresys", "robot-cover-url": " http://www.searchprocess.com", "robot-description": " An intelligent Agent Online. SearchProcess is used to provide structured information to user.", "robot-environment": " Service freeware", "modified-by": " Mannina Bruno", "robot-status": " active", "robot-owner-name": " Mannina Bruno", "robot-useragent": "searchprocess/", "robot-from": " yes", "robot-purpose": " Statistic", "robot-exclusion-useragent": " searchprocess", "robot-id": " searchprocess", "robot-exclusion": " yes", "robot-name": " SearchProcess", "robot-availability": " none", "robot-details-url": " http://www.intelligence-process.com", "robot-type": " browser", "modified-date": " Thus, 22 Dec 1999", "robot-noindex": " yes", "robot-owner-url": " http://www.intelligence-process.com", "robot-host": " searchprocess.com", "robot-language": " perl", "robot-platform": " linux", "robot-owner-email": " bruno@intelligence-process.com"}, "Hmhkki/": {"robot-history": "      (The name Hmhkki is just Finnish for spider.)", "robot-cover-url": "    http://www.fi/search.html", "robot-description": "  Its purpose is to generate a Resource Discoverydatabase from the Finnish (top-level domain .fi) www servers.The resulting database is used by the search engine at http://www.fi/search.html.", "robot-environment": "", "modified-by": "        Jaakko.Hyvatti@www.fi", "robot-status": "       active", "robot-owner-name": "   Timo Metsl", "robot-useragent": "Hmhkki/", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "  Hmhkki", "robot-id": "           finnish", "robot-exclusion": "    yes", "robot-name": "         Hmhkki", "robot-availability": " no", "robot-details-url": "  http://www.fi/www/spider.html", "robot-type": "         standalone", "modified-date": "      1996-06-25   ", "robot-noindex": "      no", "robot-owner-url": "    http://www.fi/~timo/", "robot-host": "         *.www.fi", "robot-language": "     C", "robot-platform": "     UNIX", "robot-owner-email": "  Timo.Metsala@www.fi"}, "VWbot_K/": {"robot-history": " Originally written fall 1995. Actively maintained.", "robot-cover-url": " http://vancouver-webpages.com/VWbot/", "robot-description": " Used to index BC sites for the searchBC database. Runs daily.", "robot-environment": " service commercial research", "modified-by": " Andrew Daviel", "robot-status": " active", "robot-owner-name": " Andrew Daviel", "robot-useragent": "VWbot_K/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " VWbot_K", "robot-id": " vwbot", "robot-exclusion": " yes", "robot-name": " VWbot", "robot-availability": " source", "robot-details-url": " http://vancouver-webpages.com/VWbot/aboutK.shtml", "robot-type": " standalone", "modified-date": " Tue, 4 Mar 1997 20:00:00 GMT", "robot-noindex": " yes", "robot-owner-url": "  http://vancouver-webpages.com/~admin/", "robot-host": " vancouver-webpages.com", "robot-language": " perl4", "robot-platform": " unix", "robot-owner-email": " andrew@vancouver-webpages.com"}, "RoboCrawl": {"robot-history": " Our robot is a newer project at Canadian Content.", "robot-cover-url": " http://www.canadiancontent.net/", "robot-description": " The Canadian Content robot indexes for it's search database.", "robot-environment": " service", "modified-by": " Christopher Walsh and Adam Rutter", "robot-status": " active", "robot-owner-name": " Canadian Content Interactive Media", "robot-useragent": "RoboCrawl", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " RoboCrawl", "robot-id": " robocrawl", "robot-exclusion": " yes", "robot-name": " RoboCrawl Spider", "robot-availability": " none", "robot-details-url": " http://www.canadiancontent.net/corp/spider.html", "robot-type": " standalone", "modified-date": " July 30th, 2001", "robot-noindex": " yes", "robot-owner-url": " http://www.canadiancontent.net/", "robot-host": " ncc.canadiancontent.net, ncc.air-net.no, canadiancontent.net, spider.canadiancontent.net", "robot-language": " C and C++", "robot-platform": " linux", "robot-owner-email": " staff@canadiancontent.net"}, "newscan-online/": {"robot-history": " This robot finds its roots in a prereleased software for news filtering for Lotus Notes in 1995.", "robot-cover-url": " http://www-a2k.is.tokushima-u.ac.jp/search/index.htmlrobot-owner-name: Kenji Kitarobot-owner-url: http://www-a2k.is.tokushima-u.ac.jp/member/kita/index.htmlrobot-owner-email: kita@is.tokushima-u.ac.jprobot-status: activerobot-purpose: indexingrobot-type: standalonerobot-platform: UNIXrobot-availability: nonerobot-exclusion: yesrobot-exclusion-useragent: NetScooprobot-host: alpha.is.tokushima-u.ac.jp, beta.is.tokushima-u.ac.jprobot-useragent: NetScoop/1.0 libwww/5.0arobot-language: Crobot-description: The NetScoop robot is used to build the database                   for the NetScoop search engine.robot-history: The robot has been used in the research project               at the Faculty of Engineering, Tokushima University, Japan.,               since Dec. 1996.robot-environment: researchmodified-date: Fri, 10 Jan 1997.modified-by: Kenji Kitarobot-id: newscan-onlinerobot-name: newscan-onlinerobot-cover-url: http://www.newscan-online.de/", "robot-description": " The newscan-online robot is used to build a database for the newscan-online news search service operated by smart information services. The robot runs daily and visits predefined sites in a random order.", "robot-environment": " service", "modified-by": " Axel Mueller", "robot-status": " active", "robot-owner-name": " Axel Mueller", "robot-useragent": "newscan-online/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " newscan-online", "robot-id": " netscoop", "robot-exclusion": " yes", "robot-name": " NetScoop", "robot-availability": " binary", "robot-details-url": " http://www.newscan-online.de/info.html", "robot-type": " standalone", "modified-date": " Fri, 9 Apr 1999 11:45:00 GMT", "robot-noindex": " no", "robot-owner-url": "", "robot-host": " *newscan-online.de", "robot-language": " perl", "robot-platform": " Linux", "robot-owner-email": " mueller@newscan-online.de"}, "Gulper Web Bot": {"robot-history": " Developed in a research project at SUNY Stony Brook.", "robot-cover-url": " http://yuntis.ecsl.cs.sunysb.edu/", "robot-description": " The Gulper Bot is used to collect data for the Yuntis research search engine project.", "robot-environment": " research", "modified-by": " maxim@cs.sunysb.edu", "robot-status": " active", "robot-owner-name": " Maxim Lifantsev", "robot-useragent": "Gulper Web Bot", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " gulper", "robot-id": " gulperbot", "robot-exclusion": " yes", "robot-name": " Gulper Bot", "robot-availability": " none", "robot-details-url": " http://yuntis.ecsl.cs.sunysb.edu/help/robot/", "robot-type": " standalone", "modified-date": " Tue, 28 Aug 2001 21:40:47 GMT", "robot-noindex": " yesrobot-nofollow: yes", "robot-owner-url": " http://www.cs.sunysb.edu/~maxim/", "robot-host": " yuntis*.ecsl.cs.sunysb.edu", "robot-language": " c++", "robot-platform": " Linux", "robot-owner-email": " gulperbot@ecsl.cs.sunysb.edu"}, "havIndex/": {"robot-history": " Developed to answer client requests for URL specific index capabilities.", "robot-cover-url": " http://www.hav.com/", "robot-description": " havIndex allows individuals to build searchable word index of (user specified) lists of URLs.  havIndex does not crawl - rather it requires  one or more user supplied lists of URLs to be indexed.  havIndex does (optionally) save urls parsed from indexed  pages.", "robot-environment": " commercial, service", "modified-by": " Horace A. (Kicker) Vallas", "robot-status": " active", "robot-owner-name": " hav.Software and Horace A. (Kicker) Vallas", "robot-useragent": "havIndex/", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " havIndex", "robot-id": " havindex", "robot-exclusion": " yes", "robot-name": " havIndex", "robot-availability": " binary", "robot-details-url": " http://www.hav.com/", "robot-type": " standalone", "modified-date": " 6-27-98", "robot-noindex": " yes", "robot-owner-url": " http://www.hav.com/", "robot-host": " *", "robot-language": " Java", "robot-platform": " Java VM 1.1", "robot-owner-email": " havIndex@hav.com"}, "Raven-v2": {"robot-history": " This robot is new. First active on March 25, 2000.", "robot-cover-url": " http://ravensearch.tripod.com", "robot-description": " Raven was written for the express purpose of indexing the web. It can parallel process hundreds of URLS's at a time. It runs on a sporadic basis  as testing continues. It is really several programs running concurrently. It takes four computers to run Raven Search. Scalable in sets of four.", "robot-environment": " Commercial: is a commercial product. Possibly GNU later ;-)", "modified-by": " Raven Group", "robot-status": " Development: robot under development", "robot-owner-name": " Raven Group", "robot-useragent": "Raven-v2", "robot-from": " Yes", "robot-purpose": " Indexing: gather content for commercial query engine.", "robot-exclusion-useragent": " Raven", "robot-id": " raven ", "robot-exclusion": " Yes", "robot-name": " Raven Search", "robot-availability": " None", "robot-details-url": " http://ravensearch.tripod.com", "robot-type": " Standalone: a separate program", "modified-date": " Fri, 25 Mar 2000 17:28:52 GMT", "robot-noindex": " Yesrobot-nofollow: Yes", "robot-owner-url": " http://ravensearch.tripod.com", "robot-host": " 192.168.1.*", "robot-language": " Perl-5", "robot-platform": " Unix, Windows98, WindowsNT, Windows2000", "robot-owner-email": " ravensearch@hotmail.com"}, "KIT-Fireball/": {"robot-history": " The robot was developed by Benhui Chen in a research project at the Technical University of Berlin in 1996 and was re-implemented by its developer in 1997 for the present owner.", "robot-cover-url": " http://www.fireball.de", "robot-description": " The Fireball robots gather web documents in German language for the database of the Fireball search service.", "robot-environment": " service ", "modified-by": " Detlev Kalb", "robot-status": " active", "robot-owner-name": " Gruner + Jahr Electronic Media Service GmbH", "robot-useragent": "KIT-Fireball/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " KIT-Fireball", "robot-id": " fireball", "robot-exclusion": " yes", "robot-name": " KIT-Fireball", "robot-availability": " none", "robot-details-url": " http://www.fireball.de/technik.html (in German)", "robot-type": " standalone", "modified-date": " Mon Feb 23 11:26:08 1998", "robot-noindex": " yes", "robot-owner-url": " http://www.ems.guj.de", "robot-host": " *.fireball.de", "robot-language": " c", "robot-platform": " unix", "robot-owner-email": "info@fireball.de"}, "WebLinker/": {"robot-history": "      ", "robot-cover-url": "    http://www.cern.ch/WebLinker/", "robot-description": "  it traverses a section of web, doing URN->URL conversion.        It will be used as a post-processing tool on documents createdby automatic converters such as LaTeX2HTML or WebMaker. Atthe moment it works at full speed, but is restricted tolocalsites. External GETs will be added, but these will berunning slowly. WebLinker is meant to be run locally, so ifyou see it elsewhere let the author know!", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   James Casey", "robot-useragent": "WebLinker/", "robot-from": "         ", "robot-purpose": "      maintenance", "robot-exclusion-useragent": "", "robot-id": "           weblinker", "robot-exclusion": "    ", "robot-name": "         WebLinker", "robot-availability": " ", "robot-details-url": "", "robot-type": "         ", "modified-date": "      ", "robot-noindex": "      ", "robot-owner-url": "    http://www.maths.tcd.ie/hyplan/jcasey/jcasey.html", "robot-host": "         ", "robot-language": "     ", "robot-platform": "     ", "robot-owner-email": "  jcasey@maths.tcd.ie"}, "dienstspider/": {"robot-history": " The version 1.0 was the developer's master thesis project", "robot-cover-url": " http://sappho.csi.forth.gr:22000/", "robot-description": " Indexing and searching the NCSTRL(Networked Computer Science Technical Report Library) and ERCIM Collection", "robot-environment": " research", "modified-by": " asidirop@csi.forth.gr", "robot-status": " development", "robot-owner-name": " Antonis Sidiropoulos ", "robot-useragent": "dienstspider/", "robot-from": "", "robot-purpose": " indexing", "robot-exclusion-useragent": "", "robot-id": " dienstspider", "robot-exclusion": "", "robot-name": " DienstSpider", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone ", "modified-date": " Fri, 4 Dec 1998 0:0:0 GMT", "robot-noindex": "", "robot-owner-url": " http://www.csi.forth.gr/~asidirop", "robot-host": " sappho.csi.forth.gr ", "robot-language": " C", "robot-platform": " unix", "robot-owner-email": " asidirop@csi.forth.gr"}, "IBM_Planetwide": {"robot-history": "", "robot-cover-url": "    http://www.ibm.com/%7ewebmaster/", "robot-description": "  Restricted to IBM owned or related domains.", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Ed Costello", "robot-useragent": "IBM_Planetwide", "robot-from": "         yes", "robot-purpose": "      indexing, maintenance, mirroring", "robot-exclusion-useragent": "", "robot-id": "           ibm", "robot-exclusion": "    yes", "robot-name": "         IBM_Planetwide", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone and", "modified-date": "      Mon Jan 22 22:09:19 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.ibm.com/%7ewebmaster/", "robot-host": "         www.ibm.com www2.ibm.com", "robot-language": "     Perl5", "robot-platform": "", "robot-owner-email": "epc@www.ibm.com"}, "Internet Cruiser Robot/": {"robot-history": "", "robot-cover-url": " http://www.krstarica.com/", "robot-description": " Internet Cruiser Robot is Internet Cruiser's prime index agent.", "robot-environment": " service", "modified-by": " tech@krstarica.com", "robot-status": " active", "robot-owner-name": " Internet Cruiser", "robot-useragent": "Internet Cruiser Robot/", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " Internet Cruiser Robot", "robot-id": " cruiser", "robot-exclusion": " yes", "robot-name": " Internet Cruiser Robot", "robot-availability": " none", "robot-details-url": " http://www.krstarica.com/eng/url/", "robot-type": " standalone", "modified-date": " Fri, 17 Jan 2001 12:00:00 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.krstarica.com/", "robot-host": " *.krstarica.com", "robot-language": " c++", "robot-platform": " unix", "robot-owner-email": " robot@krstarica.com"}, "Robot du CRIM": {"robot-history": "", "robot-cover-url": "", "robot-description": "  Part of the RISQ's Francoroute project for researchingfrancophone. Uses the Accept-Language tag and reduces demandaccordingly", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Marc-Antoine Parent", "robot-useragent": "Robot du CRIM", "robot-from": "         yes", "robot-purpose": "      indexing, mirroring, statistics", "robot-exclusion-useragent": "", "robot-id": "           francoroute", "robot-exclusion": "    yes", "robot-name": "         Robot Francoroute", "robot-availability": "", "robot-details-url": "", "robot-type": "         browser", "modified-date": "      Wed Jan 10 23:56:22 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.crim.ca/~maparent", "robot-host": "         zorro.crim.ca", "robot-language": "     perl5, sqlplus", "robot-platform": "", "robot-owner-email": "  maparent@crim.ca"}, "fido/": {"robot-history": " fido was originally based on the Harvest Gatherer, but has since               evolved into a new creature.  It still uses some support code               from Harvest.", "robot-cover-url": " http://www.planetsearch.com/", "robot-description": " fido is used to gather documents for the search engine                    provided in the PlanetSearch service, which is operated by                   the Philips Multimedia Center.  The robots runs on an                   ongoing basis.", "robot-environment": " service", "modified-by": " Steve DeJarnett", "robot-status": " active", "robot-owner-name": " Steve DeJarnett", "robot-useragent": "fido/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " fido", "robot-id": " fido", "robot-exclusion": " yes", "robot-name": " fido", "robot-availability": " none", "robot-details-url": " http://www.planetsearch.com/info/fido.html", "robot-type": " standalone", "modified-date": " Sat, 2 Nov 1996 00:08:18 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.planetsearch.com/staff/steved.html", "robot-host": " fido.planetsearch.com, *.planetsearch.com, 206.64.113.*", "robot-language": " c, perl5", "robot-platform": " Unix", "robot-owner-email": " fido@planetsearch.com"}, "jumpstation": {"robot-history": "      Originated as a weekend project in 1993.", "robot-cover-url": "    http://js.stir.ac.uk/jsbin/jsii", "robot-description": "", "robot-environment": "", "modified-by": "", "robot-status": "       retired", "robot-owner-name": "   Jonathon Fletcher", "robot-useragent": "jumpstation", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           jumpstation", "robot-exclusion": "    yes", "robot-name": "         JumpStation", "robot-availability": "", "robot-details-url": "", "robot-type": "", "modified-date": "      Tue May 16 00:57:42 1995.", "robot-noindex": "", "robot-owner-url": "    http://www.stir.ac.uk/~jf1", "robot-host": "         *.stir.ac.uk", "robot-language": "     perl, C, c++", "robot-platform": "", "robot-owner-email": "  j.fletcher@stirling.ac.uk "}, "ABCdatos BotLink/": {"robot-history": " This robot was developed by ABCdatos team to help               working in the directory maintenance.", "robot-cover-url": " http://www.abcdatos.com/", "robot-description": " This robot is used to verify availability of the ABCdatos                   directory entries (http://www.abcdatos.com), checking                   HTTP HEAD. Robot runs twice a week. Under HTTP 5xx                   error responses or unable to connect, it repeats                   verification some hours later, verifiying if that was a                   temporary situation.", "robot-environment": " commercial", "modified-by": " ABCdatos", "robot-status": " active", "robot-owner-name": " ABCdatos", "robot-useragent": "ABCdatos BotLink/", "robot-from": " no", "robot-purpose": " maintenance", "robot-exclusion-useragent": " BotLink", "robot-id": " abcdatos", "robot-exclusion": " no", "robot-name": " ABCdatos BotLink", "robot-availability": " none", "robot-details-url": " http://www.abcdatos.com/botlink/", "robot-type": " standalone", "modified-date": " Thu, 29 May 2003 01:00:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.abcdatos.com/", "robot-host": " 217.126.39.167", "robot-language": " basic", "robot-platform": " windows", "robot-owner-email": " botlink+AEA-abcdatos.com"}, "SiteTech-Rover": {"robot-history": " This robot originally went by the name of LiberTech-Rover", "robot-cover-url": "    http://www.sitetech.com/", "robot-description": "  Originated as part of a suite of Internet Products to        organize, search & navigate Intranet sites and to validate        links in HTML documents.", "robot-environment": "", "modified-by": " Anil Peres-da-Silva", "robot-status": "", "robot-owner-name": "   Anil Peres-da-Silva", "robot-useragent": "SiteTech-Rover", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           sitetech", "robot-exclusion": "    yes", "robot-name": "         SiteTech-Rover", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Fri Aug 9 17:06:56 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.sitetech.com", "robot-host": "", "robot-language": "     C++.", "robot-platform": "", "robot-owner-email": "  adasilva@sitetech.com"}, "UdmSearch": {"robot-history": " Formerly known as UDMSearch was developed as the search  engine for the Russian republic of Udmurtia.", "robot-cover-url": " http://www.mnogosearch.org", "robot-description": " mnoGoSearch search engine software (formerly known as UDMSearch) is an advanced search solution for large-scale websites and Intranet. It is based on SQL database and supports numerous features.", "robot-environment": " commercial", "modified-by": " Dmitry Tkatchenko", "robot-status": " active", "robot-owner-name": " Lavtech.com corp.", "robot-useragent": "UdmSearch", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " udmsearch", "robot-id": " mnogosearch", "robot-exclusion": " yes", "robot-name": " mnoGoSearch search engine software", "robot-availability": " source", "robot-details-url": " http://www.mnogosearch.org/features.html", "robot-type": " standalone", "modified-date": " Wed, 12 Sept 2001", "robot-noindex": " yes", "robot-owner-url": " http://www.mnogosearch.org", "robot-host": " *", "robot-language": " c", "robot-platform": " unix, windows, mac", "robot-owner-email": " support@mnogosearch.org"}, "dlw3robot/": {"robot-history": "      ", "robot-cover-url": "    http://hplyot.obspm.fr/~dl/robo.html", "robot-description": "  Its purpose is to validate links, and generatestatistics.", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Laurent Demailly", "robot-useragent": "dlw3robot/", "robot-from": "         yes", "robot-purpose": "      maintenance, statistics", "robot-exclusion-useragent": "", "robot-id": "           tcl", "robot-exclusion": "    yes", "robot-name": "         Tcl W3 Robot", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Tue May 23 17:51:39 1995", "robot-noindex": "      no", "robot-owner-url": "    http://hplyot.obspm.fr/~dl/", "robot-host": "         hplyot.obspm.fr", "robot-language": "     tcl", "robot-platform": "     ", "robot-owner-email": "  dl@hplyot.obspm.fr"}, "PageBoy/": {"robot-history": "none", "robot-cover-url": "http://www.webdocs.org/", "robot-description": "The robot visits at regular intervals.", "robot-environment": "service", "modified-by": "webdocs", "robot-status": "development", "robot-owner-name": "Chihiro Kuroda ", "robot-useragent": "PageBoy/", "robot-from": "yes", "robot-purpose": "indexing", "robot-exclusion-useragent": "pageboy", "robot-id": "pageboy", "robot-exclusion": "yes", "robot-name": "PageBoy", "robot-availability": "none", "robot-details-url": "http://www.webdocs.org/ ", "robot-type": "standalone", "modified-date": "Fri, 21 Oct 1999 17:28:52 GMT", "robot-noindex": "yesrobot-nofollow:yes", "robot-owner-url": "http://www.webdocs.org/", "robot-host": "*.webdocs.org", "robot-language": "c", "robot-platform": "unix", "robot-owner-email": "pageboy@webdocs.org"}, "KDD-Explorer/": {"robot-history": "          This robot was designed in Knowledge-bases Information                        processing Laboratory, KDD R&D Laboratories, 1996-1997", "robot-cover-url": "        http://mlc.kddvw.kcom.or.jp/CLINKS/html/clinks.html", "robot-description": "      KDD-Explorer is used for indexing valuable documents                which will be retrieved via an experimental cross-language                search engine, CLINKS.", "robot-environment": "      research", "modified-by": "            Kazunori Matsumoto", "robot-status": "           development (to be avtive in June 1997)", "robot-owner-name": "       Kazunori Matsumoto", "robot-useragent": "KDD-Explorer/", "robot-from": "             yes", "robot-purpose": "          indexing", "robot-exclusion-useragent": "KDD-Explorer", "robot-id": "               kdd", "robot-exclusion": "        yes", "robot-name": "             KDD-Explorer", "robot-availability": "     none", "robot-details-url": "      not available", "robot-type": "             standalone", "modified-date": "          Mon, 2 June 1997 18:00:00 JST", "robot-noindex": "          no", "robot-owner-url": "        not available", "robot-host": "             mlc.kddvw.kcom.or.jp", "robot-language": "         c", "robot-platform": "         unix", "robot-owner-email": "      matsu@lab.kdd.co.jp"}, "JCrawler/": {"robot-history": "", "robot-cover-url": " http://www.nihongo.org/jcrawler/", "robot-description": " JCrawler is currently used to build the Vietnam topic                   specific WWW index for VietGATE                   <URL:http://www.vietgate.net/>. It schedules visits                   randomly, but will not visit a site more than once                   every two minutes. It uses a subject matter relevance                   pruning algorithm to determine what pages to crawl                   and index and will not generally index pages with                   no Vietnam related content. Uses Unicode internally,                   and detects and converts several different Vietnamese                   character encodings.", "robot-environment": " service", "modified-by": " Benjamin Franz", "robot-status": " active", "robot-owner-name": " Benjamin Franz", "robot-useragent": "JCrawler/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " jcrawler", "robot-id": " jcrawler", "robot-exclusion": " yes", "robot-name": " JCrawler", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Wed, 08 Oct 1997 00:09:52 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.nihongo.org/snowhare/", "robot-host": " db.netimages.com", "robot-language": " perl5", "robot-platform": " unix", "robot-owner-email": " snowhare@netimages.com"}, "I Robot": {"robot-history": "The robot was started in june 2000robot-environment1: servicerobot-environment2: hobbymodified-date: Fri, 27 Oct 2000 09:08:06 GMTmodified-by: BombJack mameadm@chaos.dkrobot-id:iron33robot-name:Iron33robot-cover-url:http://verno.ueda.info.waseda.ac.jp/iron33/robot-details-url:http://verno.ueda.info.waseda.ac.jp/iron33/history.htmlrobot-owner-name:Takashi Watanaberobot-owner-url:http://www.ueda.info.waseda.ac.jp/~watanabe/robot-owner-email:watanabe@ueda.info.waseda.ac.jprobot-status:activerobot-purpose:indexing, statisticsrobot-type:standalonerobot-platform:unixrobot-availability:sourcerobot-exclusion:yesrobot-exclusion-useragent:Iron33robot-noindex:norobot-host:*.folon.ueda.info.waseda.ac.jp, 133.9.215.*robot-from:yesrobot-useragent:Iron33/0.0robot-language:crobot-description:The robot Iron33 is used to build the                  database for the WWW search engine Verno.robot-history:", "robot-cover-url": " http://irobot.mame.dk/", "robot-description": " I Robot is used to build a fresh database for the emulation community. Primary focus is information on emulation and especially old arcade machines. Primarily english sites will be indexed and only if they have their own domain. Sites are added manually on based on submitions after they has been evaluated.", "robot-environment": "research", "modified-by": "Watanabe Takashi", "robot-status": " active", "robot-owner-name": " [mame.dk]", "robot-useragent": "I Robot", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " irobot", "robot-id": " irobot", "robot-exclusion": " yes", "robot-name": " I, Robot", "robot-availability": " none", "robot-details-url": " http://irobot.mame.dk/about.phtml", "robot-type": " standalone", "modified-date": "Fri, 20 Mar 1998 18:34 JST", "robot-noindex": " yes", "robot-owner-url": " http://www.mame.dk/", "robot-host": " *.mame.dk, 206.161.121.*", "robot-language": " c", "robot-platform": " unix", "robot-owner-email": " irobot@chaos.dk"}, "LabelGrab/": {"robot-history": " N/A", "robot-cover-url": " http://www.w3.org/PICS/refcode/LabelGrabber/index.htm", "robot-description": " The label grabber searches for PICS labels and submits them to a label bureau", "robot-environment": " research", "modified-by": " jamieson@mit.edu", "robot-status": " active", "robot-owner-name": " Kyle Jamieson", "robot-useragent": "LabelGrab/", "robot-from": " no", "robot-purpose": " Grabs PICS labels from web pages, submits them to a label bueau", "robot-exclusion-useragent": " label-grabber", "robot-id": " labelgrabber.txt", "robot-exclusion": " yes", "robot-name": " LabelGrabber", "robot-availability": " source", "robot-details-url": " http://www.w3.org/PICS/refcode/LabelGrabber/index.htm", "robot-type": " standalone", "modified-date": " Wed, 28 Jan 1998 17:32:52 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.w3.org/PICS/refcode/LabelGrabber/index.htm", "robot-host": " head.w3.org", "robot-language": " java", "robot-platform": " windows, windows95, windowsNT, unix", "robot-owner-email": " jamieson@mit.edu"}, "gazz/": {"robot-history": " Its root is TITAN project in NTT.", "robot-cover-url": " http://www.gammasite.comrobot-owner-name: gammasiterobot-owner-url: http://www.gammasite.comrobot-owner-email:support@gammasite.comrobot-status: activerobot-purpose: indexing, maintenancerobot-type: standalonerobot-platform: unix, windows, windows95, windowsNT, linuxrobot-availability: nonerobot-exclusion: yesrobot-exclusion-useragent: gammaSpiderrobot-noindex:norobot-nofollow: norobot-host: *robot-from: norobot-useragent: gammaSpider xxxxxxx ()/robot-language: c++robot-description:  Information gathering.  Focused carwling on specific topic.  Uses gammaFetcherServer  Product for selling.  RobotUserAgent may changed by the user.  More features are being added.  The product is constatnly under development.  AKA FocusedCrawlerrobot-history: AKA FocusedCrawlerrobot-environment: service, commercial, researchmodified-date: Sun, 25 Mar 2001 18:49:52 GMTrobot-id: gazzrobot-name: gazzrobot-cover-url: http://gazz.nttrd.com/", "robot-description": " This robot is used for research purposes.", "robot-environment": " research", "modified-by": " noto@isl.ntt.co.jp", "robot-status": " development", "robot-owner-name": " NTT Cyberspace Laboratories", "robot-useragent": "gazz/", "robot-from": " yes", "robot-purpose": " statistics", "robot-exclusion-useragent": " gazz", "robot-id": "      gama", "robot-exclusion": " yes", "robot-name": " gammaSpider, FocusedCrawlerrobot-details-url: http://www.gammasite.com, http://www.gammasite.com/gammaSpider.html", "robot-availability": " none", "robot-details-url": " http://gazz.nttrd.com/", "robot-type": " standalone", "modified-date": " Wed, 09 Jun 1999 10:43:18 GMT", "robot-noindex": " yes", "robot-owner-url": " http://gazz.nttrd.com/", "robot-host": " *.nttrd.com, *.infobee.ne.jp", "robot-language": " c", "robot-platform": " unix", "robot-owner-email": " gazz@nttrd.com"}, "Roverbot": {"robot-history": "", "robot-cover-url": " http://dmoz.org/", "robot-description": "  Targeted email gatherer utilizing user-defined seed pointsand interacting with both the webserver and MX servers ofremote sites.", "robot-environment": "", "modified-by": "", "robot-status": " active", "robot-owner-name": "Rob O'Zilla", "robot-useragent": "Roverbot", "robot-from": "         yes", "robot-purpose": " maintenance", "robot-exclusion-useragent": "", "robot-id": " robozilla", "robot-exclusion": "    yes", "robot-name": " Robozilla", "robot-availability": "", "robot-details-url": " http://www.dmoz.org/newsletter/2000Aug/robo.html", "robot-type": " standalonerobot-availability: nonerobot-exclusion: norobot-noindex: norobot-host: directory.mozilla.orgrobot-useragent: Robozilla/1.0robot-description: Robozilla visits all the links within the Open Directory periodically, marking the ones that return errors for review.robot-environment: servicerobot-id:           roverbotrobot-name:         Roverbotrobot-cover-url:    http://www.roverbot.com/robot-details-url:robot-owner-name:   GlobalMedia Design (Andrew Cowan & BrianClark)robot-owner-url:    http://www.radzone.org/gmd/robot-owner-email:  gmd@spyder.netrobot-status:robot-purpose:      indexingrobot-type:         standalone", "modified-date": "      Tue Jun 18 19:16:31 1996.", "robot-noindex": "", "robot-owner-url": " http://dmoz.org/profiles/robozilla.html", "robot-host": "         roverbot.com", "robot-language": "     perl5", "robot-platform": "", "robot-owner-email": " robozilla@dmozed.org"}, "DNAbot/": {"robot-history": " Developed by DNA, Inc.(Niigata City, Japan) in 1998.", "robot-cover-url": " www.directhit.com", "robot-description": " A search robot in 100 java, with its own built-in database engine and web server . Currently in Japanese.", "robot-environment": " commercial", "modified-by": " Tom Tanaka", "robot-status": " development       ", "robot-owner-name": " Direct Hit Technologies, Inc.", "robot-useragent": "DNAbot/", "robot-from": " yes ", "robot-purpose": " indexing ", "robot-exclusion-useragent": "", "robot-id": " directhit", "robot-exclusion": " yes", "robot-name": " Direct Hit Grabber", "robot-availability": " data", "robot-details-url": " http://www.directhit.com/about/company/spider.htmlrobot-status: activerobot-description: Direct Hit Grabber indexes documents and collects Web statistics for the Direct Hit Search Engine (available at www.directhit.com and our partners' sites)robot-purpose: Indexing and statisticsrobot-type: standalonerobot-platform: unixrobot-language: C++", "robot-type": " standalone          ", "modified-date": " Mon, 4 Jan 1999 14:30:00 GMT", "robot-noindex": " no", "robot-owner-url": " www.directhit.com", "robot-host": " xx.dnainc.co.jp", "robot-language": " java ", "robot-platform": " unix, windows, windows95, windowsNT, mac", "robot-owner-email": " DirectHitGrabber@directhit.comrobot-exclusion: yesrobot-exclusion-useragent: grabberrobot-noindex: yesrobot-host: *.directhit.comrobot-from: yesrobot-useragent: grabberrobot-environment: servicemodified-by: grabber@directhit.comrobot-id: dnabotrobot-name: DNAbotrobot-cover-url: http://xx.dnainc.co.jp/dnabot/robot-details-url: http://xx.dnainc.co.jp/dnabot/robot-owner-name: Tom Tanakarobot-owner-url: http://xx.dnainc.co.jprobot-owner-email: tomatell@xx.dnainc.co.jp"}, "webvac/": {"robot-history": "", "robot-cover-url": "robot-owner-name: Nicolas Fraijirobot-owner-email: u610468@csi.uottawa.carobot-status: active, under further enhancement.robot-purpose: maintenance, link diagnosticsrobot-type: standalonerobot-exclusion: yesrobot-noindex: norobot-exclusion-useragent: webspiderrobot-host: severalrobot-from: Yesrobot-language: Perl4robot-history: developped as a course project at the University of     Ottawa, Canada in 1996.robot-environment: Educational use and Researchrobot-id:           webvacrobot-name:         WebVacrobot-cover-url:    http://www.federated.com/~tim/webvac.html", "robot-description": "", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Tim Jensen", "robot-useragent": "webvac/", "robot-from": "         no", "robot-purpose": "      mirroring", "robot-exclusion-useragent": "", "robot-id": " webspider", "robot-exclusion": "    no", "robot-name": " WebSpiderrobot-details-url: http://www.csi.uottawa.ca/~u610468", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Mon May 13 03:19:17 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.federated.com/~tim", "robot-host": "", "robot-language": "     C++", "robot-platform": "", "robot-owner-email": "  tim@federated.com"}, "SG-Scout": {"robot-history": "      Run since 27 June 1994, for an internal XEROX researchproject", "robot-cover-url": "    http://www-swiss.ai.mit.edu/~ptbb/SG-Scout/SG-Scout.html", "robot-description": "Does a server-oriented breadth-first search in around-robin fashion, with multiple processes.", "robot-environment": "", "modified-by": "", "robot-status": "       active", "robot-owner-name": "   Peter Beebee", "robot-useragent": "SG-Scout", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           sgscout", "robot-exclusion": "    yes", "robot-name": "         SG-Scout", "robot-availability": " ", "robot-details-url": "", "robot-type": "         ", "modified-date": "      ", "robot-noindex": "      no", "robot-owner-url": "    http://www-swiss.ai.mit.edu/~ptbb/personal/index.html", "robot-host": "         beta.xerox.com", "robot-language": "     ", "robot-platform": "     ", "robot-owner-email": "  ptbb@ai.mit.edu, beebee@parc.xerox.com"}, "cosmos/": {"robot-history": "", "robot-cover-url": " http://xyleme.com/", "robot-description": " index XML, follow HTML", "robot-environment": " service", "modified-by": " Mihai Preda", "robot-status": " development", "robot-owner-name": " Mihai Preda", "robot-useragent": "cosmos/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " cosmos", "robot-id": " cosmos", "robot-exclusion": " yes", "robot-name": " XYLEME Robot", "robot-availability": " data", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Fri, 24 Nov 2000 00:00:00 GMT", "robot-noindex": " norobot-nofollow: no", "robot-owner-url": " http://www.mihaipreda.com/", "robot-host": "", "robot-language": " c++", "robot-platform": " unix", "robot-owner-email": " preda@xyleme.com"}, "MOMspider/": {"robot-history": "      Originated as a research project at the University ofCalifornia, Irvine, in 1993. Presented at the FirstInternational WWW Conference in Geneva, 1994.", "robot-cover-url": "    http://www.ics.uci.edu/WebSoft/MOMspider/", "robot-description": "  to validate links, and generate statistics. It's usually runfrom anywhere", "robot-environment": "", "modified-by": "        fielding@ics.uci.edu", "robot-status": "       active", "robot-owner-name": "   Roy T. Fielding", "robot-useragent": "MOMspider/", "robot-from": "         yes", "robot-purpose": "      maintenance, statistics", "robot-exclusion-useragent": "", "robot-id": "           momspider", "robot-exclusion": "    yes", "robot-name": "         MOMspider", "robot-availability": " source", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sat May 6 08:11:58 1995", "robot-noindex": "      no", "robot-owner-url": "    http://www.ics.uci.edu/dir/grad/Software/fielding", "robot-host": "         *", "robot-language": "     perl 4", "robot-platform": "     UNIX", "robot-owner-email": "  fielding@ics.uci.edu"}, "WebCatcher/": {"robot-history": " This robot finds its roots in a research project            at Nagoya University in 1998.", "robot-cover-url": " http://oscar.lang.nagoya-u.ac.jp", "robot-description": " WebCatcher gathers web pages                   that Japanese collage students want to visit.", "robot-environment": " research", "modified-by": "Reiji SUZUKI <reiji@infonia.ne.jp>", "robot-status": " development", "robot-owner-name": " Reiji SUZUKI", "robot-useragent": "WebCatcher/", "robot-from": " no", "robot-purpose": " indexing  ", "robot-exclusion-useragent": " webcatcher", "robot-id": " webcatcher", "robot-exclusion": " yes", "robot-name": " WebCatcher", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone   ", "modified-date": " Fri, 16 Oct 1998 17:28:52 JST", "robot-noindex": " no", "robot-owner-url": " http://oscar.lang.nagoya-u.ac.jp/~reiji/index.html", "robot-host": " oscar.lang.nagoya-u.ac.jp", "robot-language": " perl5", "robot-platform": " unix, windows, mac", "robot-owner-email": " reiji@infonia.ne.jprobot-owner-name2: Masatoshi SUGIURArobot-owner-url2: http://oscar.lang.nagoya-u.ac.jp/~sugiura/index.htmlrobot-owner-email2: sugiura@lang.nagoya-u.ac.jp"}, "CyberSpyder/": {"robot-history": " The original robot was created to fill a widely seen need for a easy to use link checking program.", "robot-cover-url": " http://www.cyberspyder.com/cslnkts1.html", "robot-description": " CyberSpyder Link Test is intended to be used as a site management tool to validate that HTTP links on a page are functional and to produce various analysis reports to assist in managing a site.", "robot-environment": " commercial", "modified-by": " Tom Aman", "robot-status": " active", "robot-owner-name": " Tom Aman", "robot-useragent": "CyberSpyder/", "robot-from": " no", "robot-purpose": " link validation, some html validation", "robot-exclusion-useragent": " cyberspyder", "robot-id": " cyberspyder", "robot-exclusion": " user configurable", "robot-name": " CyberSpyder Link Test", "robot-availability": " binary", "robot-details-url": " http://www.cyberspyder.com/cslnkts1.html", "robot-type": " standalone", "modified-date": " Tue, 31 Mar 1998 01:02:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.cyberspyder.com/", "robot-host": " *", "robot-language": " Microsoft Visual Basic 4.0", "robot-platform": " windows 3.1x, windows95, windowsNT", "robot-owner-email": " amant@cyberspyder.com"}, "Jobot/": {"robot-history": "      ", "robot-cover-url": "    http://www.micrognosis.com/~ajack/jobot/jobot.html", "robot-description": "Its purpose is to generate a Resource Discovery database.Intended to seek out sites of potential career interest.Hence - Job Robot.", "robot-environment": "", "modified-by": "", "robot-status": "       inactive", "robot-owner-name": "   Adam Jack", "robot-useragent": "Jobot/", "robot-from": "         yes", "robot-purpose": "      standalone", "robot-exclusion-useragent": "", "robot-id": "           jobot", "robot-exclusion": "    yes", "robot-name": "         Jobot", "robot-availability": " ", "robot-details-url": "", "robot-type": "         ", "modified-date": "      Tue Jan  9 18:55:55 1996", "robot-noindex": "      no", "robot-owner-url": "    http://www.micrognosis.com/~ajack/index.html", "robot-host": "         supernova.micrognosis.com", "robot-language": "     perl 4", "robot-platform": "     ", "robot-owner-email": "  ajack@corp.micrognosis.com"}, "JBot ": {"robot-history": " -", "robot-cover-url": " http://www.matuschek.net/software/jbot", "robot-description": " Java web crawler to download web sites", "robot-environment": " hobby", "modified-by": " Daniel Matuschek <daniel@matuschek.net>", "robot-status": " development", "robot-owner-name": " Daniel Matuschek", "robot-useragent": "JBot ", "robot-from": " -", "robot-purpose": " indexing", "robot-exclusion-useragent": " JBot", "robot-id": " JBot", "robot-exclusion": " yes", "robot-name": " JBot Java Web Robot", "robot-availability": " source", "robot-details-url": " http://www.matuschek.net/software/jbot", "robot-type": " standalone", "modified-date": " Thu, 03 Jan 2000 16:00:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.matuschek.net", "robot-host": " *", "robot-language": " Java", "robot-platform": " Java", "robot-owner-email": " daniel@matuschek.net"}, "PiltdownMan/": {"robot-history": " To maintain a database of search engines,               we needed an automated tool. That's why               we began the creation of this robot.", "robot-cover-url": " http://profitnet.bizland.com/", "robot-description": " The PiltdownMan robot is used to get a                   list of links from the search engines                   in our database. These links are                   followed, and the page that they refer                   is downloaded to get some statistics                   from them.                   The robot runs once a month, more or                   less, and visits the first 10 pages                   listed in every search engine, for a                   group of keywords.", "robot-environment": " service", "modified-by": " Daniel Vil", "robot-status": " active", "robot-owner-name": " Daniel Vil", "robot-useragent": "PiltdownMan/", "robot-from": " no", "robot-purpose": " statistics", "robot-exclusion-useragent": " piltdownman", "robot-id": " piltdownman", "robot-exclusion": " yes", "robot-name": " PiltdownMan", "robot-availability": " none", "robot-details-url": " http://profitnet.bizland.com/piltdownman.html", "robot-type": " standalone", "modified-date": " Mon, 13 Dec 1999 21:50:32 GMT", "robot-noindex": " norobot-nofollow: no", "robot-owner-url": " http://profitnet.bizland.com/aboutus.html", "robot-host": " 62.36.128.*, 194.133.59.*, 212.106.215.*", "robot-language": " c++", "robot-platform": " windows95, windows98, windowsNT", "robot-owner-email": " profitnet@myezmail.com"}, "Magpie/": {"robot-history": " Part of a research project. Alpha testing from 10 July 1996, Beta testing from 10 September.", "robot-cover-url": "", "robot-description": " Used to obtain information from a specified list of web pages for local indexing. Runs every two hours, and visits only a small number of sites.", "robot-environment": " research", "modified-by": " Keith Jones", "robot-status": " development", "robot-owner-name": " Keith Jones", "robot-useragent": "Magpie/", "robot-from": " no", "robot-purpose": " indexing, statistics", "robot-exclusion-useragent": "", "robot-id": " magpie", "robot-exclusion": " no", "robot-name": " Magpie", "robot-availability": "", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Wed, 10 Oct 1996 13:15:00 GMT", "robot-noindex": " no", "robot-owner-url": " ", "robot-host": " *.blueberry.co.uk, 194.70.52.*, 193.131.167.144", "robot-language": " perl5", "robot-platform": " unix", "robot-owner-email": " Keith.Jones@blueberry.co.uk"}, "elfinbot": {"robot-history": "", "robot-cover-url": "http://letsfinditnow.com", "robot-description": "ELFIN is used to index and add data to the Lets Find It Now Search Engine (http://letsfinditnow.com). The robot runs every 30 days.", "robot-environment": "", "modified-by": "", "robot-status": "Active", "robot-owner-name": "Lets Find It Now Ltd", "robot-useragent": "elfinbot", "robot-from": "no", "robot-purpose": "Indexing for the Lets Find It Now search Engine", "robot-exclusion-useragent": "elfinbot", "robot-id": " elfinbot", "robot-exclusion": " yes", "robot-name": "ELFINBOT", "robot-availability": "None", "robot-details-url": "http://letsfinditnow.com/elfinbot.html", "robot-type": "Standalone", "modified-date": "", "robot-noindex": "yes", "robot-owner-url": "http://letsfinditnow.com", "robot-host": "*.letsfinditnow.com", "robot-language": "Perl5", "robot-platform": "Unix", "robot-owner-email": "admin@letsfinditnow.com"}, "Informant": {"robot-history": " The robot is part of a research project at Dartmouth College.   The robot may become part of a commercial service (at which time it may be  subsumed by some other, existing robot).", "robot-cover-url": " http://informant.dartmouth.edu/", "robot-description": " The Informant robot continually checks the Web pages that are relevant to user queries.  Users are notified of any new or updated pages.  The robot runs daily, but the number of hits per site per day should be quite small, and these hits should be randomly distributed over several hours.  Since the robot does not actually  follow links (aside from those returned from the major search engines  such as Lycos), it does not fall victim to the common looping problems. The robot will support the Robot Exclusion Standard by early December, 1996.", "robot-environment": " research, service", "modified-by": " Bob Gray", "robot-status": " active", "robot-owner-name": " Bob Grayrobot-owner-name2: Aditya Bhasinrobot-owner-name3: Katsuhiro Moizumirobot-owner-name4: Dr. George V. Cybenko", "robot-useragent": "Informant", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " Informant", "robot-id": " informant", "robot-exclusion": " no", "robot-name": " Informant", "robot-availability": " none", "robot-details-url": " http://informant.dartmouth.edu/about.html", "robot-type": " standalone", "modified-date": " Sun, 3 Nov 1996 11:55:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://informant.dartmouth.edu/", "robot-host": " informant.dartmouth.edu", "robot-language": " c, c++", "robot-platform": " unix", "robot-owner-email": " info_adm@cosmo.dartmouth.edu"}, "WebCopy/": {"robot-history": "      ", "robot-cover-url": "    http://www.inf.utfsm.cl/~vparada/webcopy.html", "robot-description": "  Its purpose is to perform mirroring. WebCopy can retrievefiles recursively using HTTP protocol.It can be used as adelayed browser or as a mirroring tool. It cannot jump fromone site to another.", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Victor Parada", "robot-useragent": "WebCopy/", "robot-from": "         no", "robot-purpose": "      mirroring", "robot-exclusion-useragent": "", "robot-id": "           webcopy", "robot-exclusion": "    no", "robot-name": "         WebCopy", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sun Jul 2 15:27:04 1995", "robot-noindex": "      no", "robot-owner-url": "    http://www.inf.utfsm.cl/~vparada/", "robot-host": "         *", "robot-language": "     perl 4 or perl 5", "robot-platform": "     ", "robot-owner-email": "  vparada@inf.utfsm.cl"}, "combine/": {"robot-history": " A complete re-design of the NWI robot (w3index) for DESIRE project. ", "robot-cover-url": " http://www.ub2.lu.se/~tsao/combine.ps", "robot-description": " An open, distributed, and efficient harvester.", "robot-environment": " research", "modified-by": " Yong Cao", "robot-status": " development", "robot-owner-name": " Yong Cao", "robot-useragent": "combine/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " combine", "robot-id": " combine", "robot-exclusion": " yes", "robot-name": " Combine System", "robot-availability": " source", "robot-details-url": " http://www.ub2.lu.se/~tsao/combine.ps", "robot-type": " standalone", "modified-date": " Tue, 04 Mar 1997 16:11:40 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.ub2.lu.se/", "robot-host": " *.ub2.lu.se", "robot-language": " c, perl5", "robot-platform": " unix", "robot-owner-email": " tsao@munin.ub2.lu.se"}, "Atomz/": {"robot-history": " Developed for Atomz.com, launched in 1999.", "robot-cover-url": "robot-owner-name: All That Netrobot-owner-url: http://www.allthatnet.comrobot-owner-email: info@allthatnet.comrobot-status: activerobot-purpose: indexingrobot-type:robot-platform:robot-availability:robot-exclusion: yesrobot-exclusion-useragent: ATN_Worldwiderobot-noindex:robot-nofollow:robot-host: www.allthatnet.comrobot-from:robot-useragent: ATN_Worldwiderobot-language:robot-description: The ATN robot is used to build the database for the AllThatNet search service operated by All That Net.  The robot runs weekly, and visits sites in a random order.robot-history:robot-environment:modified-date: July 09, 2000 17:43 GMTrobot-id: atomzrobot-name: Atomz.com Search Robotrobot-cover-url: http://www.atomz.com/help/", "robot-description": " Robot used for web site search service.", "robot-environment": " service", "modified-by": " Mike Thompson", "robot-status": " active", "robot-owner-name": " Mike Thompson", "robot-useragent": "Atomz/", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " Atomz", "robot-id": " atn", "robot-exclusion": " yes", "robot-name": " ATN Worldwiderobot-details-url:", "robot-availability": " service", "robot-details-url": " http://www.atomz.com/", "robot-type": " standalone", "modified-date": " Tue Jul 13 03:50:06 GMT 1999", "robot-noindex": " yes", "robot-owner-url": " http://www.atomz.com/", "robot-host": " www.atomz.com", "robot-language": " c", "robot-platform": " unix", "robot-owner-email": " mike@atomz.com"}, "Orbsearch/": {"robot-history": " This robot was started as a hobby.", "robot-cover-url": " http://orbsearch.home.ml.org", "robot-description": " Orbsearch builds the database for Orb Search Engine.  It runs when requested.", "robot-environment": " hobby", "modified-by": " Matt Weber", "robot-status": " active", "robot-owner-name": " Matt Weber", "robot-useragent": "Orbsearch/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " Orbsearch/1.0", "robot-id": " orb_search", "robot-exclusion": " yes", "robot-name": " Orb Search", "robot-availability": " data", "robot-details-url": " http://orbsearch.home.ml.org", "robot-type": " standalone", "modified-date": " Sun, 31 Aug 1997 02:28:52 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.weberworld.com", "robot-host": " cow.dyn.ml.org, *.dyn.ml.org", "robot-language": " Perl5", "robot-platform": " unix", "robot-owner-email": " webernet@geocities.com"}, "Monster/": {"robot-history": "      Now the full (I suppose) list of ex-USSR sites is produced.", "robot-cover-url": "    http://www.neva.ru/monster.list/russian.www.html", "robot-description": "  The Monster has two parts - Web searcher and Web analyzer.Searcher is intended to perform the list of WWW sites of desired domain (for example it can perform list of all WWW sites of mit.edu, com, org, etc... domain)In the User-agent field $TYPE is set to 'Mapper' for Web searcherand 'StAlone' for Web analyzer. ", "robot-environment": "  ", "modified-by": "", "robot-status": "       active", "robot-owner-name": "   Dmitry Dicky", "robot-useragent": "Monster/", "robot-from": "         ", "robot-purpose": "      maintenance, mirroring", "robot-exclusion-useragent": "", "robot-id": "           monster", "robot-exclusion": "    yes", "robot-name": "         Monster", "robot-availability": " binary", "robot-details-url": "  ", "robot-type": "         standalone", "modified-date": "      Tue Jun 25 10:03:36 1996", "robot-noindex": "      no", "robot-owner-url": "    http://wild.stu.neva.ru/", "robot-host": "         wild.stu.neva.ru", "robot-language": "     C", "robot-platform": "     UNIX (Linux)", "robot-owner-email": "  diwil@wild.stu.neva.ru"}, "vision-search/": {"robot-history": "", "robot-cover-url": "    http://www.ius.cs.cmu.edu/cgi-bin/vision-search", "robot-description": "  Intended to be an index of computer vision pages, containingall pages within <em>n</em> links (for some small<em>n</em>) of the Vision Home Page", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Henry A. Rowley", "robot-useragent": "vision-search/", "robot-from": "         no", "robot-purpose": "      indexing.", "robot-exclusion-useragent": "", "robot-id": "           visionsearch", "robot-exclusion": "    yes", "robot-name": "         vision-search", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Fri Mar  8 16:03:04 1996", "robot-noindex": "", "robot-owner-url": "    http://www.cs.cmu.edu/~har", "robot-host": "         dylan.ius.cs.cmu.edu", "robot-language": "     Perl 5", "robot-platform": "", "robot-owner-email": "  har@cs.cmu.edu"}, "Pioneer": {"robot-history": "", "robot-cover-url": "    http://sequent.uncfsu.edu/~micah/pioneer.html", "robot-description": "  Pioneer is part of an undergraduate researchproject.", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Micah A. Williams", "robot-useragent": "Pioneer", "robot-from": "         yes", "robot-purpose": "      indexing, statistics", "robot-exclusion-useragent": "", "robot-id": "           pioneer", "robot-exclusion": "    yes", "robot-name": "         Pioneer", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Mon Feb  5 02:49:32 1996.", "robot-noindex": "", "robot-owner-url": "    http://sequent.uncfsu.edu/~micah/", "robot-host": "         *.uncfsu.edu or flyer.ncsc.org", "robot-language": "     C.", "robot-platform": "", "robot-owner-email": "  micah@sequent.uncfsu.edu"}, "Voyager/": {"robot-history": "", "robot-cover-url": " http://www.lisa.co.jp/voyager/", "robot-description": " This robot is used to build the database for the                   Lisa Search service.  The robot manually launch                     and visits sites in a random order.", "robot-environment": " service", "modified-by": " Hideyuki Ezaki", "robot-status": " development", "robot-owner-name": " Voyager Staff", "robot-useragent": "Voyager/", "robot-from": " yes", "robot-purpose": " indexing, maintenance", "robot-exclusion-useragent": " Voyager ", "robot-id": " voyager", "robot-exclusion": " yes", "robot-name": " Voyager", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Mon, 30 Nov 1998 08:00:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.lisa.co.jp/voyager/", "robot-host": " *.lisa.co.jp", "robot-language": " perl5 ", "robot-platform": " unix", "robot-owner-email": " voyager@lisa.co.jp"}, "PlumtreeWebAccessor/": {"robot-history": "", "robot-cover-url": "", "robot-description": " The Plumtree Web Accessor is a component that customers can add to the        Plumtree Server to index documents on the World Wide Web.", "robot-environment": " commercial", "modified-by": " Joseph A. Stanko <josephs@plumtree.com>", "robot-status": " development", "robot-owner-name": " Joseph A. Stanko ", "robot-useragent": "PlumtreeWebAccessor/", "robot-from": " yes", "robot-purpose": " indexing for the Plumtree Server", "robot-exclusion-useragent": " PlumtreeWebAccessor", "robot-id": " plumtreewebaccessor", "robot-exclusion": " yes", "robot-name": " PlumtreeWebAccessor ", "robot-availability": " none", "robot-details-url": " http://www.plumtree.com/", "robot-type": " standalone", "modified-date": " Thu, 17 Dec 1998", "robot-noindex": " yes", "robot-owner-url": "", "robot-host": "", "robot-language": " c++", "robot-platform": " windowsNT", "robot-owner-email": " josephs@plumtree.com"}, "WebMoose/": {"robot-history": " This robot is under development. It will support ROBOTS.TXT soon.", "robot-cover-url": " ", "robot-description": " This robot collects statistics and verifies links. It  builds an graph of its visit path.", "robot-environment": " hobby", "modified-by": " Mike Blaszczak", "robot-status": " development", "robot-owner-name": " Mike Blaszczak", "robot-useragent": "WebMoose/", "robot-from": " no", "robot-purpose": " statistics, maintenance", "robot-exclusion-useragent": " WebMoose", "robot-id": " webmoose", "robot-exclusion": " no", "robot-name": " The Web Moose", "robot-availability": " data", "robot-details-url": " http://www.nwlink.com/~mikeblas/webmoose/", "robot-type": " standalone", "modified-date": " Fri, 30 Aug 1996 00:00:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.nwlink.com/~mikeblas/", "robot-host": " msn.com", "robot-language": " C++", "robot-platform": " Windows NT", "robot-owner-email": " mikeblas@nwlink.com"}, "WebReaper": {"robot-history": " Written for personal use, and then distributed to the public as freeware.", "robot-cover-url": " http://www.otway.com/webreaper", "robot-description": " Freeware app which downloads and saves sites locally for offline browsing.", "robot-environment": " hobby", "modified-by": " Mark Otway", "robot-status": " active", "robot-owner-name": " Mark Otway", "robot-useragent": "WebReaper", "robot-from": " no", "robot-purpose": " indexing/offline browsing", "robot-exclusion-useragent": " webreaper", "robot-id": " webreaper", "robot-exclusion": " yes", "robot-name": " WebReaper", "robot-availability": " binary", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Thu, 25 Mar 1999 15:00:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.otway.com", "robot-host": " *", "robot-language": " c++", "robot-platform": " windows95, windowsNT", "robot-owner-email": " webreaper@otway.com"}, "W3M2/": {"robot-history": "      ", "robot-cover-url": "            http://www.ub2.lu.se/NNC/projects/NWI/the_nwi_robot.htmlrobot-owner-name:     Sigfrid Lundberg, Lund university, Swedenrobot-owner-url:      http://nwi.ub2.lu.se/~siglunrobot-owner-email:    siglun@munin.ub2.lu.serobot-status:         activerobot-purpose:        discovery,statisticsrobot-type:           standalonerobot-platform:       UNIXrobot-availability:   none (at the moment)robot-exclusion:      yesrobot-noindex:        Norobot-host:   nwi.ub2.lu.se, mars.dtv.dk and a few othersrobot-from:   yesrobot-useragent:      w3indexrobot-language:       perl5robot-description:    A resource discovery robot, used primarily forthe indexing of the Scandinavian Webrobot-history:        It is about a year or so old.Written by Anders Ard, Mattias Borrell, Hkan Ard and myself.robot-environment: service,researchmodified-date:        Wed Jun 26 13:58:04 MET DST 1996modified-by:          Sigfrid Lundbergrobot-id:           w3m2robot-name:         W3M2robot-cover-url:    http://tronche.com/W3M2", "robot-description": "  to generate a Resource Discovery database, validate links,validate HTML, and generate statistics", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Christophe Tronche", "robot-useragent": "W3M2/", "robot-from": "         yes", "robot-purpose": "      indexing, maintenance, statistics", "robot-exclusion-useragent": "", "robot-id": "             w3index", "robot-exclusion": "    yes", "robot-name": "           The NWI Robot", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Fri May 5 17:48:48 1995", "robot-noindex": "      no", "robot-owner-url": "    http://tronche.com/", "robot-host": "         *", "robot-language": "     Perl 4, Perl 5, and C++", "robot-platform": "     ", "robot-owner-email": "  tronche@lri.fr"}, "NorthStar": {"robot-history": "      ", "robot-cover-url": "    http://comics.scs.unr.edu:7000/top.html", "robot-description": "  Recent runs (26 April 94) will concentrate on textualanalysis of the Web versus GopherSpace (from the Veronicadata) as well as indexing.", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Fred Barrie", "robot-useragent": "NorthStar", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           northstar", "robot-exclusion": "    ", "robot-name": "         The NorthStar Robot", "robot-availability": " ", "robot-details-url": "", "robot-type": "         ", "modified-date": "      ", "robot-noindex": "      ", "robot-owner-url": "    ", "robot-host": "         frognot.utdallas.edu, utdallas.edu, cnidir.org", "robot-language": "     ", "robot-platform": "     ", "robot-owner-email": "  barrie@unr.edu"}, "spiderline/": {"robot-history": " Developed for Spiderline.com, launched in 2001.", "robot-cover-url": " http://www.spiderline.com/", "robot-description": " ", "robot-environment": " service", "modified-by": " Benjamin Benson", "robot-status": " active", "robot-owner-name": " Benjamin Benson", "robot-useragent": "spiderline/", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " spiderline", "robot-id": " spiderline", "robot-exclusion": " yes", "robot-name": " Spiderline Crawler", "robot-availability": " free and commercial services", "robot-details-url": " http://www.spiderline.com/", "robot-type": " standalone", "modified-date": " Wed, 21 Feb 2001 03:36:39 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.spiderline.com/", "robot-host": " *.spiderline.com, *.spiderline.org", "robot-language": " c, c++", "robot-platform": " unix", "robot-owner-email": " ben@spiderline.com"}, "NetCarta CyberPilot Pro": {"robot-history": "", "robot-cover-url": "    http://www.netcarta.com/", "robot-description": "  The NetCarta WebMap Engine is a general purpose, commercialspider. Packaged with a full GUI in the CyberPilo Proproduct, it acts as a personal spider to work with a browserto facilitiate context-based navigation.  The WebMapperproduct uses the robot to manage a site (site copy, sitediff, and extensive link management facilities).  Allversions can create publishable NetCarta WebMaps, whichcapture the crawled information.  If the robot sees apublished map, it will return the published map rather thancontinuing its crawl. Since this is a personal spider, itwill be launched from multiple domains. This robot tends tofocus on a particular site.  No instance of the robot shouldhave more than one outstanding request out to any given siteat a time. The User-agent field contains a coded IDidentifying the instance of the spider; specific users canbe blocked via robots.txt using this ID.", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   NetCarta WebMap Engine", "robot-useragent": "NetCarta CyberPilot Pro", "robot-from": "         yes", "robot-purpose": "      indexing, maintenance, mirroring, statistics", "robot-exclusion-useragent": "", "robot-id": "           netcarta", "robot-exclusion": "    yes", "robot-name": "         NetCarta WebMap Engine", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sun Feb 18 02:02:49 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.netcarta.com/", "robot-host": "", "robot-language": "     C++.", "robot-platform": "", "robot-owner-email": "  info@netcarta.com"}, "PerlCrawler/": {"robot-history": " Originated in modified form on 25 June 1998", "robot-cover-url": " http://perlsearch.hypermart.net/", "robot-description": " The PerlCrawler robot is designed to index and build a database of pages relating to the Perl programming language.", "robot-environment": " hobby", "modified-by": " Matt McKenzie", "robot-status": " active", "robot-owner-name": " Matt McKenzie ", "robot-useragent": "PerlCrawler/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " perlcrawler", "robot-id": " perlcrawler", "robot-exclusion": " yes", "robot-name": " PerlCrawler 1.0", "robot-availability": " source", "robot-details-url": " http://www.xav.com/scripts/xavatoria/index.html", "robot-type": " standalone", "modified-date": " Fri, 18 Dec 1998 23:37:40 GMT", "robot-noindex": " yes", "robot-owner-url": " http://perlsearch.hypermart.net/", "robot-host": " server5.hypermart.net", "robot-language": " perl5", "robot-platform": " unix", "robot-owner-email": " webmaster@perlsearch.hypermart.net"}, "mouse.house/": {"robot-history": " This robot is under development and currently active", "robot-cover-url": " http://www.mobrien.com/add_site.html", "robot-description": " Robot runs every 30 days for a full index and weekly = on a list of accumulated visitor requests", "robot-environment": " written as an employee / guest service", "modified-by": " MPRM Group Limited", "robot-status": " robot actively in use", "robot-owner-name": " MPRM Group Limited", "robot-useragent": "mouse.house/", "robot-from": " yes", "robot-purpose": " gather content for a free indexing service", "robot-exclusion-useragent": " spider_monkey", "robot-id": " spider_monkey", "robot-exclusion": " yes", "robot-name": " spider_monkey", "robot-availability": " bulk data gathered by robot available", "robot-details-url": " http://www.mobrien.com/add_site.html", "robot-type": " FDSE robot", "modified-date": " Mon, 22 May 2000 12:28:52 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.mobrien.com", "robot-host": " snowball.ionsys.com", "robot-language": " perl5", "robot-platform": " unix", "robot-owner-email": " mprm@ionsys.com"}, "Hazel": {"robot-history": "", "robot-cover-url": "    http://www.greenearth.com/", "robot-description": "The wild ferret web hopper's are designed as specific agentsto retrieve data from all available sources on the internet.They work in an onion format hopping from spot to spot onelevel at a time over the internet. The information isgathered into different relational databases, known asHazel's Horde. The information is publicly available andwill be free for the browsing at www.greenearth.com.Effective date of the data posting is to beannounced.", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Greg Boswell", "robot-useragent": "Hazel", "robot-from": "         yes", "robot-purpose": "      indexing maintenance statistics", "robot-exclusion-useragent": "", "robot-id": "           ferret", "robot-exclusion": "    no", "robot-name": "         Wild Ferret Web Hopper #1, #2, #3", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Mon Feb 19 00:28:37 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.greenearth.com/", "robot-host": "", "robot-language": "     C++, Visual Basic, Java", "robot-platform": "", "robot-owner-email": "  ghbos@postoffice.worldnet.att.net"}, "esculapio/": {"robot-history": " First, a research project. Now, an internal tool. Next, ???.", "robot-cover-url": " http://esculapio.cype.com", "robot-description": " Checks the integrity of the links between several domains.", "robot-environment": " research, service", "modified-by": "", "robot-status": " active", "robot-owner-name": " CYPE Ingenieros", "robot-useragent": "esculapio/", "robot-from": " yes", "robot-purpose": " link validation", "robot-exclusion-useragent": " esculapio", "robot-id": " esculapio", "robot-exclusion": " yes", "robot-name": " esculapio", "robot-availability": " none", "robot-details-url": " http://esculapio.cype.com/details.htm", "robot-type": " standalone", "modified-date": " Mon, 6 Jun 2004 08:25 +1 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.cype.com", "robot-host": " 80.34.92.45", "robot-language": " C++", "robot-platform": " linux", "robot-owner-email": " imasd@cype.com"}, "gcreep/": {"robot-history": " Spare time project begun late '96, maybe early '97", "robot-cover-url": " http://www.instrumentpolen.se/gcreep/index.html", "robot-description": " Indexing robot to learn SQL", "robot-environment": " hobby", "modified-by": " Anders Hedstrom", "robot-status": " development", "robot-owner-name": " Instrumentpolen AB", "robot-useragent": "gcreep/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " gcreep", "robot-id": " gcreep", "robot-exclusion": " yes", "robot-name": " GCreep", "robot-availability": " none", "robot-details-url": " http://www.instrumentpolen.se/gcreep/index.html", "robot-type": " browser+standalone", "modified-date": " Fri, 23 Jan 1998 16:09:00 MET", "robot-noindex": " yes", "robot-owner-url": " http://www.instrumentpolen.se/ip-kontor/eng/index.html", "robot-host": " mbx.instrumentpolen.se", "robot-language": " c", "robot-platform": " linux+mysql", "robot-owner-email": " anders@instrumentpolen.se"}, "IsraeliSearch/": {"robot-history": "", "robot-cover-url": "    http://www.idc.ac.il/Sandbag/", "robot-description": "", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Etamar Laron", "robot-useragent": "IsraeliSearch/", "robot-from": "         no", "robot-purpose": "      indexing.", "robot-exclusion-useragent": "", "robot-id": "           israelisearch", "robot-exclusion": "    yes", "robot-name": "         Israeli-search", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Tue Apr 23 19:23:55 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.xpert.com/~etamar/", "robot-host": "         dylan.ius.cs.cmu.edu", "robot-language": "     C A complete software designed to collect information in adistributed workload and supports context queries. Intendedto be a complete updated resource for Israeli sites andinformation related to Israel or IsraeliSociety.", "robot-platform": "", "robot-owner-email": "  etamar@xpert.co"}, "Motor/": {"robot-history": " ", "robot-cover-url": " http://www.cybercon.de/Motor/index.html", "robot-description": " The Motor robot is used to build the database for the  www.webindex.de search service operated by CyberCon. The robot ios under  development - it runs in random intervals and visits site in a priority  driven order (.de/.ch/.at first, root and robots.txt first)", "robot-environment": " service", "modified-by": " Michael Goeckel (Michael@cybercon.technopark.gmd.de)", "robot-status": " developement", "robot-owner-name": " Mr. Oliver Runge, Mr. Michael Goeckel", "robot-useragent": "Motor/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " Motor", "robot-id": " motor", "robot-exclusion": " yes", "robot-name": " Motor", "robot-availability": " data", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Wed, 3 Jul 1996 15:30:00 +0100", "robot-noindex": " no", "robot-owner-url": " http://www.cybercon.de/index.html", "robot-host": " Michael.cybercon.technopark.gmd.de", "robot-language": " 4th dimension", "robot-platform": " mac", "robot-owner-email": " Motor@cybercon.technopark.gmd.de"}, "phpdig/": {"robot-history": " writen first 2001-03-30", "robot-cover-url": " http://phpdig.toiletoine.net/", "robot-description": " Small robot and search engine written in php.", "robot-environment": " hobby", "modified-by": " Antoine Bajolet", "robot-status": " *", "robot-owner-name": " Antoine Bajolet", "robot-useragent": "phpdig/", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " phpdig", "robot-id": " phpdig", "robot-exclusion": " yes", "robot-name": " PhpDig", "robot-availability": " source", "robot-details-url": " http://phpdig.toiletoine.net/", "robot-type": " standalone", "modified-date": " Sun, 21 Nov 2001 20:01:19 GMT", "robot-noindex": " yes", "robot-owner-url": " http://phpdig.toiletoine.net/", "robot-host": " yes", "robot-language": " php 4.x", "robot-platform": " all supported by Apache/php/mysql", "robot-owner-email": " phpdig@toiletoine.net"}, "Arachnophilia": {"robot-history": "      ", "robot-cover-url": "    ", "robot-description": "  The purpose (undertaken by HaL Software) of this run was tocollect approximately 10k html documents for testingautomatic abstract generation", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Vince Taluskie", "robot-useragent": "Arachnophilia", "robot-from": "         ", "robot-purpose": "      ", "robot-exclusion-useragent": "", "robot-id": "           arachnophilia", "robot-exclusion": "    yes", "robot-name": "         Arachnophilia", "robot-availability": " ", "robot-details-url": "", "robot-type": "         ", "modified-date": "      ", "robot-noindex": "      no", "robot-owner-url": "    http://www.ph.utexas.edu/people/vince.html", "robot-host": "         halsoft.com", "robot-language": "     ", "robot-platform": "     ", "robot-owner-email": "  taluskie@utpapa.ph.utexas.edu"}, "iajaBot/": {"robot-history": " None, brand new.", "robot-cover-url": "", "robot-description": " Finds adult content", "robot-environment": " research", "modified-by": " Pat Morin", "robot-status": " development", "robot-owner-name": " Pat Morin", "robot-useragent": "iajaBot/", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " iajabot", "robot-id": " iajabot", "robot-exclusion": " no", "robot-name": " iajaBot", "robot-availability": " none", "robot-details-url": " http://www.scs.carleton.ca/~morin/iajabot.html", "robot-type": " standalone", "modified-date": " Tue, 27 Jun 2000, 11:17:50 EDT", "robot-noindex": " no", "robot-owner-url": " http://www.scs.carleton.ca/~morin/", "robot-host": " *.scs.carleton.ca", "robot-language": " c", "robot-platform": " unix, windows", "robot-owner-email": " morin@scs.carleton.ca"}, "FelixIDE/": {"robot-history": "This robot began as an in-house tool for the lucrative Felix  IDS (Information Discovery Service) and has gone retail.", "robot-cover-url": "http://www.pentone.com", "robot-description": "Felix IDE is a retail personal search spider sold by  The Pentone Group, Inc.  It supports the proprietary exclusion Frequency: ?????????? in the  robots.txt file. Question marks represent an integer  indicating number of milliseconds to delay between document requests. This  is called VDRF(tm) or Variable Document Retrieval Frequency. Note that  users can re-define the useragent name.", "robot-environment": "service, commercial, research", "modified-by": "Kerry B. Rogers", "robot-status": "active", "robot-owner-name": "The Pentone Group, Inc.", "robot-useragent": "FelixIDE/", "robot-from": "yes", "robot-purpose": "indexing, statistics", "robot-exclusion-useragent": "FELIX IDE", "robot-id": "felix", "robot-exclusion": "yes", "robot-name": "Felix IDE", "robot-availability": "binary", "robot-details-url": "http://www.pentone.com", "robot-type": "standalone", "modified-date": "Fri, 11 Apr 1997 19:08:02 GMT", "robot-noindex": "yes", "robot-owner-url": "http://www.pentone.com", "robot-host": "*", "robot-language": "visual basic", "robot-platform": "windows95, windowsNT", "robot-owner-email": "felix@pentone.com"}, "DesertRealm.com": {"robot-history": " The spider originally was created to learn more about how search engines work.", "robot-cover-url": " http://www.desertrealm.com", "robot-description": " The spider indexes fantasy and science fiction sites by using a customizable keyword algorithm. Only home pages are indexed, but all pages are looked at for links. Pages are visited randomly to limit impact on any one webserver.", "robot-environment": " hobby", "modified-by": " Brian B.", "robot-status": " robot actively in use", "robot-owner-name": " Brian B.", "robot-useragent": "DesertRealm.com", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " desertrealm, desert realm", "robot-id": " desertrealm", "robot-exclusion": " yes", "robot-name": " Desert Realm Spider", "robot-availability": " none", "robot-details-url": " http://spider.desertrealm.com", "robot-type": " standalone", "modified-date": " Fri, 19 Sep 2003 08:57:52 GMT", "robot-noindex": " yesrobot-nofollow: yes", "robot-owner-url": " http://www.desertrealm.com", "robot-host": " *", "robot-language": " java 1.3, java 1.4", "robot-platform": " cross platform", "robot-owner-email": " spider@desertrealm.com"}, "Site Valet": {"robot-history": " builds on cg-eye, the WDG Validator, and the Link Valet", "robot-cover-url": " http://valet.webthing.com/", "robot-description": " a deluxe site monitoring and analysis service", "robot-environment": " service", "modified-by": " nick@webthing.com", "robot-status": " active", "robot-owner-name": " Nick Kew", "robot-useragent": "Site Valet", "robot-from": " yes", "robot-purpose": " maintenance", "robot-exclusion-useragent": " Site Valet", "robot-id": " site-valet", "robot-exclusion": " yes", "robot-name": " Site Valet", "robot-availability": " data", "robot-details-url": " http://valet.webthing.com/", "robot-type": " standalone", "modified-date": " Tue, 27 June 2000", "robot-noindex": " no", "robot-owner-url": "", "robot-host": " valet.webthing.com,valet.*", "robot-language": " perl", "robot-platform": " unix", "robot-owner-email": " nick@webthing.com"}, "NHSEWalker/": {"robot-history": "      ", "robot-cover-url": "    http://nhse.mcs.anl.gov/", "robot-description": "  to generate a Resource Discovery database", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Robert Olson", "robot-useragent": "NHSEWalker/", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           nhse", "robot-exclusion": "    yes", "robot-name": "         NHSE Web Forager", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Fri May 5 15:47:55 1995", "robot-noindex": "      no", "robot-owner-url": "    http://www.mcs.anl.gov/people/olson/", "robot-host": "         *.mcs.anl.gov", "robot-language": "     perl 5", "robot-platform": "     ", "robot-owner-email": "  olson@mcs.anl.gov"}, "WWWC/": {"robot-history": " 1997", "robot-cover-url": " http://www.kinet.or.jp/naka/tomo/wwwc.html", "robot-description": "", "robot-environment": " hobby", "modified-by": " Tomoaki Nakashima (naka@kinet.or.jp)", "robot-status": " active", "robot-owner-name": " Tomoaki Nakashima.", "robot-useragent": "WWWC/", "robot-from": " yes", "robot-purpose": " maintenance", "robot-exclusion-useragent": " WWWC", "robot-id": " wwwc", "robot-exclusion": " yes", "robot-name": " WWWC Ver 0.2.5", "robot-availability": " binary", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Tuesday, 18 Feb 1997 06:02:47 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.kinet.or.jp/naka/tomo/", "robot-host": "", "robot-language": " c", "robot-platform": " windows, windows95, windowsNT", "robot-owner-email": " naka@kinet.or.jp"}, "JavaBee": {"robot-history": "", "robot-cover-url": " http://www.javabee.com", "robot-description": "This robot is used to grab java applets and run them locally overriding the security implemented", "robot-environment": "commercial", "modified-by": "", "robot-status": "Active", "robot-owner-name": "ObjectBox", "robot-useragent": "JavaBee", "robot-from": "no", "robot-purpose": "Stealing Java Code", "robot-exclusion-useragent": "", "robot-id": " javabee", "robot-exclusion": "no", "robot-name": " JavaBee", "robot-availability": "binary", "robot-details-url": "", "robot-type": "standalone", "modified-date": "", "robot-noindex": "no", "robot-owner-url": "http://www.objectbox.com/", "robot-host": "*", "robot-language": "Java", "robot-platform": "Java", "robot-owner-email": "info@objectbox.com"}, "Poppi/": {"robot-history": " Created by Antonio Provenzano in the april of 2000, has  been acquired from Tomi Officine Multimediali srl and it is next to  release as service and commercial", "robot-cover-url": " http://members.tripod.com/poppisearch", "robot-description": " Poppi is a crawler to index the web that runs weekly  gathering and indexing hypertextual, multimedia and executable file  formats", "robot-environment": " service", "modified-by": " Antonio Provenzano", "robot-status": " active", "robot-owner-name": " Antonio Provenzano", "robot-useragent": "Poppi/", "robot-from": "", "robot-purpose": " indexing", "robot-exclusion-useragent": "", "robot-id": " poppi", "robot-exclusion": "", "robot-name": " Poppi", "robot-availability": " none", "robot-details-url": " http://members.tripod.com/poppisearch", "robot-type": " standalone", "modified-date": " Mon, 22 May 2000 15:47:30 GMT", "robot-noindex": " yes", "robot-owner-url": " Antonio Provenzano", "robot-host": "=20", "robot-language": " C", "robot-platform": " unix/linux", "robot-owner-email": ""}, "AITCSRobot/": {"robot-history": "      ", "robot-cover-url": "    http://cs6.cs.ait.ac.th:21870/pa.html", "robot-description": "  Its purpose is to generate a Resource Discovery database.This Robot traverses the net and creates a searchabledatabase of Web pages. It stores the title string of theHTML document and the absolute url. A search engine providesthe boolean AND & OR query models with or without filteringthe stop list of words. Feature is kept for the Web pageowners to add the url to the searchable database.", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Razzakul Haider Chowdhury", "robot-useragent": "AITCSRobot/", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           hi", "robot-exclusion": "    no", "robot-name": "         HI (HTML Index) Search", "robot-availability": " ", "robot-details-url": "", "robot-type": "         ", "modified-date": "      Wed Oct  4 06:54:31 1995", "robot-noindex": "      no", "robot-owner-url": "    http://cs6.cs.ait.ac.th:21870/index.html", "robot-host": "         ", "robot-language": "     perl 5", "robot-platform": "     ", "robot-owner-email": "  a94385@cs.ait.ac.th"}, "CoolBot": {"robot-history": " none so far", "robot-cover-url": " www.suchmaschine21.de", "robot-description": " The CoolBot robot is used to build and maintain the directory of the german search engine Suchmaschine21.", "robot-environment": " service", "modified-by": " Stefan Fischerlaender", "robot-status": " active", "robot-owner-name": " Stefan Fischerlaender", "robot-useragent": "CoolBot", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " CoolBot", "robot-id": " coolbot", "robot-exclusion": " yes", "robot-name": " CoolBot", "robot-availability": " none", "robot-details-url": " www.suchmaschine21.de", "robot-type": " standalone", "modified-date": " Wed, 21 Jan 2001 12:16:00 GMT", "robot-noindex": " yes", "robot-owner-url": " www.suchmaschine21.de", "robot-host": " www.suchmaschine21.de", "robot-language": " perl5", "robot-platform": " unix", "robot-owner-email": " info@suchmaschine21.de"}, "Lycos/": {"robot-history": "      ", "robot-cover-url": "    http://lycos.cs.cmu.edu/", "robot-description": "  This is a research program in providing informationretrieval and discovery in the WWW, using a finite memorymodel of the web to guide intelligent, directed searches forspecific  information needs", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Dr. Michael L. Mauldin", "robot-useragent": "Lycos/", "robot-from": "         ", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           lycos", "robot-exclusion": "    yes", "robot-name": "         Lycos", "robot-availability": " ", "robot-details-url": "", "robot-type": "         ", "modified-date": "      ", "robot-noindex": "      no", "robot-owner-url": "    http://fuzine.mt.cs.cmu.edu/mlm/home.html", "robot-host": "         fuzine.mt.cs.cmu.edu, lycos.com", "robot-language": "     ", "robot-platform": "     ", "robot-owner-email": "  fuzzy@cmu.edu"}, "Resume Robot": {"robot-history": "", "robot-cover-url": "    http://www.onramp.net/proquest/resume/robot/robot.html", "robot-description": "", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   James Stakelum", "robot-useragent": "Resume Robot", "robot-from": "         yes", "robot-purpose": "      indexing.", "robot-exclusion-useragent": "", "robot-id": "           resumerobot", "robot-exclusion": "    yes", "robot-name": "         Resume Robot", "robot-availability": "", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Tue Mar 12 15:52:25 1996.", "robot-noindex": "", "robot-owner-url": "    http://www.onramp.net/proquest/resume/java/resume.html", "robot-host": "", "robot-language": "     C++.", "robot-platform": "", "robot-owner-email": "  proquest@onramp.net"}, "arks/": {"robot-history": "finds its root from s/w development project for a portal", "robot-cover-url": "http://www.dpsindia.com", "robot-description": "The Arks robot is used to build the database           for the dpsindia/lawvistas.com search service .           The robot runs weekly, and visits sites in a random order", "robot-environment": "commercial", "modified-by": "Aniruddha Choudhury", "robot-status": "development", "robot-owner-name": "Aniruddha Choudhury", "robot-useragent": "arks/", "robot-from": "no", "robot-purpose": "indexing", "robot-exclusion-useragent": "arks", "robot-id": "arks", "robot-exclusion": "yes", "robot-name": "arks", "robot-availability": "data", "robot-details-url": "http://www.dpsindia.com", "robot-type": "standalone", "modified-date": "6 th November 2000", "robot-noindex": "no", "robot-owner-url": "", "robot-host": "dpsindia.com", "robot-language": "Java 1.2", "robot-platform": "PLATFORM INDEPENDENT", "robot-owner-email": "aniruddha.c@usa.net"}, "Wget/": {"robot-history": "", "robot-cover-url": " ftp://gnjilux.cc.fer.hr/pub/unix/util/wget/", "robot-description": "  Wget is a utility for retrieving files using HTTP and FTP protocols.  It works non-interactively, and can retrieve HTML pages and FTP  trees recursively.  It can be used for mirroring Web pages and FTP  sites, or for traversing the Web gathering data.  It is run by the  end user or archive maintainer.", "robot-environment": " hobby, research", "modified-by": " Hrvoje Niksic", "robot-status": " development", "robot-owner-name": " Hrvoje Niksic", "robot-useragent": "Wget/", "robot-from": " yes", "robot-purpose": " mirroring, maintenance", "robot-exclusion-useragent": " wget", "robot-id": " wget", "robot-exclusion": " yes", "robot-name": " Wget", "robot-availability": " source", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Mon, 11 Nov 1996 06:00:44 MET", "robot-noindex": " no", "robot-owner-url": "", "robot-host": " *", "robot-language": " C", "robot-platform": " unix", "robot-owner-email": " hniksic@srce.hr"}, "Verticrawlbot": {"robot-history": " Verticrawl is based on web services for knowledge management and Web portals services and sitesearch solutions", "robot-cover-url": " http://www.verticrawl.com/", "robot-description": " Verticrawl is a global search engine dedicated to appliance service providing in ASP search & Appliance search solution", "robot-environment": " commercial", "modified-by": " webmaster@verticrawl.com", "robot-status": " active", "robot-owner-name": " Datamean, Malinge, Lhuisset", "robot-useragent": "Verticrawlbot", "robot-from": " Yes", "robot-purpose": " indexing, searching, and classifying urls in a global ASP search & Appliance search solution", "robot-exclusion-useragent": " verticrawl", "robot-id": " verticrawl", "robot-exclusion": "  verticrawl", "robot-name": " Verticrawl", "robot-availability": " none", "robot-details-url": " http://www.verticrawl.com/", "robot-type": " standalone", "modified-date": " mon,  27 Jul 2006 17:28:52 GMT", "robot-noindex": " yes", "robot-owner-url": " http://www.verticrawl.com/", "robot-host": " http://www.verticrawl.com/", "robot-language": "  c, perl, php", "robot-platform": " Unix, Linux", "robot-owner-email": " webmaster@verticrawl.com"}, "KO_Yappo_Robot/": {"robot-history": " The robot is hobby of k,osawa           at the Tokyo in 1997", "robot-cover-url": " http://yappo.com/info/robot.html", "robot-description": " The KO_Yappo_Robot robot is used to build the database           for the Yappo search service by k,osawa           (part of AOL).           The robot runs random day, and visits sites in a random order.", "robot-environment": " hobby", "modified-by": " KO", "robot-status": " active", "robot-owner-name": " Kazuhiro Osawa", "robot-useragent": "KO_Yappo_Robot/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " ko_yappo_robot", "robot-id": " ko_yappo_robot", "robot-exclusion": " yes", "robot-name": " KO_Yappo_Robot", "robot-availability": " none", "robot-details-url": " http://yappo.com/", "robot-type": " standalone", "modified-date": " Fri, 18 Jul 1996 12:34:21 GMT", "robot-noindex": " yes", "robot-owner-url": " http://yappo.com/", "robot-host": " yappo.com,209.25.40.1", "robot-language": " perl", "robot-platform": " unix", "robot-owner-email": " office_KO@yappo.com"}, "Gulliver/": {"robot-history": " Oct 1996: development; Dec 1996-Jan 1997: crawl & debug;  Mar 1997: crawl again;", "robot-cover-url": "", "robot-description": " Gulliver is a robot to be used to collect  web pages for indexing and subsequent searching of the index.", "robot-environment": " service", "modified-by": " Mike Mulligan", "robot-status": " active", "robot-owner-name": " Mike Mulligan", "robot-useragent": "Gulliver/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " gulliver", "robot-id": " gulliver", "robot-exclusion": " yes", "robot-name": " Northern Light Gulliver", "robot-availability": " none", "robot-details-url": "", "robot-type": " standalone", "modified-date": " Wed, 21 Apr 1999 16:00:00 GMT", "robot-noindex": " yes", "robot-owner-url": "", "robot-host": " scooby.northernlight.com, taz.northernlight.com,  gulliver.northernlight.com", "robot-language": " c", "robot-platform": " unix", "robot-owner-email": " crawler@northernlight.com"}, "Digimarc WebReader/": {"robot-history": " First operation in August 1997.", "robot-cover-url": " http://www.digimarc.com/prod_fam.html", "robot-description": " Examines image files for watermarks.  In order to not waste internet bandwidth with yet another crawler, we have contracted with one of the major crawlers/seach engines to provide us with a list of specific URLs of interest to us.  If an URL is to an image, we may read the image, but we do not crawl to any other URLs.  If an URL is to a page of interest (ususally due to CGI), then we access the page to get the image URLs from it, but we do not crawl to any other pages.", "robot-environment": " service", "modified-by": " Brian MacIntosh", "robot-status": " active", "robot-owner-name": " Digimarc Corporation", "robot-useragent": "Digimarc WebReader/", "robot-from": " yes", "robot-purpose": " maintenance", "robot-exclusion-useragent": "", "robot-id": " webreader", "robot-exclusion": " yes", "robot-name": " Digimarc MarcSpider", "robot-availability": " none", "robot-details-url": " http://www.digimarc.com/prod_fam.html", "robot-type": " standalone", "modified-date": " Mon, 20 Oct 1997 16:44:29 GMT", "robot-noindex": "", "robot-owner-url": " http://www.digimarc.com", "robot-host": " 206.102.3.*", "robot-language": " c++", "robot-platform": " windowsNT", "robot-owner-email": " wmreader@digimarc.com"}, "Patric/": {"robot-history": "          (contained at http://www.nwnet.net/technical/ITR/index.html )", "robot-cover-url": "        http://www.nwnet.net/technical/ITR/index.html", "robot-description": "      (contained at http://www.nwnet.net/technical/ITR/index.html )", "robot-environment": "      service ", "modified-by": "            toney@nwnet.net", "robot-status": "           development", "robot-owner-name": "       toney@nwnet.net", "robot-useragent": "Patric/", "robot-from": "             no", "robot-purpose": "          statistics", "robot-exclusion-useragent": " patric       ", "robot-id": "               patric", "robot-exclusion": "        yes", "robot-name": "             Patric", "robot-availability": "     data", "robot-details-url": "      http://www.nwnet.net/technical/ITR/index.html", "robot-type": "             standalone", "modified-date": "          Thurs, 15 Aug 1996", "robot-noindex": "          yes     ", "robot-owner-url": "        http://www.nwnet.net/company/staff/toney", "robot-host": "             *.nwnet.net     ", "robot-language": "         perl", "robot-platform": "         unix", "robot-owner-email": "      webmaster@nwnet.net"}, "ASpider/": {"robot-history": "      ", "robot-cover-url": "    ", "robot-description": "  ASpider is a CGI script that searches the web for keywords given by the user through a form.", "robot-environment": "  hobby", "modified-by": "", "robot-status": "       retired", "robot-owner-name": "   Fred Johansen", "robot-useragent": "ASpider/", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           aspider", "robot-exclusion": "    ", "robot-name": "         ASpider (Associative Spider)", "robot-availability": " ", "robot-details-url": "", "robot-type": "         ", "modified-date": "      ", "robot-noindex": "      no", "robot-owner-url": "    http://www.pvv.ntnu.no/~fredj/", "robot-host": "         nova.pvv.unit.no", "robot-language": "     perl4", "robot-platform": "     unix", "robot-owner-email": "  fredj@pvv.ntnu.no"}, "RuLeS/": {"robot-history": " none", "robot-cover-url": " http://www.rules.be", "robot-description": " ", "robot-environment": " hobby", "modified-by": " Marc Wils", "robot-status": " active", "robot-owner-name": " Marc Wils", "robot-useragent": "RuLeS/", "robot-from": " yes", "robot-purpose": " indexing", "robot-exclusion-useragent": " yes", "robot-id": " rules", "robot-exclusion": " yes", "robot-name": " RuLeS", "robot-availability": " none", "robot-details-url": " http://www.rules.be", "robot-type": " standalone", "modified-date": " Sun, 8 Apr 2001 13:06:54 CET", "robot-noindex": " yes", "robot-owner-url": " http://www.rules.be", "robot-host": " www.rules.be", "robot-language": " Dutch (Nederlands)", "robot-platform": " unix", "robot-owner-email": " marc@rules.be"}, "Araneo/": {"robot-history": "(The name Araneo means spider in Esperanto.)", "robot-cover-url": "    http://esperantisto.net", "robot-description": "  Araneo is a web robot developed for crawling and indexing web pages written in the international language Esperanto.  The database will be used to build a web search engine and auxiliary services to be published at esperantisto.net.", "robot-environment": "  hobby, research", "modified-by": "        Arto Sarle", "robot-status": "       development", "robot-owner-name": "   Arto Sarle", "robot-useragent": "Araneo/", "robot-from": "         yes", "robot-purpose": "      indexing, statistics", "robot-exclusion-useragent": " araneo", "robot-id": "           araneo", "robot-exclusion": "    yes", "robot-name": "         Araneo", "robot-availability": " none", "robot-details-url": "  http://esperantisto.net/araneo/", "robot-type": "         standalone", "modified-date": "      Fri, 16 Nov 2001 08:30:00 GMT", "robot-noindex": "      yesrobot-nofollow:     yes", "robot-owner-url": "    http://esperantisto.net", "robot-host": "         *.esperantisto.net", "robot-language": "     Python, Java", "robot-platform": "     Linux", "robot-owner-email": "  araneo@esperantisto.net"}, "JoBo ": {"robot-history": " JoBo was developed as a simple download tool and became a full featured web spider during development", "robot-cover-url": " http://www.matuschek.net/software/jobo/", "robot-description": " JoBo is a web site download tool. The core web spider can be used for any purpose.", "robot-environment": " hobby", "modified-by": " Daniel Matuschek <daniel@matuschek.net>", "robot-status": " active", "robot-owner-name": " Daniel Matuschek", "robot-useragent": "JoBo ", "robot-from": " yes", "robot-purpose": " downloading, mirroring, indexing", "robot-exclusion-useragent": " jobo", "robot-id": " jobo", "robot-exclusion": " yes", "robot-name": " JoBo Java Web Robot", "robot-availability": " source", "robot-details-url": " http://www.matuschek.net/software/jobo/", "robot-type": " standalone", "modified-date": " Fri, 20 Apr 2001 17:00:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.matuschek.net", "robot-host": " *", "robot-language": " java", "robot-platform": " unix, windows, os/2, mac", "robot-owner-email": " daniel@matuschek.net"}, "InfoSeek Robot": {"robot-history": "      ", "robot-cover-url": "    http://www.infoseek.com", "robot-description": "  Its purpose is to generate a Resource Discovery database.Collects WWW pages for both InfoSeek's free WWW search andcommercial search. Uses a unique proprietary algorithm toidentify the most popular and interesting WWW pages. Veryfast, but never has more than one request per siteoutstanding at any given time. Has been refined for morethan a year.", "robot-environment": "", "modified-by": "", "robot-status": "       ", "robot-owner-name": "   Steve Kirsch", "robot-useragent": "InfoSeek Robot", "robot-from": "         yes", "robot-purpose": "      indexing", "robot-exclusion-useragent": "", "robot-id": "           infoseek", "robot-exclusion": "    yes", "robot-name": "         InfoSeek Robot 1.0", "robot-availability": " ", "robot-details-url": "", "robot-type": "         standalone", "modified-date": "      Sun May 28 01:35:48 1995", "robot-noindex": "      no", "robot-owner-url": "    http://www.infoseek.com", "robot-host": "         corp-gw.infoseek.com", "robot-language": "     python", "robot-platform": "     ", "robot-owner-email": "  stk@infoseek.com"}, "CrawlPaper/": {"robot-history": " started as screensaver the program has evolved to a crawler including an audio player, etc.", "robot-cover-url": " http://www.crawlpaper.com/", "robot-description": " a crawler for pictures download and offline browsing", "robot-environment": " hobby", "modified-by": "", "robot-status": " active", "robot-owner-name": " Luca Piergentili", "robot-useragent": "CrawlPaper/", "robot-from": "", "robot-purpose": " indexing", "robot-exclusion-useragent": " crawlpaper", "robot-id": " wallpaper", "robot-exclusion": " yes", "robot-name": " WallPaper (alias crawlpaper)", "robot-availability": " source, binary", "robot-details-url": " http://sourceforge.net/projects/crawlpaper/", "robot-type": " standalone", "modified-date": " Mon, 25 Aug 2003 09:00:00 GMT", "robot-noindex": " no", "robot-owner-url": " http://www.geocities.com/lpiergentili/", "robot-host": "", "robot-language": " C++", "robot-platform": " windows", "robot-owner-email": " lpiergentili@yahoo.com"}, "MwdSearch/": {"robot-history": " (none)", "robot-cover-url": " (none)", "robot-description": " Robot for indexing finnish (toplevel domain .fi)                   webpages for search engine called Fifi.                   Visits sites in random order.", "robot-environment": " service (+ commercial)mwd.sci.fi>", "modified-by": " Antti.Westerberg@mwd.sci.fi", "robot-status": " active", "robot-owner-name": " Antti Westerberg", "robot-useragent": "MwdSearch/", "robot-from": " no", "robot-purpose": " indexing", "robot-exclusion-useragent": " MwdSearch", "robot-id": " mwdsearch", "robot-exclusion": " yes", "robot-name": " Mwd.Search", "robot-availability": " none", "robot-details-url": " (none)", "robot-type": " standalone", "modified-date": " Mon, 26 May 1997 15:55:02 EEST", "robot-noindex": " yes", "robot-owner-url": " (none)", "robot-host": " *.fifi.net", "robot-language": " perl5, c", "robot-platform": " unix (Linux)", "robot-owner-email": " Antti.Westerberg@mwd.sci.fi"}, "htdig/": {"robot-history": "      ", "robot-cover-url": "    http://www.htdig.org/", "robot-description": "  A mirroring robot. Configured to stay within a directory,sleeps between requests, and the next version will use HEADto check if the entire document needs to beretrieved", "robot-environment": "", "modified-by": "", "robot-status": "", "robot-owner-name": "   Andrew Scherpbier", "robot-useragent": "htdig/", "robot-from": "         no", "robot-purpose": "      indexing", "robot-exclusion-useragent": " htdig", "robot-id": "           htdig", "robot-exclusion": "    yes", "robot-name": "         ht://Dig", "robot-availability": " source", "robot-details-url": "  http://www.htdig.org/howitworks.html", "robot-type": "         standalone", "modified-date": "      ", "robot-noindex": "      yes", "robot-owner-url": "    http://www.htdig.org/author.html", "robot-host": "         *", "robot-language": "     C,C++.robot-history:This robot was originally developed for use at San Diego State University.robot-environment:modified-date:Tue, 3 Nov 1998 10:09:02 EST modified-by: Geoff Hutchison <Geoffrey.R.Hutchison@williams.edu>robot-id:           htmlgobblerobot-name:         HTMLgobblerobot-cover-url:    robot-details-url:robot-owner-name:   Andreas Leyrobot-owner-url:    robot-owner-email:  ley@rz.uni-karlsruhe.derobot-status:       robot-purpose:      mirrorrobot-type:         robot-platform:     robot-availability: robot-exclusion:    robot-exclusion-useragent:robot-noindex:      norobot-host:         tp70.rz.uni-karlsruhe.derobot-from:         yesrobot-useragent:    HTMLgobble v2.2robot-language:     ", "robot-platform": "     unix", "robot-owner-email": "  andrew@contigo.comrobot-owner-name2:  Geoff Hutchison robot-owner-url2:   http://wso.williams.edu/~ghutchis/robot-owner-email2: ghutchis@wso.williams.edu"}, "griffon/": {"robot-history": "Its root is TITAN project in NTT.                                  ", "robot-cover-url": "http://navi.ocn.ne.jp/                                           ", "robot-description": "The Griffon robot is used to build database for the OCN navi          search service operated by NTT Communications Corporation.       It mainly gathers pages written in Japanese.            ", "robot-environment": "service                                                        ", "modified-by": "toka@navi.ocn.ne.jp", "robot-status": "active                                                              ", "robot-owner-name": "NTT Communications Corporate Users Business Division            ", "robot-useragent": "griffon/", "robot-from": "yes                                                                   ", "robot-purpose": "indexing                                                           ", "robot-exclusion-useragent": "griffon                                                ", "robot-id": "griffon", "robot-exclusion": "yes                                                              ", "robot-name": "Griffon                                                               ", "robot-availability": "none                                                          ", "robot-details-url": "http://navi.ocn.ne.jp/griffon/                                 ", "robot-type": "standalone                                                            ", "modified-date": "Mon,25 Jan 2000 15:25:30 GMT                                       ", "robot-noindex": "yes                                                                robot-nofollow:yes                                                              ", "robot-owner-url": "http://navi.ocn.ne.jp/                                           ", "robot-host": "*.navi.ocn.ne.jp                                                      ", "robot-language": "c                                                                 ", "robot-platform": "unix                                                              ", "robot-owner-email": "griffon@super.navi.ocn.ne.jp                                   "}}
'''

__bots_dict = jsonSerializer.loads(__bots_json)

__bots = __bots_dict.keys()


def get_browser_from(s_user_agent,seps=[';', ','],regex=r"\(.*?\)"):
    __analysis = {}
    item = s_user_agent
    matches = [aMatch for aMatch in re.findall(regex,item)]
    analysis = [[t.strip() for t in _utils.splits(aMatch.replace('(', '').replace(')', ''),seps)] for aMatch in matches]
    isMSIE = any([(m.find(__msie['MSIE']) > -1) if (not misc.isList(m)) else any([(mm.find(__msie['MSIE']) > -1) for mm in m]) for m in analysis])
    isAndroid = any([(m.find(__android['Android']) > -1) if (not misc.isList(m)) else any([(mm.find(__android['Android']) > -1) for mm in m]) for m in analysis])
    isRobot = False
    try:
        for aBot in __bots:
            if (item.find(aBot) > -1):
                _bucket = {}
                isRobot = True
                aBrowser = __bots_dict[aBot]['robot-name']
                if (__analysis.has_key(aBrowser)):
                    _bucket = __analysis[aBrowser]
                else:
                    __analysis[aBrowser] = {}
                    _bucket = __analysis[aBrowser]
                try:
                    _version = [v for v in analysis[0] if (v.find(aBot) > -1)][0].split(aBot)[-1]
                except IndexError:
                    _item = item
                    for aMatch in matches:
                        _item = _item.replace(aMatch,'')
                    _version = _item.split(aBot)[-1]
                _bucket['version'] = _version
                _bucket['isRobot'] = isRobot
                break
    except:
        pass
    if (isMSIE):
        aBrowser = __msie.keys()[0]
        _bucket = {}
        if (__analysis.has_key(aBrowser)):
            _bucket = __analysis[aBrowser]
        else:
            __analysis[aBrowser] = {}
            _bucket = __analysis[aBrowser]
        _version = [v for v in analysis[0] if (v.find(__msie['MSIE']) > -1)][0].split(__msie['MSIE'])[-1]
        _bucket['version'] = _version
        _bucket['isRobot'] = isRobot
    elif (not isRobot):
        for aMatch in matches:
            item = item.replace(aMatch,'')
        toks = item.split()
        for aBrowser,aToken in __browsers.iteritems():
            _bucket = {}
            aBrowser = '%s%s'%(__android.keys()[0],aBrowser) if (isAndroid) else aBrowser
            if (__analysis.has_key(aBrowser)):
                _bucket = __analysis[aBrowser]
            else:
                __analysis[aBrowser] = {}
                _bucket = __analysis[aBrowser]
            _bucket['isRobot'] = isRobot
            _token = aToken if (not misc.isList(aToken)) else aToken[0]
            _analysis = [t for t in toks if (t.find(_token) > -1)]
            isAnalysis = len(_analysis) > 0
            _version = [[tt for tt in t.split(_token) if (len(tt) > 0)] for t in _analysis]
            _version = _version[0] if (misc.isList(_version) and (len(_version) > 0)) else _version
            _version = [t for t in _version if (all([n.isdigit() for n in t.split('.')]))] if (misc.isList(_version) and (len(_version) > 0)) else _version
            _version = _version[0] if (misc.isList(_version) and (len(_version) > 0)) else _version
            if (isAnalysis):
                try:
                    _bucket['version'] = float('.'.join(_version.split('.')[0:2]))
                except Exception as e:
                    info_string = _utils.formattedException(details=e)
                    pass
            else:
                del __analysis[aBrowser]
    return __analysis

def get_browser_name_version_from(s_user_agent,seps=[';', ','],regex=r"\(.*?\)"):
    a = get_browser_from(s_user_agent,seps=seps,regex=regex)
    k = a.keys()
    browserName = k[0] if (len(k) > 0) else 'UNKNOWN'
    try:
        browserVersion = a[browserName]['version']
    except KeyError:
        browserVersion = 'UNKNOWN'
    try:
        isRobot = a[browserName]['isRobot']
    except KeyError:
        isRobot = 'UNKNOWN'
    return browserName,browserVersion,isRobot
